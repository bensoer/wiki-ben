{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to Wiki Ben! This is an archive of various tips, tricks, topics and guides I have collected.</p>"},{"location":"cryptography/aes/","title":"Advanced Encryption Standard (AES)","text":"<p>The Nationional Institute of Standards and Technology (NIST) put out a notice in early 1999 requesting submissions for a new encryption standards. The requirements were as follows: * A symmetric block cipher with a variable length key (128, 192 or 256 bit) and a 128-bit block * More secure then Triple-DES * Must belong to the public domain - royalty free world wide * Remain secure for at least 30 years</p> <p>15 algorithms were submitted from 10 different countries. NIST relied on public participation in algorithm proposals, cryptanalysis and efficiency testing.</p> <p>From August 20 - April 15, 1999 round 1 submissions were taken. A second conference for AES was held then on February 1, 1999 where the top 5 finalists were announced. The following 6 to 9 months was then spend analysing the finalists. At the thrid AES conference in 2001, the winner was announced. The Rijnhael implementation was selected, and the AES standard was born</p> <p>The Reijnhael implementation contained the following specs:</p> <ul> <li>Variable block lenghts of 128, 192 and 256 bit</li> <li>Varialbe key lenght of 128, 192 and 256 bit</li> <li>Variable number of rounds (iterations) of 10,12 and 14<ul> <li>The number of rounds corresponds and depend on the key/block length</li> </ul> </li> </ul>"},{"location":"cryptography/aes/#algorithm-example","title":"Algorithm Example","text":"<p>For the algorithm example we will be primarily following documentation based on the NIST FIPS 197 Publication of the AES Algorithm. The original documentation can be found here : http://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.197.pdf</p> <p>We are going to use the message:  </p> <ul> <li><code>AESbeststherest!</code></li> </ul> <p>and encrypt it with the key: </p> <ul> <li><code>2B 7F 15 16 28 AE D2 A6 AB F7 15 88 08 CF 4F 3C</code></li> </ul> <p>Note that both of these make perfect 128-bit message and key. This example will use the 128-bit version of AES. This version requires a 128-bit key. For a message that is larger then 128 bits, this process merely needs to be repeated. The last bit then padded to fit into the 128-bit block.</p> <p>Note that this implementation is also abstracted from either being EBS or CBC as these portions of the algorithm have to do with how each block is treated with the next in the final encryption. This can be easily wrapped over the current example to implement them</p>"},{"location":"cryptography/aes/#encryption","title":"Encryption","text":""},{"location":"cryptography/aes/#step-1-convert-message-and-place-into-matrix","title":"Step 1: Convert Message And Place Into Matrix","text":"<p>Step one is to simply place the message into a matrix, in then convert it to its hexadecimal values, this is done simply with an ASCII table lookup</p> <pre><code>//plaintext\n+===+===+===+===+    \n| A | e | t | e |\n+===+===+===+===+\n| E | a | h | s |\n+===+===+===+===+\n| S | t | e | t |\n+===+===+===+===+\n| b | s | r | ! |\n+===+===+===+===+\n\n//convert to hex\n+===+===+===+===+\n|41 |65 |74 |65 |\n+===+===+===+===+\n|45 |61 |68 |73 |\n+===+===+===+===+\n|53 |74 |65 |74 |\n+===+===+===+===+\n|62 |73 |72 |21 |\n+===+===+===+===+\n</code></pre>"},{"location":"cryptography/aes/#step-2-s-box-lookup","title":"Step 2: S-box Lookup","text":"<p>Next take the matrix we created in step 1 and using each hex number as a row then column indexes, apply a substitution to each value. </p> <p>Taking the top left value in the matrix of 41, this means then to replace this value with the value located at row 4, column 1 in the S-box matrix (83). The S-box matrix is as follows:</p> <p></p> <p>After running all values through the S-box, our matrix looks as follows: <pre><code>+===+===+===+===+\n|83 |4D |92 |4D |\n+===+===+===+===+\n|6E |EF |45 |8F |\n+===+===+===+===+\n|ED |92 |4D |92 |\n+===+===+===+===+\n|AA |8F |40 |FD |\n+===+===+===+===+\n</code></pre></p>"},{"location":"cryptography/aes/#step-3-row-shift-operation","title":"Step 3: Row Shift Operation","text":"<p>The next step is to row shift the matrix. We do this by starting with the top row and rotating it 0 boxes to the left, then the 2nd row by 1, 3rd by 2, and 4th by 3. The end result looks as follows:</p> <pre><code>//pre-shift\n+===+===+===+===+\n|83 |4D |92 |4D |\n+===+===+===+===+\n|6E |EF |45 |8F |\n+===+===+===+===+\n|ED |92 |4D |92 |\n+===+===+===+===+\n|AA |8F |40 |FD |\n+===+===+===+===+\n\n//post-shift\n+===+===+===+===+\n|83 |4D |92 |4D |\n+===+===+===+===+\n|EF |45 |8F |6E |\n+===+===+===+===+\n|4D |92 |ED |92 |\n+===+===+===+===+\n|FD |AA |8F |40 |\n+===+===+===+===+\n</code></pre> <p>The effect here is similar to a left-shift except we wrap the end values around to the other end</p>"},{"location":"cryptography/aes/#decryption","title":"Decryption","text":""},{"location":"cryptography/aes/#key-schedule-key-generation","title":"Key Schedule / Key Generation","text":"<p>The Key schedule process shares similar steps to the encryption process. The initial key is used to help generate additional sub-keys, which are then used at each iteration of the AES encryption procedure.</p> <p>The initial key we will use is: * <code>2B 7F 15 16 28 AE D2 A6 AB F7 15 88 08 CF 4F 3C</code></p> <p>From this we will generate the rest of the keys</p> <p>We start by placing the key into a matrix: <pre><code>+===+===+===+===+\n|2B |28 |AB |08 |\n+===+===+===+===+\n|7F |AE |F7 |CF |\n+===+===+===+===+\n|15 |D2 |15 |4F |\n+===+===+===+===+\n|16 |A6 |88 |3C |\n+===+===+===+===+\n</code></pre></p> <p>Now, when we start, we start with the first column and the column 3 columns ahead of it (column 4 in this case). With these 2 columns we will generate the next column that will be appended onto the matrix. After doing this 4 times in this example, we will have successfully generated another key.</p> <p>There is one special rule though in the generation process, whenever you are using the first column adn 4th column of a single key, a number of extra steps must be done.</p> <p>To start, take the first column and the 4th column as such: <pre><code>+===+       +===+\n|2B |       |08 |\n+===+       +===+\n|7F |       |CF |\n+===+       +===+\n|15 |       |4F |\n+===+       +===+\n|16 |       |3C |\n+===+       +===+\n</code></pre></p> <p>Because this is the 1st and 4th column of the key we need to apply a shift to the 4th collumn followed by running the column through the S-box. You can see the S-box in the encryption section. Like the encryption step involving the S-box the procdure is to take the first value as the row and the second number as the column to determine what value is substituted in. Taking the first value 08 means to look in the S-box on row 0, column 8 for the value to replace it with. After shifting and then substituting the results through the s-box you will have the following results:</p> <pre><code>//post-rotate\n+===+\n|CF |\n+===+\n|4F |\n+===+\n|3C |\n+===+\n|08 |\n+===+\n//post s-box substitution\n+===+\n|8A |\n+===+\n|84 |\n+===+\n|EB |\n+===+\n|30 |\n+===+\n</code></pre> <p>From here we then XOR the 1st column with our newly manipulated 4th column. In addition we also XOR this value with an Rcon column value. The Rcon we choose is always the farthest left column, which after it has been used is removed from the Rcon table. This way we always use a different Rcon value every time we have this situation with the 1st and 4th column of the same key</p> <p>The Rcon table looks as follows:</p> <p></p> <p>Our equation to XOR all together looks as follows:</p> <pre><code>+===+       +===+       +===+\n|2B |       |8A |       |01 |\n+===+       +===+       +===+\n|7F |       |84 |       |00 |\n+===+  XOR  +===+  XOR  +===+\n|15 |       |EB |       |00 |\n+===+       +===+       +===+\n|16 |       |30 |       |00 |\n+===+       +===+       +===+\n</code></pre> <p>Once these values have been XORd together, this newly generated column is appended to the end of our original matrix as such:</p> <pre><code>+===+===+===+===++===+\n|2B |28 |AB |08 ||A0 |\n+===+===+===+===++===+\n|7F |AE |F7 |CF ||FB |\n+===+===+===+===++===+\n|15 |D2 |15 |4F ||FE |\n+===+===+===+===++===+\n|16 |A6 |88 |3C ||26 |\n+===+===+===+===++===+\n</code></pre> <p>Now we take the 2nd column and the now just added new column. Note that we are choosing this because we are simply incrementing along the columns from the last cycle ?(last cycle was colums 1 and 4, now we are using 2 and 5)</p> <p>Since this iteration does not use 2 columns from the same key (column 5 is part of the partialy generated 2nd key) We do not need to do the intermediate rotate and s-box substitution steps. Note the next time you will need to do the extra rotate and s-box substitution steps in this example would be when we have iterated to using column 5 and column 8.</p> <p>In this simpler mode we simply take column 2 and column 5 as such:</p> <pre><code>    +===+       ++===+\n    |28 |       ||A0 |\n    +===+       ++===+\n    |AE |       ||FB |\n    +===+       ++===+\n    |D2 |       ||FE |\n    +===+       ++===+\n    |A6 |       ||26 |\n    +===+       ++===+\n</code></pre> <p>And then XOR them together</p> <pre><code>    +===+       ++===+\n    |28 |       ||A0 |\n    +===+       ++===+\n    |AE |       ||FB |\n    +===+  XOR ++===+\n    |D2 |       ||FE |\n    +===+       ++===+\n    |A6 |       ||26 |\n    +===+       ++===+\n</code></pre> <p>The resulting new column is then appended back to our original matrix as such:</p> <pre><code>+===+===+===+===++===+===+\n|2B |28 |AB |08 ||A0 |88 |\n+===+===+===+===++===+===+\n|7F |AE |F7 |CF ||FB |55 |\n+===+===+===+===++===+===+\n|15 |D2 |15 |4F ||FE |2C |\n+===+===+===+===++===+===+\n|16 |A6 |88 |3C ||26 |80 |\n+===+===+===+===++===+===+\n</code></pre> <p>This loop then continues until all required keys have been generated</p>"},{"location":"cryptography/des/","title":"Data Encryption Standard (DES)","text":"<p>In the mid-70's the US government decided that a powerful standard cipher system was necessary. The Nation Burea of Standards posted requests for the development of such a cipher. Several companies submitted proposals but the winnder was IBM with their cipher system called Lucifer. With some modifications by the National Security Agency Lucifer became know as the Data Encryption Standard (DES) in 1977</p> <p>DES was most widely used until the introduction of the Advanced Encryption Standard (AES) in 2001. The DES algorithm itself is referred to as the Data Encryption Algorithm (DEA). DEA uses the following specifications: * Data encryption in 64-bit blocks using 56-bit keys * 64-bit input is transformed through a number of steps to produce 64-bit output</p> <p>The majority of this documentation is a re-iteration of the NIST FIPS46-3 Publication in an effort to ease understanding and allow for quick reference by those already knowledgable in the DES/DEA cipher. The original documentation can be found here : http://csrc.nist.gov/publications/fips/fips46-3/fips46-3.pdf</p>"},{"location":"cryptography/des/#algorithm-example","title":"Algorithm Example","text":"<p>The following will be a walkthrough of the DES algorithm</p> <p>Note that this implementation does not include any ECB or CBC components as the example only walksthrough using a single block of data. With ECB and CBC implementations effect how the data is handled between each block when they are encrypted, and can easily be implemented around this demonstration of DES.</p> <p>The overall encryption procedure looks as follows:</p> <p></p> <p>We are going to encrypt with the message <code>MyMessag</code> with the key <code>password</code>. Note that both of these words work out perfectly as 64-bits (1 bytes per letter = 8 bits per letter). This is for simplicity but with a larger value, the solution is still the same. Both the message and key will be converted and manipulated as binary, with larger messages that do not fit into 64-bit chunks, these can be left-padded with zeros until they evenly work in 64-bit chunks. Following the FIPS implementation the key in this case MUST be 64-bits as there is no alternative implementation as of this writing.</p>"},{"location":"cryptography/des/#encryption","title":"Encryption","text":""},{"location":"cryptography/des/#1-initial-permutation-ip","title":"1. Initial Permutation (IP)","text":"<p>The first step is to reformat the message into the initial permutation. The DES initial permutation and inverse permutation (IP-1) are as follows. We will be using the IP-1 at the end</p> <p></p> <p>Before we can even do this though, we have to convert <code>MyMessag</code> to binary. This is simply done with ASCII conversion. <code>MyMessag</code> in binary is as follows:</p> <pre><code>01001101 01111001 01001101 01100101 01110011 01110011 01100001 0110011\n  M         y        M        e        s        s        a       g\n</code></pre> <p>Now we run each bit through the IP chart. Starting from the top left of the chart it specifies that bit 1 should be replaced with whatever value is in bit 58, bit 2 with bit 50, bit 3 with bit 42, etc. This continues along the row for each row until the entire 64-bit message block has been converted. Note that we are only dealing with 1 message block, if there is multiple message blocks, the process for each block must start at this step again</p> <p>With the contents converted <code>MyMessag</code> will look as follows (as of this writing this is only the first 32-bits):</p> <pre><code>11111111 00110010 10001101 011111111 00000000 11111010 00000111 10110000\n   M        y        M        e          s        s       a         g\n</code></pre> <p>Note that these binary values no longer represent their ASCII characters, they are just there to assist in clarity</p>"},{"location":"cryptography/des/#2-f-calculation","title":"2. F Calculation","text":"<p>From the IP result we then split the permuted input (the result of our IP) into two halfs. Following the diagram we will label the left half with L and the right half with R. From here we will take the R half and using a key K, execute the F function. The R value is as follows:</p> <pre><code>00000000 11111010 00000111 10110000\n   s        s        a        g\n</code></pre> <p>An overview of the F function is as follows:</p> <p></p> <p>This as you can see has a number of substeps</p>"},{"location":"cryptography/des/#21-processing-r-through-e","title":"2.1 Processing R through E","text":"<p>This first substep requires converting the original 32-bit R value to a 48-bit value, to match our key (K) length. This is done using the following \"E-bit selection table\"</p> <p></p> <p>This mapping is done identicaly to how we ran our plaintext message through the IP table. Starting from the top left we take from our R value the 32nd bit and place it in position 1, followed by bit 1 in position 2 and bit 2 in position 3, etc. Following the rows down. Notice that there is an overlap in the bits. Looking at the first column of the chart we can check and see that bit 32, 4, and 8 are used multiple times. This is intentional, and is how DES generates the 48 needed bits from only the 32-bits its been given for this step. After running our R value through the E-bit selection table, we should have the following:</p> <pre><code>000000 000001 011111 110100 000000 001111 110110 100000\n</code></pre> <p>It has been cutup in groups of 6 to reflect the E-chart</p>"},{"location":"cryptography/des/#22-xor-with-the-key","title":"2.2 XOR With The Key","text":"<p>This is a simple step of XORing the provided key into the 48-bit generated message segment. For details on how the key is generated for this step, see the Key Schedule / Key Generation section of these documents. Reading through those steps will show how the keys are generated, running through the first key (K1) that is used here. The documentation will make reference to this point in those instructions.</p> <p>The Key (K1) Is As Follows: </p> <p>Using XOR we get the following answer: </p>"},{"location":"cryptography/des/#23-processing-through-s-tables","title":"2.3 Processing Through S-Tables","text":"<p>After our message block has been XORd we now must run each section of bits through the appropriate S-table. Each S-table tabkes 6 bits as a source and resolves to a 4-bit output. This allows us to revert back to our original R size of 32-bits for the next iterations of the DES algorithm</p> <p>Out of the 48-bit value generated from Step 2.2 cut the binary into sections of 6 bits. In total you will have 8 groups of 6 bits. Each of these sections are mapped to the appropriate S table simply based on their index. The first section is mapped with S-table 1, the second with S-table 2, etc.</p> <p>Using our example, cutting the 48-bits into 8 groups of 6 bits looks as follows. Below it is also the S-table mapping each section will be run through: </p> <p>The 6 bit block taken is then parsed to generate an X and a Y cordinate value. The values then resolve to a 4-bit number located in the appropriate S-table. The numbers are generated as follows:</p> <ul> <li>From the 6-bit block - take the first and last bits and append them together. This will represent a decimal value between 0 and 3. This value is the Y-coordinate</li> <li>From the 6-bit block - take the middle 4 bits not yet used and append them together. This will represent a decimal value between 0 and 15. This value will be the X-coordinate</li> </ul> <p>Each block from our example will generate the following X,Y coordinates: </p> <p>Viewing the S-tables it is then a simple process of mapping these X,Y coordinates to their appropriate value within the table. Upon determining this value, convert it to binary.</p> <p>Resolving the X,Y coordinates in the appropriate S-table and then converting the value back to binary resulted in the following: </p> <p>We can append this value back together to create a 32-bit value</p>"},{"location":"cryptography/des/#24-p-table","title":"2.4 P-Table","text":"<p>Taking our 32-bit value from step 2.4 we now do a similar step from earlier in running the value through another table. This like the IP table is a replacement table. The table looks as follows:</p> <p></p> <p>Starting from the top left, we replace bit 1 with bit 15, bit 2 with bit 7, bit 3 with bit 20, etc. Through all the rows until all 32-bits have been converted</p>"},{"location":"cryptography/des/#3-xor-f-results-with-l","title":"3. XOR F Results with L","text":"<p>Step 2, processes all of the F functionality of the DES cipher. Following this calculation, the results of F is then XOR'd with the L value, which was split from the results of the IP table.</p> <p>The XORing of the L value and the result of F creates the following results</p> <pre><code> //show work XORing\n</code></pre>"},{"location":"cryptography/des/#4-swap","title":"4. Swap","text":"<p>After XORing F with the L value, the final step is a swap and variable reassignment. The L value now will point to the original R value, and R will now equal to the XOR result in step 3 of F and L. In summary</p> <p>L = R R = Result of Step 3 (XOR F and Old L)</p> <p>At this point you have completed a full iteration of the DES encryption cipher. To continue onto the next iteration, take the newly assigned L and R values from this step and proceed to Step 2 of the encryption process. Continue this cycle for 15 iterations, using 15 of the generated keys.</p> <p>On the 16th/Final iteration, having now just used the 16th key when calculating F, DO NOT carry out the swap stated above, instead the assignment does not change and is as follows: L = L R = R</p> <p>Please see the encryption overview diagram for additional clarification of this swap step</p> <p>From here you can proceed to Step 5 and the Inverse IP value.</p>"},{"location":"cryptography/des/#5-inverse-initial-permutation-ip-1","title":"5. Inverse Initial Permutation (IP-1)","text":"<p>The inverse initial permutation is an identical procedure from the initial permutation in Step 1, except using the IP-1 table to carryout the substitutions. Note the source data being used will be the L and R values appended together, creating a 64-bit data segment. This segment is then run through the IP-1 table to generate the final encrypted output</p>"},{"location":"cryptography/des/#key-schedulekey-generation","title":"Key Schedule/Key Generation","text":"<p>The key schedule is the processing code that takes the supplied key and generates a number of sub keys which are used throughout the encryption process. DES's key schedule is quite simple and merely repeats a number of steps 16 times to generate all 16 keys. An overview of the process is as follows:</p> <p></p>"},{"location":"cryptography/des/#1-permuted-choice-1","title":"1) Permuted Choice 1","text":"<p>The first step is to generate the permuted choice, which like the IP and E tables is a process of remapping the key bits</p> <p></p> <p>Note that the table has been split up. The top portion refers to the portion of bits that will make up the C portion shown in the key schedule overview diagram, and the second portion with make up the D portion</p> <p>Converting our key of \"password\" to binary comes out as follows:</p> <pre><code>01110000 01100001 01110011 01110011 01110111 01101111 01110010 01100100\n   p         a        s       s        w        o         r       d\n</code></pre> <p>And after running through the PC-1 table we have the following results. We have split the result into the appropriate C and D portions already aswell: <pre><code>\n</code></pre></p>"},{"location":"cryptography/des/#2-left-shift","title":"2) Left Shift","text":"<p>The next step is to \"left-shift\" the C and D portions. This left shit is a kind of left-shift-rotate though as typical left shifting would mean zeros are appended onto the right end of the binary. In this case though we will shift and then append to the right side, the values on the left that were shifted off.</p> <p>The amount of left-shiting is defined by which key we are trying to generate, based on the following table:</p> Iteration Number Number of Left Shifts 1 1 2 1 3 2 4 2 5 2 6 2 7 2 8 2 9 1 10 2 11 2 12 2 13 2 14 2 15 2 16 1"},{"location":"cryptography/des/#3-append-permute-choice-2","title":"3) Append &amp; Permute Choice 2","text":"<p>The next step simply involves appending the 2 left-shifted C and D values together and then running it through the PC-2 table, same as PC-1</p> <p>Appended, our left-shited C and D values looks like this:</p> <p>The PC-2 table is as follows:</p> <p>After running our results through PC-2 our answer is this:</p> <p>We have now successfully created K1 ! If you are working through the Encryption steps of this page, you can take this value and return to step 2.2 and XOR this value with the message.</p> <p>To generate further keys K2 all the way up to K16, simply repeat the process of Step 2 and 3 using the C and D values created post left-shift in Step 3 as the C and D values to be left-shifted in step 2. </p> <p>Note that you only run the appended C and D values through PC-2 when you are generating the key, this appending and PC-2 step is not applied to the data used for C and D in the next key iteration</p>"},{"location":"cryptography/rsa/","title":"RSA","text":"<p>RSA is an asymettric cryptographic algorithm by Ronal Rivest, Adi Shami and Leonard Adleman. RSA is an implementation based on Diffie &amp; Hellman public key cipher ideas. The implementation allows each user to have their own keyset and allows simpler distribution of those keys between members. Each member has a public and private key. The public key is what is shared to others, and is used to encrypt the messages using RSA. The private key is kept secret by the key-pair owner and is used to decrypt the message.</p> <p>The idea behind RSA is the use of prime numbers. Prime numbers are numbers that can only be divided by themselves or 1. Generating large prime numbers is incredibly time consuming and is hard to find. This fact alone is what RSA relys on for its security. RSA in this case is \"computationaly secure\", as the information being encrypted will be obsolete by the time the user can crack it. One of the largest prime numbers: 2^(657,885,161)-1 for example is 17,425,170 digits long, fills a 17MB file and takes 4-6 days to prove it is prime.</p> <p>Another factor behind RSA is the use of coprime numbers. Coprime numbers are two numbers that's greatest common divisor is 1 ( GCD(A,B)==1 ). Note that A and B do not have to be prime numbers to be coprime numbers.</p>"},{"location":"cryptography/rsa/#algorithm","title":"Algorithm","text":"<p>The RSA algorithm is quite simple. First you must generate a public and private key. This is done by answering a number of equations as follows:</p> <ol> <li>Choose 2 prime numbers P and Q</li> <li>Calculate N by multiplying P and Q ( N = P*Q )</li> <li>Calculate the totient T by multiplying 1 less of P with 1 less then Q ( T = (P-1)*(Q-1) )</li> <li>Choose a 3rd prime number E which is greater then prime P or prime Q but less then T. E when compared with T must also have a GCD of 1 ( E where E &gt; (P || Q ) &amp;&amp; E &lt; T &amp;&amp; GCD(E,T)==1 )</li> <li>Calculate D by stabilizing the following congruency: E * D = 1(mod T). This can be calculated through manual trial and error or using the Extended Euclidean Algorithm</li> <li>After this, test that E and D are coprime</li> </ol> <p>After completing this you will have all the components needed for creating a public and private key. In the most simplistic form, E and N make up the public key. D and N make up the private key. This information alone is all that is needed to carry out the RSA algorithm, however for transferring this information a standard ASN.1 format is used most commonly when circulating private and public keys.</p>"},{"location":"cryptography/rsa/#encryption","title":"Encryption","text":"<p>Encryption is simply done by running each segment of data through an equation using the public key components created above, and retrieving the ciphertext segments.</p> <p>The general equation is as follows: <pre><code>C = M^E(mod N)\n</code></pre> C is the ciphertext. M is the plaintext. E and N come from the public key generated in the above section. Note that size of M matters when encrypting with RSA. The value of M as an integer cannot be larger then the value of N as it will otherwise be lost (a consequence of modular arithmetic). Thus when encryption plaintext, some earlier calculations must be done to ensure the M value is small enough.</p> <p>Given ASCII characters map to 8-bit this means that N must always be larger then 2^8 = 256. This is not always possible if you are using smaller prime numbers such as P = 5 and Q = 11, N = 55. In this case the ASCII letter need to be cut up into smaller segments. The segment size can be calculated with: Math.floor(Log(N)/Log2). This will give the number of bits that the message must be broken into and the maximum number of plaintext bits that can be used in M.</p> <p>Using our example of P = 5 and Q = 11, we know N = 55. Using the above mentioned equation we know that Math.floor(Log(55)/Log(2)) = 5. That means whatever plaintext value M is, cannot be larger then the maximum value that can be represented by 5 bits or 2^5 = 32. We can also prove this limitation: if we used 6 bits the maxiumum value would be 2^6 = 64 which is larger then N = 55, thus 5 is the maximum we can use.</p> <p>After the encryption, you will need to do an extra calculation to determine how large the encrypted values should be. This is important as some of the encrypted values may need padding, so that they can be properly parsed at decrytpion. To determine how many bits should be in the encrypted value use the equation Math.ceil(Log(N)/Log(2)). Using our example Math.ceil(Log(55)/Log(2)) = 6. So with our encrypted values in binary, ensure that they are left padded to 6-bits before doing any appending actions of the ciphertext bits.</p>"},{"location":"cryptography/rsa/#decryption","title":"Decryption","text":"<p>Decryption is done in the exact same way as Encryption except with the inverse equation using the private key components.</p> <p><pre><code>M = C^D(mod N)\n</code></pre> C is the ciphertext. M is the plaintext. D and N come from the private key generated from earlier. Like encryption the size of N matters when decrypting with RSA. It differs slightly from encryption though as the encrypted values can represent inclusively up to the value N. Thus our equation for determining how to parse apart the ciphertext segments differs slightly. To determine the the number of bits to represent the ciphertext we use the equation Math.ceil(Log(N)/Log(2)). Using N = 55, we know that the ciphertext must be broken up into Math.ceil(Log(55)/Log(2)) = 6 bit segments.</p> <p>Like encryption, after decryption you can use Math.floor(Log(N)/Log(2)) to calculate how many bits should be in the plaintext value. Left-pad the value until it is the length calculated, and then append the values together. If the message received is ASCII characters, you can now process the binary in 8-bit segments. Note that to work with N during encryption there may be extra padding on the end. Read in the ASCII data from left to right, parsing it into 8-bit segments. If you reach remaining data, which should all be zeros, simply drop it, before converting the binary to characters.</p>"},{"location":"cryptography/rsa/#example","title":"Example","text":"<p>Lets take the simple example of the message \"YO\" and encrypt it. First we need to generate all of our key components. Lets use P = 5 and Q = 11. Therefor: <pre><code>N = P * Q = 5 * 11 = 55\nT = (P-1)*(Q-1) = 4 * 10 = 40\nE = 13 (note E could be other numbers that meet the requirements but we chose this one to keep it simple)\n</code></pre> Next we need to calculate D. This is done using the Extended Euclidean Algorithm. See the bottom of this page for additional resources on the Extended Euclidean Algorithm. It looks like this: <pre><code>A = 13 B = 40\n1) 40 = 3*13 + 1\n2) 13 = 13*1 + 0\n----------------\n1 = 40 - 3*13\n  = (1)40 + (-3)13\n\n(-3) + 40 = 37\nD = 37\n</code></pre> We can test this is correct by checking the congruency of the equation: <pre><code>E * D = 1(mod T)\n\n13 * 37 = 1(mod 40)\n\n(13*37) - 1 = 480 which is a factor of 40\n</code></pre> So D = 37</p> <p>We now have all of the components. Are public key is comprised of E and N (13, 55) and our private key is D and N (37, 55)</p> <p>Now to encrypt our message:</p>"},{"location":"cryptography/rsa/#encrypt","title":"Encrypt","text":"<p>To encrypt we use the encryption algorithm above, but first we must convert this message to binary. In binary the message \"YO\" looks as follows:</p> <p><pre><code>010110001 01010001\n   Y         O\n</code></pre> Normal ASCII uses 8-bits which is much larger then our N value. Thus we must calculate the maximum that can be used for our N value. By calculating Math.floor(Log(55)/Log(2)) we get 5. So our M value must be 5 bits long. Recutting our message into 5-bit segments looks like this <pre><code>01011 00101 01000 10000\n</code></pre> Note that we have appended 4 zero's onto the end of the last segment. This is to make it even out into 5 bits. for RSA, POST appending is the most effective way to even out the segments. When decryption occurs we will be able to determine what to remove as the message will be regrouped into 8-bit segments as before. When this happens we will have leftovers and know to drop it.</p> <p>Now we take the integer values those binary numbers represent and run them through the equation. After encrypting, we left pad the binary until it contains Math.ceil(Log(55)/Log(2)) = 6 bits. It looks like this: <pre><code>01011 00101 01000 10000\n  11    5     8      16\n\n11^13(mod 55) = 11 = 001011\n5^13(mod 55) = 15 = 001111\n8^13(mod 55) = 28 = 011100\n16^13(mode 55) = 26 = 011010\n</code></pre></p> <p>We now have our encrypted message: <pre><code>001011001111011100011010\n</code></pre></p>"},{"location":"cryptography/rsa/#decrypt","title":"Decrypt","text":"<p>To decrypt the above encrypted message we use the D and the N value in another equation. Again using N we can calculated how to split up the binary. Using Match.ceil(Log(55)/Log(2)) = 6 bits. We cutup the binary into 6-bit segments. This looks as follows: <pre><code>001011 001111 011100 011010\n</code></pre> Using then the decryption equation we can convert these segments back to their plaintext values. Afterwards we calculate Math.floor(Log(55)/Log(2)) = 5 bits to determine how much to pad the plaintext values. <pre><code>001011 = 11 = 11^37(mod 55) = 11 = 01011\n001111 = 15 = 15^37(mod 55) = 5 = 00101\n011111 = 28 = 28^37(mod 55) = 8 = 01000\n011010 = 26 = 26^37(mod 55) = 16 = 1000\n</code></pre> Now to convert back we append all of these together and parse them into 8-bit ASCII <pre><code>0101100101010001000\n\n01011001 01010001   000\n   Y        O     [extra]\n</code></pre> Note the extra we had talked about that would be appended. We know we can get rid of this because it does not equate to 8-bits of a valid ASCII character and it is also all zeros.</p>"},{"location":"cryptography/rsa/#python-code-example","title":"Python Code Example","text":"<p>Below is a python walkthrough example of an encryption and then decryption using python. Note that this implementation is not the most efficient implementation as it converts the binary representation of the numbers into string, thus allowing each binary value to be accessed at an index. Conversion between binary and binary strings in python is quite trivial, and it does make the below example a bit more legible if you are unfamiliar with bit manipulation actions, but note this is at the expense of performance.</p> <p>The code sample below includes print statements, copy the the code snippet below and execute with Python3 to get output through each stage of the algorithm</p> <pre><code>import math\nimport sys\n\nunencryptedMessage = \"Hello World!\"\n\np1 = 5  # prime number 1\np2 = 11  # prime number 2\n\nt = 40  # totient. AKA (p1-1)*(p2-1)\nn = 55  # n. AKA p*q - part of public key\ne = 13  # e. prime number - part of public key\nd = 37  # d. private key\n\n\nplaintext_message_seg_length = int(math.floor(math.log(float(n), 2)))\nencrypted_message_seg_length = int(math.ceil(math.log(float(n), 2)))\n\nprint(\"Plaintext: \" + str(plaintext_message_seg_length))\nprint(\"Encrypted: \" + str(encrypted_message_seg_length))\nprint(\"Original Message: \" + unencryptedMessage)\n\n# convert the message to all binary bits - padd out to make sure they all are 8 bits long for the character\nbinaryUnencryptedMessage = ''.join(format(ord(x), '08b') for x in unencryptedMessage)\nprint(binaryUnencryptedMessage)\n\n# post pad the string to get an even number\nwhile len(binaryUnencryptedMessage) % plaintext_message_seg_length != 0:\n    binaryUnencryptedMessage += '0'\n\nprint(binaryUnencryptedMessage)\n\n# split it up into segments of plaintext_message_seg_length\nunencryptedMessageSegments = list()\nfor i in range(0, len(binaryUnencryptedMessage), plaintext_message_seg_length):\n    unencryptedMessageSegments.append(binaryUnencryptedMessage[i: i + plaintext_message_seg_length])\n\nprint(unencryptedMessageSegments)\n\n#encrypt each segment using RSA\nencryptedMessageSegments = list()\nfor i in unencryptedMessageSegments:\n    print(\"------------------\")\n    print(i)\n    segmentInt = int(i, 2)  # converts string to int, interpreting it as in base 2\n    print(str(segmentInt) + \" - \" + bin(segmentInt))\n    encryptedSegmentInt = (segmentInt ** e) % n\n    print(str(encryptedSegmentInt) + \" - \" + bin(encryptedSegmentInt))\n    encryptedSegmentBinary = format(encryptedSegmentInt, '0' + str(encrypted_message_seg_length) + 'b')\n    print(encryptedSegmentBinary)\n    encryptedMessageSegments.append(encryptedSegmentBinary)\n\n\nprint(\"***********************\")\nprint(encryptedMessageSegments)\nencryptedMessageBinaryString = ''.join(encryptedMessageSegments)\nprint(encryptedMessageBinaryString)\n\nencryptedMessageInt = int(encryptedMessageBinaryString, 2)\nprint(encryptedMessageInt)\nprint(bin(encryptedMessageInt))\n\n\nencryptedMessage = encryptedMessageInt.to_bytes(byteorder=sys.byteorder,\n                                            length=math.ceil(len(encryptedMessageBinaryString) / 8 ))\nprint(encryptedMessage)\n\n# -- AT THIS POINT THE MESSAGE IS ENCRYPTED AS A BYTE ARRAY--\n\nnumber = int.from_bytes(encryptedMessage, byteorder=sys.byteorder, signed=False)\nprint(number)\n\nprint (\" ** BEGINNING DECRYPTION **\")\n\nbinaryEncryptedMessage = str(bin(number))[2:]\nprint(binaryEncryptedMessage)\n\n# pre pad encrypted until is appropriate length to be cut up\nwhile len(binaryEncryptedMessage) % encrypted_message_seg_length != 0:\n    binaryEncryptedMessage = '0' + binaryEncryptedMessage\n\n# cut into decryptable segments\nencryptedMessageSegments = list()\nfor i in range(0, len(binaryEncryptedMessage), encrypted_message_seg_length):\n    encryptedMessageSegments.append(binaryEncryptedMessage[i: i + encrypted_message_seg_length])\n\nprint(encryptedMessageSegments)\n\nunencryptedSegments = list()\nfor i in encryptedMessageSegments:\n    print(\"------------\")\n    segmentInt = int(i, 2)  # converts string to int, interpreting it as in base 2\n    print(i)\n    print(str(segmentInt) + \" - \" + bin(segmentInt))\n    unencryptedSegmentInt = int((segmentInt ** d) % n)\n    print(unencryptedSegmentInt)\n\n    # left pad with 0 to return segment to decrypted segment length\n    unencryptedSegmentBinary = format(unencryptedSegmentInt, '0' + str(plaintext_message_seg_length) + 'b')\n    print(unencryptedSegmentBinary)\n    unencryptedSegments.append(unencryptedSegmentBinary)\n\nprint(unencryptedSegments)\njoinedSegments = ''.join(unencryptedSegments)\nprint(joinedSegments)\n\n\nletters = list()\nfor i in range(0, len(joinedSegments), 8):\n    letters.append(joinedSegments[i: i + 8])\n\nprint(letters)\n\nplainMessage = \"\"\nfor letter in letters:\n    letterInt = int(letter, 2)\n    character = chr(letterInt)\n    plainMessage += character\n\nprint(plainMessage)\n</code></pre>"},{"location":"cryptography/rsa/#extended-euclidean-algorithm","title":"Extended Euclidean Algorithm","text":"<p>Additional Resources:</p> <ul> <li>https://en.wikipedia.org/wiki/Extended_Euclidean_algorithm</li> <li>https://en.wikibooks.org/wiki/Algorithm_Implementation/Mathematics/Extended_Euclidean_algorithm</li> <li>http://www.mast.queensu.ca/~math418/m418oh/m418oh04.pdf</li> </ul>"},{"location":"development/android/","title":"Android Development Guide","text":"<p>The following page is a collection of helpful and useful tricks for Android App Development and general Android use. This can include apps, tools, and how-tos when developing certain android components</p>"},{"location":"development/android/#design","title":"Design","text":"<ul> <li>Build with Asynchronous in Mind</li> <li>Modularize as much as possible<ul> <li>Android likes to create heavy cupling with the UI which is not maintainably desirable</li> <li>Sometimes heavy coupling is unavoidable</li> </ul> </li> <li>Interfaces and callbacks are primary method of communication between components - unless you want to use Broadcasts or Messages<ul> <li>Service &lt;-&gt; Activity</li> <li>Activity &lt;-&gt; Fragment</li> </ul> </li> <li>Spaghetti Code Is A Serious Problem</li> </ul>"},{"location":"development/android/#development","title":"Development","text":"<p>General Tips: * Use try/catch a fair amount. Errors show up in odd places * Use isFinishing in activities, to ensure UI changes don't occur when the Activity is being deconstructed * finish() and startActivity() are asynchronous - onDelete() is not gauranteed to be called or completed before onCreate() for the new activity is run</p>"},{"location":"development/android/#services","title":"Services","text":"<p>Multiple simultanious services can largely cause a mess, and is not a recommended approach when developing applications.</p> <p>When creating services using the <code>bindService</code> and <code>unbindService</code> calls - always call then in context of the <code>applicationContext</code>. Thus making these calls as such</p> <pre><code>//Kotlin code\napplicationContext.bindService()\napplicationContext.unbindService()\n</code></pre> <p>This is because binding events take the current context when determinging what services need to be started or stopped (and used to check if its leaking). With multiple bind and unbind calls you may end up with some bind soup where an unbind won't unbind because the context which the bind call was made in was different. Using the applicationContext you can avoid this problem</p>"},{"location":"development/android/#broadcast-receivers","title":"Broadcast Receivers","text":"<p>If registering a service to receive broadcasts, its best to make the register call in the Service's <code>onCreate</code> call and possibly again in the <code>onStartCommand</code> call. Calling <code>registerReceiver</code> using the <code>applicationContext</code> is also recommended so your calls should look like this</p> <pre><code>//Kotlin code\napplicationContext.registerReceiver()\napplicationContext.unregisterReceiver()\n</code></pre> <p>This is again to avoid the context soup problem described when binding and unbinding services</p>"},{"location":"development/raw-sockets/","title":"Raw Sockets","text":"<p>The C++ Raw Sockets library is powerful, complicated and largely poorly documented. Numerous examples show inconsistent methods of solving a single solution and not many of these solutions seem to be able to resolve all functionality of Raw Sockets in a relatively clean and consistent way. This is what this article intends to try and clear up by describing a consistent method of developing, exploring and learning how to use all areas of Raw Sockets.</p> <p>This documentation will cover largely the C libraries and will display examples using the C/C++ library. Much of Raw Sockets from here is the same in any language, and after studying Raw Sockets in C/C++ it is highly transferable to other languages and their libraries such as Python, Ruby or Java</p>"},{"location":"development/raw-sockets/#create-a-raw-socket","title":"Create A Raw Socket","text":"<p>Creating a standard socket is typicaly done like this: <pre><code>int socket = socket(AF_INET, SOCK_STREAM, 0);\n</code></pre> This would create a IPv4 TCP Socket. The 3 parameters passed to the socket function all have special purposes and values. From the linux documentation the socket methods parameters are listed as: <pre><code>int socket(int domain, int type, int protocol);\n</code></pre> The domain parameter handles the version, the type handles the type of protocol and the protocol parameter largely depends on the previous two parameters. This third parameter becomes of great use when using Raw Sockets as it dictates how the stack should react when sending or receiving packets. Below is a list of the various options that can be passed to each of these parameters. Note that this list is not comprehensive</p>"},{"location":"development/raw-sockets/#domain","title":"Domain","text":"<ul> <li><code>AF_INET</code> - Creates an IPv4 Socket</li> <li><code>AF_INET6</code> - Creates an IPv6 Socket</li> </ul>"},{"location":"development/raw-sockets/#type","title":"Type","text":"<ul> <li><code>SOCK_STREAM</code> - Creates A TCP Socket</li> <li><code>SOCK_DGRAM</code> - Creates A UDP Socket</li> <li><code>SOCK_RAW</code> - Creates A Raw Socket</li> </ul>"},{"location":"development/raw-sockets/#protocols","title":"Protocols","text":"<ul> <li><code>0</code> - This seems to create default behavior, if SOCK_DGRAM or SOCK_STREAM is used, the network stack will handle and generate an appropriate IP Header for the underlying protocol</li> <li><code>IPPROTO_RAW</code> - When used with SOCK_RAW this tells the stack by default that this is a Raw Socket and that the IP packet data will be supplied by the client</li> <li><code>IPPROTO_TCP</code> - When used with SOCK_RAW this tells the stack the data inputed will be TCP data. The network stack will create the IP Header data itself and will set the Protocol section of the IP Header to TCP</li> <li><code>IPPROTO_UDP</code> - When used with SOCK_RAW this tells the stack the data inputed will be UDP data. The network stack will create the IP Header data itself and will set the Protocol seciton of the IP Header to UDP</li> </ul>"},{"location":"development/raw-sockets/#sources","title":"Sources","text":"<ul> <li>Linux Raw Socket Man Page: http://man7.org/linux/man-pages/man7/raw.7.html</li> <li>Linux Socket Man Page: http://man7.org/linux/man-pages/man2/socket.2.html</li> <li> <p>Linux IPv4 Implementation Man Page: http://man7.org/linux/man-pages/man7/ip.7.html</p> </li> <li> <p>Raw Sockets in C Code: http://www.binarytides.com/raw-sockets-c-code-linux/</p> </li> </ul>"},{"location":"guides/mac-fixes/","title":"Mac Fixes","text":""},{"location":"guides/mac-fixes/#disable-git-credential-caching-from-osx-keychain","title":"Disable Git Credential Caching From OSX keychain","text":""},{"location":"guides/mac-fixes/#problem","title":"Problem","text":"<p>Receive 403s from git when applying actions to CodeCommit or GHE accounts. Seemingly happens spontaniously, though generally occurs every 8-24hrs</p>"},{"location":"guides/mac-fixes/#cause","title":"Cause","text":"<p>This problem occurs when switching between numerous CodeCommit and GHE accounts. The error that happens is also extremely unclear. You simply just get 403's all the time. Its because git by default caches the credentials being used, but credentials being used for GHE and CodeCommit are temporary and expire anywhere from 8 to 24hrs.\u00a0</p>"},{"location":"guides/mac-fixes/#quick-fix","title":"Quick Fix","text":"<p>The quick fix solution is to delete the entries from keychain:</p> <ol> <li>Type CMD + Spacebar to open search</li> <li>Search \"keychain\" and hit enter</li> <li>Select \"login\" under \"Keychains\" on the top right menu</li> <li>Select \"All Items\" under \"Category\" on the bottom right menu</li> <li>In the search on the top right search \"codecommit\", \"git\", and \"ghe\"</li> <li>Delete all the entries returned from those search queries</li> <li>All cached credentials for git have now been removed</li> </ol>"},{"location":"guides/mac-fixes/#permanent-fix","title":"Permanent Fix","text":"<p>There are a couple of ways to apply this. A previous trick by Martin Ho in his docs has solved this originally :\u00a0Stuff Martin Knew#GitConfiguration\u00a0. But this solution only works if you are then manually configuring git's /.git/config for every single repository you are cloning. When wanting to take advantage of git's more default configurations and hierarchy, more needs to be done. Complete the following to configure git to stop using OSX Keychain cache:</p> <p>1) Open terminal and run the following command <pre><code>git config --get-all --show-origin credential.helper\n</code></pre> This will show you all the layers of configuration of the credential.helper module. You will likely see the following entry: <pre><code>file:/usr/local/git/etc/gitconfig       osxkeychain\n</code></pre> 2) Open the file <code>/usr/local/git/etc/gitconfig</code> and comment out the following line <pre><code>[credential]\n    helper = osxkeychain\n</code></pre> 3) Save changes and close</p> <p>4) Open the file <code>/Library/Developer/CommandLineTools/user/share/git-core/gitconfig</code> and comment out the following line <pre><code>[credential]\n    helper = osxkeychain\n</code></pre> 5) Save changes and clone</p> <p>6) (Optional) Add as an added bonus. I would recommend still applying martin's fix to enforce a default \"no credential helper defined\" across all projects. This can be done simply by putting the following code block at the top of your <code>/etc/gitconfig</code> and <code>~/.gitconfig</code> files: <pre><code>[credential]\n    helper = \"\"\n</code></pre> Any credential blocks created further down the <code>/etc/gitconfig</code> or <code>~/.gitconfig</code> files will still be valid, but by default nothing created before hand will be valid. This will help enforce removal of any default osxkeychain caching</p> <p>7) Rerun the following command to verify changes were made. You should no longer see the oskeychain entry in the response <pre><code>git config --get-all --show-origin credential.helper\n</code></pre></p>"},{"location":"guides/virtualbox/","title":"Convert VirtualBOX VDI to Hyper-V VHD","text":"<p>Conversion is very simple although it requires the instalation of full VirtualBox. To make the conversion you need to use VirtualBoxManage Tool. VirtualBoxManage is not available by itself as of this writing</p> <ol> <li>Install Virtual Box Here: https://www.virtualbox.org/wiki/Downloads</li> <li>If you are on Windows VirtualBoxManage Tool will be located in <code>C:\\ProgramFiles\\Oracle\\VirtualBox</code> <ul> <li>It is easiest to add this directory to your system path as it makes the conversion command easier as your source and destination paths can be relative to the directory you call virtualboxmanage from</li> </ul> </li> <li>Enter Command:      <pre><code>vboxmanage clonehd &lt;source .vdi file&gt; &lt;destination .vhd file&gt; --format VHD\n</code></pre> Example: <pre><code>vboxmanage clonehd WinXP.vdi F:\\winxp.vhd --format VHD\n</code></pre></li> <li>Wait for it to complete. For about 10 gigs it will take about 2 minutes. 100gigs can take roughly 30min</li> </ol>"},{"location":"guides/virtualbox/#resources","title":"Resources","text":"<ul> <li>http://www.sysprobs.com/vdi-vhd-convert-virtualbox-virtual-machines-virtual-pc</li> </ul>"},{"location":"guides/wireless-telecom/","title":"Wireless Telecom","text":"<p>EARFCN Calculator: http://niviuk.free.fr/lte_band.php</p> <p>Everything Else: http://www.sharetechnote.com/</p>"},{"location":"kubernetes/","title":"Kubernetes","text":"<p>Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation</p> Website Description Link KubeTools An extensive list of Kubernetes Tools https://collabnix.github.io/kubetools/ AWS EKS Best Practices Documentation https://docs.aws.amazon.com/eks/latest/best-practices/introduction.html AWS EKS Workshop https://www.eksworkshop.com/ K8 AF Failure stories and lessons learned from K8s https://k8s.af/"},{"location":"kubernetes/configuration-management-tools/","title":"Configuration Management Tools","text":"<p>The following doc is a breakdown of some of the different configuration management tools out there for kubernetes, along with comments and considerations on their uses</p> <p>In summary, the following are some known tools that exist:</p> Name Description Links <code>helm</code> <code>kustomize</code> <code>jsonnet</code> / <code>ksonnet</code> Builds on the shortcomings of YAML/JSON (used in Helm and Kustomize) where there is no programmable logic such as reusable variables <code>kapitan</code> <code>cdk8s</code> Developed by AWS, allows you to generate manifests using software languages <code>cuelang</code> <p>The main debate is always, whats the difference between them all ? And which one should you use ? This varies on your use case I think.</p>"},{"location":"kubernetes/configuration-management-tools/#helm-vs-kustomize","title":"Helm vs Kustomize","text":"<p>These two are the most commonly debated between and they both approach manifest management in slightly different ways. But they also come with their limitations. For small / medium scale projects, either of these will probably do.</p> <p>A major defining difference between the two is Helm keeps template control within the package itself, so the end user can only modify it from the available <code>values.yaml</code>. Kustomize gives the end user full control as they can write overlays for absolutely everything within it. This functionality is most prominent when you have hierarachies of manifests or multiple use-cases being needed for a single application</p> <p>In Helm, all use-cases (whatever your Helm chart is designed to do) have to be accomodated within the template files and be accessible / configurable with configuration. In Kustomize, this is not an issue, as the end user can modify the Manifests as they wish. Kustomise, though means that the end user must define everything they want to override, and do so correctly to still work with the original application. </p> <p>Basically, when you are using Helm charts, developed by others, you do not have to worry about what configurations work, because they will not be available to you, BUT you are limited only to what has been defined as possible by the Helm chart. When using Kustomize, you have the freedom to add and develop and configure whatever you want ontop of the original Kustomize, but it is then up to you to make sure the application still works.</p>"},{"location":"kubernetes/configuration-management-tools/#why-use-cue-cuelang","title":"Why use CUE / cuelang","text":"<p>DevOps Toolkit does a very good breakdown of the requirements and reasons to choose cuelang: https://www.youtube.com/watch?app=desktop&amp;v=m6g0aWggdUQ&amp;t=0s</p> <p>cuelang can also work to provide a missing feature in Helm - the ability to have variables in the <code>values.yaml</code>. This provides more dynamic definitions and saves typing writing configuration over and over as the <code>values.yaml</code> is just yaml, and doesn't offer any common variables, conditions or ability to share logic. cuelang can be used to write your entire application configurations, but if you already are setup with Helm, you can use it to fill in this known shortcoming of Helm, and migrate over eventually as you go.</p> <p>One of the major points DevOps Toolkit makes of using cuelang over Helm and Kustomize, is that both of these systems are text template parsers, they don't work with YAML, they don't recognise its YAML underneath, and thus also do not validate the YAML produced from themselves. These means you can have highly organised incorrect and invalid YAML. Which is pretty easy to become a debugging nightmare.</p> <p>CUE / cuelang is a language that was created without any influence or intention of K8s. There are a few tools that have evolved cuelang further for use with kubernetes:</p> <ul> <li>Timoni</li> <li>KCL</li> </ul>"},{"location":"kubernetes/container-runtimes-and-tools/","title":"Container Runtimes &amp; Their Tools","text":"<p>Container runtimes are the engines that run your containers. Different container runtimes are designed for different purposes and can have varying compliance with the Open Container Initiative and the Container Runtime Interface.</p> <p>The Open Container Initiative (OCI) and Container Runtime Interface (CRI) specifications are two standards for dictating container image packaging and kubernetes compatibility. OCI typically refers to the standard of formatting and packaging of container images, but in order for a container runtime to use images that follow the OCI standard, the runtime must be OCI compatible. If a runtime is to be used by Kubernetes, it must be CRI compliant.</p> <p>A summary of some common container runtimes and their compliance is listed in the following table:</p> Container Runtime Open Container Initiative (OCI) Compatible Container Runtime Interface (CRI) Compatible Runtime Is Used In ContainerD YES YES Docker, Most Cloud K8s Docker YES NO Docker CRI-O YES YES Minikube, OpenShift, Oracle rkt (deprecated) YES YES podman YES NO RHEL based systems, OpenShift"},{"location":"kubernetes/container-runtimes-and-tools/#open-container-initiative-oci","title":"Open Container Initiative (OCI)","text":"<p>OCI is a standardisation of bundling of container images. It was created to standardise images creation and execution. OCI packages are made up of two components:</p> Component Description imagespec contains information on how the image should be created runtimespec contains information on how to run the image <p>TL;DR</p> <ul> <li>If the runtime is OCI compliant, this means it follows cross compatible standard when it builds container images. </li> <li>If the runtime is OCI compatible, this means it can execute OCI compliant images</li> </ul> <p>Pretty much all images these days are OCI compliant and thus most runtime engines, are at minimum OCI compatible.</p>"},{"location":"kubernetes/container-runtimes-and-tools/#container-runtime-interface-cri","title":"Container Runtime Interface (CRI)","text":"<p>CRI is another standardisation specifically from K8s defining how K8s will communicate with runtime systems, allowing development of alternative runtimes that can work with K8s. Kubernetes used to only work with Docker as its container runtime. This was eventually abstracted and thus the Container Runtime Interface was created.</p> <p>With the creation of the CRI, tooling was also made to test out and debug CRI implementations. The <code>crictl</code> is a CLI tool for interacting with CRI compatible services using the container runtime defined interface</p> <p>TL;DR</p> <p>If something is CRI compliant, this means it is compatible to be run by Kubernetes as a runtime engine</p>"},{"location":"kubernetes/container-runtimes-and-tools/#containe-runtime-toolings","title":"Containe Runtime Toolings","text":"<p>Each of these runtimes do not have to run in kubernetes to run. Many of them can be used independently. In order to interact with them outside of kubernetes, these runtime engines come with a handful of CLI tools for working with them. CRI compatibility offers an extra bonus of also allow cross-compatibility where CLI tools can work on multiple container runtimes. </p> <p>A list of runtimes, compatible CLI tools and details about each tool is listed below:</p> CLI Tool Description Runtimes Compatible With <code>docker</code> CLI For interacting with Docker Docker <code>ctr</code> Built in debug CLI for ContainerD ContainerD <code>nerdctl</code> Docker-like general purpose CLI for ContainerD ContainerD <code>crictl</code> CRI Compatible CLI for debugging and testing CRI compliant container runtimes ContainerD, CRI-D, rkt"},{"location":"kubernetes/container-runtimes-and-tools/#a-big-map","title":"A Big Map","text":"<p>Below is a diagram of a bunch of the toolings all listed above and how they relate and interact with eachother</p> <pre><code>graph TD\n    K[docker CLI]\n    A[Docker];\n    B[ContainerD];\n    E[CRI-O];\n    F[nerdctl];\n    G[ctr];\n    H[crictl];\n    I[rkt];\n    J[podman];\n\n    K --&gt; | Interacts With | A;\n    A &lt;--&gt; | Is Bundled With | B;\n    G --&gt; | CLI To Debug | B;\n    F --&gt; | General Purpose CLI For | B;\n    H --&gt; | CLI for CRI compatible services | E;\n    H --&gt; | CLI for CRI compatible services | I;\n    H --&gt; | CLI for CRI compatible services | B;\n</code></pre>"},{"location":"kubernetes/kubeadm-kubespray-kops/","title":"KOPS vs Kubeadm vs Kubespray","text":"<p>All three are are setting up Kubernetes clusters, but each is intended for different use cases</p> Service Usage Links <code>kops</code> For setting up K8 clusters in cloud environments. Integrating with AWS, GCP, etc <code>kubeadm</code> Meant to be a minimum production ready installation of K8s. It contains no tools for handling infrastructure it is running on <code>kubespray</code> A production ready deployment meant to be generic and work on all kinds of environments from bare-metal to cloud. Includes handling and creation of infrastructure for its deployment. Uses Ansible to automate provisioning of infrastructure and components"},{"location":"kubernetes/kubernetes-on-hyperv/","title":"Setup A Kubernetes Cluster on Hyper-V","text":""},{"location":"kubernetes/kubernetes-on-hyperv/#configure-virtual-machines","title":"Configure Virtual Machines","text":""},{"location":"kubernetes/kubernetes-on-hyperv/#hyper-v-configuration","title":"Hyper-V Configuration","text":"<p>From the Hyper-V Manager you will be creating 3 new Virtual Machines. 1 Virtual Machine will be the master machine, and 2 others will be minion/node machines. This setup is a basic minimum setup for configuring a kubernetes cluster</p> <ol> <li>From the UI select New &gt; Virtual Machine</li> <li>Click Next if prompted the \"Before You Begin\" section</li> <li>Set the virtual machine name. Note that each virtual machine MUST have a unique name. A recommended naming scheme is kmaster1 for the master node and kminion1 and kminion2 for the minions. If you choose to expand your cluster later, you can simply add each vm in their appropriate roles and increment their number value</li> <li>Select \"Generation 2\"</li> <li>Assign a minimum of 2 GB of RAM. Recommended is 4GB. Anything less than 2GB your kubernetes cluster will struggle just to run on its own. Also check the \"Dynamic Memory\" option as this will release memory to the system if the virtual machine is not requiring it. This is especially useful if your hosting system has limited resources</li> <li>Configure your networking by selecting the a public switch from the drop-down. Your virtual machines will need an internet connection in order to update and install aswell as Kubernetes in order to contact all of its nodes or use features such as Helm or Kubeapps. Using a public switch is the easiest way to avoid connectivity issues</li> <li>Create a virtual hard disk. The default size is sufficient, 200GB is ideal</li> <li>Install the operating system from an .iso. For this tutorial we use Debian 9.6. You can download a Debian iso from here: https://www.debian.org/distrib/netinst. These are net install images which will require and internet connection to complete the operating system installation</li> <li>Select Finish, and then select the VM from the list and open Settings</li> <li>Select \"Security\" from the Hardware section and uncheck \"Enable Secure Boot\". This feature does not work well with Linux on Hyper-V</li> <li>Select \"Processor\" from the Hardware section and set the \"Number of virtual processors\" at minimum to 2. The more CPUs you can provide the better. It is also not an issue to overlap or assign all CPUs on your host provider to each of the kubernetes virtual machines.</li> <li>Select \"Advanced Features\" under the \"Network Adapter\" section. You may need to expand \"Network Adapter\" to see the \"Advanced Features\". Selecting \"Advanced Features\" set the \"MAC Address\" to \"Static\". Kubernetes uses the MAC address and IP to uniquely identify each of its nodes. Hyper-V though will change the MAC address after reboot. You can use https://www.miniwebtool.com/mac-address-generator/ to generate MAC addresses for you to fill in</li> <li>Select \"OK\" at the bottom to close and submit your settings</li> </ol> <p>You have now configured 1 of the 3 Virtual Machines for your kubernetes cluster. Repeast these steps for the other nodes in the cluster. Remember to give unique name and MAC addresses to each virtual machine created</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#router-configuration","title":"Router Configuration","text":"<p>Kubernetes requires each node to have a unique hostname, MAC and IP address in order to uniquely identify and contact each member of the cluster. By assigning unique names in Hyper-V and static MAC addresses we have already resolves 2 of these 3 requirements. The last requirement is to assign static IP addresses to each of the virtual machines. This is done by configuring static routing or DHCP reservation in your router where Hyper-V shares the network with. We configured each virtual machine to use the public switch - which means each virtual machine will pretend to be a device on the Hyper-V host machines' network. This means the router will be in charge of assigning IP addresses.</p> <p>Access your network router and configure either static routing which will enforce assignment of static IPs to every device in the network or DHCP reservation. DHCP reservation is forcing certain IPs to be assigned to certain machines based on their MAC address. This setup is likely how your own router is configured. Refer to your routers manual or help on how to configure these settings. Having assigned the static MAC addresses and machine names in Hyper-V you can setup the static routing or DHCP reservations before even starting the VMs. This will ensure everything is configured properly before any of the installation process begins.</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#debian-9-configuration","title":"Debian 9 Configuration","text":"<p>On bootup of the Virtual Machines, the Debian 9 installer will launch. Start the graphical install. Configure the system to have a minimal install. Configure HDD partitioning to put everything on the same partition. Make sure the installation has no GUI as this will consume resources that will not be taken advantage of. This is especially important if your virtual machine is running with limited resources</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#check-ip-and-mac","title":"Check IP and MAC","text":"<p>Once you have setup static routing or DHCP reservation and completed the Debian 9 install, check that your IP and MAC have been assigned correctly. Check with the following command:</p> <p><pre><code>sudo ip addr\n</code></pre> Check the listed interfaces that \"eth0\" has been assigned the IP configured in your router and the MAC configured in Hyper-V</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#configure-software-on-virtual-machines","title":"Configure Software On Virtual Machines","text":"<p>You have now setup 3 Virtual Machines in Hyper-V. One is labelled as the master and two others labelled as mionion/nodes. Next we need to apply the bare bones software packages and system configurations needed for these virtual machines to work in a cluster. Apply the following steps to all 3 Virtual Machines:</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#updateupgrade-and-install-ssh","title":"Update/Upgrade And Install SSH","text":"<p>Installing SSH on each node is ideal for remote management of the nodes, especially since you will be flipping through them alot during the remainder of this tutorial. Opening more ports, especially on a production build, is a risk but comes with added ease of management. SSH can also be easily disabled and removed after completion of the install</p> <p>Before any install with new machines, its also good to make sure your nodes are up to date. Execute the following commands to check and install updates for your Debian 9 nodes <pre><code>sudo apt-get update\nsudo apt-get upgrade -y\n</code></pre></p> <p>After updating, install ssh server <pre><code>sudo apt-get install openssh-server\n</code></pre></p> <p>In the latest version of Debian (Debian 9.6 as of this writing), ssh is already included. Executing this command may prompt that it is already installed</p> <p>Next, open <code>sshd_config</code> in nano and add or edit the following setting:</p> <pre><code>PermitRootLogin yes\n</code></pre> <p>This setting will allow the root user to login over ssh to the system. For best security practices, it is recommended you disable this setting later. Setting this up is purely for convenience. Restart SSH to apply your setting changes <pre><code>sudo systemctl sshd restart\n</code></pre></p> <p>You can now access your nodes over SSH!. If you are on linux you can simply connect using ssh in terminal, or for Windows install Putty. This is primarily a convenience feature for all future steps of the installation process</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#install-docker","title":"Install Docker","text":"<p>Install Docker on Debian 9 with the following commands:</p> <pre><code>sudo apt update\nsudo apt install apt-transport-https ca-certificates curl gnupg2 software-properties-common\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable\"\nsudo apt update\napt-cache policy docker-ce\nsudo apt install docker-ce\n</code></pre>"},{"location":"kubernetes/kubernetes-on-hyperv/#install-kubernetes","title":"Install Kubernetes","text":"<p>Install Kubernetes for Debian 9 with the following commands:</p> <pre><code>apt-get update &amp;&amp; apt-get install -y apt-transport-https curl\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\ncat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list\ndeb https://apt.kubernetes.io/ kubernetes-xenial main\nEOF\napt-get update\napt-get install -y kubelet kubeadm kubectl\n</code></pre>"},{"location":"kubernetes/kubernetes-on-hyperv/#disable-swap","title":"Disable Swap","text":"<p>Before starting any nodes, make sure to disable swap. Execute the following command to find the location of your swap file first:</p> <p><pre><code>cat /proc/swaps\n</code></pre> Additionally execute: <pre><code>blkid\n</code></pre> To find the id of the swap file block</p> <p>Then disable swap on the system by entering: <pre><code>sudo swapoff -a\n</code></pre> This will temporarily disable it and anything using it. If you enter the command <code>cat /proc/swaps</code> again there should be nothing listed</p> <p>Next, open the file <code>/etc/fstab</code> and comment out any lines referring to the swap path mentioned. Also comment out any entries that match the same block id as the swap. It is possible there will be none.</p> <p>After commenting it out, delete the swap partition using parted. Execute the following commands</p> <pre><code>sudo apt-get install parted\nparted\n</code></pre> <p>Danger</p> <p>PLEASE READ NEXT PART CAREFULY AS IT MAY BRICK THE SYSTEM</p> <p>This starts up parted. Type <code>print</code> and then enter. This will list all the partitions on the system, look for the one with the same name listed when checking <code>cat /proc/swaps</code>. Note the number listed in the one column for that partition. Type <code>rm  and then enter. Confirm the action as you may be prompted by parted that the kernel will not know of the change. Ignore any warnings. Type <code>print</code> again to confirm successful removal of the parititon. Type <code>quit</code> to exit parted <p>At this point, reboot your server. You will notice the server takes a notable longer amount of time to boot. It is possible it will also fail to boot if you rebooted via a restart. A full shutdown and then startup may be required. Again boot up will take longer, this is because swap has been disabled and you have essentially disabled system local caching. If booting fails, you have likely deleted the wrong partition or commented out the wrong data in <code>/etc/fstab</code></p>"},{"location":"kubernetes/kubernetes-on-hyperv/#125-configure-iptables","title":"1.2.5 Configure IPTables","text":"<p>If reboot is successful, next edit <code>/etc/sysctl.conf</code> and add to the end of the file the following: <pre><code>...\n#Kubernetes\nnet.bridge.bridge-nf-call-iptables = 1\n...\n</code></pre> This allows bridged networks to have their traffic passed through iptables</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#126-start-docker-service","title":"1.2.6 Start Docker Service","text":"<p>Lastly, start the docker service: <pre><code>sudo systemctl enable docker.service\n</code></pre></p> <p>Once this is all complete, reboot your system again</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#127-create-checkpoints-in-hyper-v","title":"1.2.7 Create Checkpoints in Hyper-V","text":"<p>Upon this all being successfully applied to the master and all minion nodes. Its ideal at this point to create checkpoints in hyper-v. The boxes are ready now to start running as a kubernetes cluster. This is a good point to revert back to if there are any issues during setup. Creating a checkpoint can be done within hyper-v</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#13-configure-master-and-minion-nodes","title":"1.3 Configure Master and Minion Nodes","text":""},{"location":"kubernetes/kubernetes-on-hyperv/#131-master-node","title":"1.3.1 Master Node","text":""},{"location":"kubernetes/kubernetes-on-hyperv/#1311-initialization","title":"1.3.1.1 Initialization","text":"<p>Execute the following command on the master node to startup kubernetes:</p> <p><pre><code>sudo kubeadm init\n</code></pre> Upon that being successful you will be prompted with \"Your Kubernetes master has initialized successfully\". Below this is some important instructions.</p> <p>The first section contains the following commands: <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> Run these commands on the master node. This is to properly setup the config information generated by kubeadm. Do this now</p> <p>The second section contains a kubeadm command on how to have your nodes join the master node. It looks something like this <pre><code>kubeadm join 192.168.1.200:6443 --token **** --discovery-token-ca-cert-hash **********\n</code></pre></p>"},{"location":"kubernetes/kubernetes-on-hyperv/#1312-install-weave-net","title":"1.3.1.2 Install Weave Net","text":"<p>Before running this second section, kubeadm has not installed all the required components. There is still missing a networking addon. There are multiple options available but for this tutorial we will use Weave Net as it requires the least amount of configuration. You can view all other networking addons here: https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/. </p> <p>Execute the following command on the master node to install Weave Net:</p> <pre><code>kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"\n</code></pre>"},{"location":"kubernetes/kubernetes-on-hyperv/#132-minion-node","title":"1.3.2 Minion Node","text":""},{"location":"kubernetes/kubernetes-on-hyperv/#1321-join-master-node","title":"1.3.2.1 Join Master Node","text":"<p>Once Weave Net has successfully installed you can then run the kubeadm join command on all of the nodes. The nodes will then join the master node. Again, the join command will have been supplied in the terminal output when initializing the master node. It will look something like this:</p> <pre><code>kubeadm join 192.168.1.200:6443 --token **** --discovery-token-ca-cert-hash **********\n</code></pre>"},{"location":"kubernetes/kubernetes-on-hyperv/#133-validate-and-check-connection-healthstatus","title":"1.3.3 Validate And Check Connection Health/Status","text":"<p>Once all nodes have successfully joined. Execute the following command on the master node to check their health status: <pre><code>kubectl cluster-info\nkubectl get nodes\n</code></pre> cluster-info will show general endpoint information of the cluster. get nodes will then list the connection status of all nodes. If the nodes are not all listed as status \"Ready\" give it a couple minutes and try again. Otherwise enter <code>kubectl describe nodes</code> to get a more verbose log output</p> <p>Upon all of the nodes displaying the \"Ready\" status, your Kubernetes cluster is now built. From the master node using kubectl you can now execute commands to interact with your cluster.</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#configure-local-kubectl-to-access-outside-of-cluster","title":"Configure Local Kubectl To Access Outside Of Cluster","text":"<p>With the above setup complete, you are able to interact and use your cluster from within the master node. This may not be desirable, especially if you do not want to SSH into the machine, or keep SSH enabled on the cluster virtual machines at all. To resolve this issue, you can install and configure a kubectl on your local machine to access your cluster. There are better ways to setup this configuration using RBAC, but this a quick and easy method for home and private solutions.</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#install-kubectl","title":"Install Kubectl","text":"<p>Install Kubectl on your local machine. There are many ways to do this depending on your system. See https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl for details. For debain 9 this can be simply done in terminal by executing the following commands:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\necho \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre>"},{"location":"kubernetes/kubernetes-on-hyperv/#configure-kubectl","title":"Configure Kubectl","text":"<p>This solution simply copied the configuration setup in your master node to your local machine.</p> <p>On your local machine create the kubectl config directory: <pre><code>sudo mkdir $HOME/.kube\n</code></pre></p> <p>Then copy over the config file located in the same directory as created above on the master node. You are essentially copying from the file on the master node located at <code>$HOME/.kube/config</code> to your locale machine located also at <code>$HOME/.kube/config</code></p> <p>Next, set permissions for the config file so that your local kubectl client can access the config <pre><code>sudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre></p> <p>Your local kubectl is now configured to connect with your kubernetes cluster. You can test your connection works by calling <pre><code>kubectl cluster-info\nkubectl get nodes\n</code></pre> Giving you cluster info and node status'</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#install-kubernetes-dashboard","title":"Install Kubernetes Dashboard","text":"<p>The Kubernetes Dashboard is a useful minimal dashboard that lets you browse the current state of your Kubernetes cluster. It is extremely useful for debugging and early configuration.</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#add-rbac-roles-for-dashboard","title":"Add RBAC Roles For Dashboard","text":"<p>Before installing dashboard, you need to have configured the appropriate Roles and User permissions for the Dashboard to operate within. Create the following YAML files to configure RBAC:</p> dashboard-adminuser.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kube-system\n</code></pre> dashboard-rbac.yaml<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kube-system\n</code></pre> <p>Apply the above configuration templates using kubectl with the following commands: <pre><code>kubectl apply -f ./dashboard-adminuser.yaml\nkubectl apply -f ./dashboard-rbac.yaml\n</code></pre></p>"},{"location":"kubernetes/kubernetes-on-hyperv/#install-kubernetes-dashboard_1","title":"Install Kubernetes Dashboard","text":"<p>Install Kubernetes Dashboard with kubectl: <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml\n</code></pre></p>"},{"location":"kubernetes/kubernetes-on-hyperv/#access-kubernetes-dashboard","title":"Access Kubernetes Dashboard","text":"<p>The Kubernetes Dashboard is installed as non public facing, and thus requires the use of the kubectl proxy in order to access it. This requires kubectl to be configured and installed on your local system to and accessing your cluster. Access the dashboard by running the following commands in your local kubectl: <pre><code>kubectl proxy\n</code></pre> Note the terminal will hang after executing the command. With the command hanging, all applications from your kubernetes cluster are mapped to localhost. If you terminate the proxy, the mapping will be removed. With the call hanging, you can then access the dashboard at the following url on localhost: <pre><code>http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/\n</code></pre></p> <p>When loading the page you will be prompted to enter a token. This token has been configured with the installation and the RBAC configuration. You can get the value of this token by calling the following command</p> <pre><code>kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')\n</code></pre> <p>Copy the secret key into the field and you will be forwarded to the dashboard page. If the proxy is closed or token expiry occurs, you will need to reenter this secret key.</p> <p>You have now configured Kubernetes Dashboard in your kubernetes cluster</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#install-helm","title":"Install Helm","text":"<p>Helm is a kubernetes package manager that allows easy install of popular kubernetes deployable applications. Helm is made of 2 components, a client - which is installed on your local machine, and a server - which is installed inside the kubernetes cluster. Using the kubectl client configured on your local machine, Helm can automatically detect and install itself into the cluster and on your local machine on its own.</p> <p>Note as a prerequisite to installing Helm, you must have installed kubectl on your local system, allowing you to remotely access and control your kubernetes cluster from your local computer</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#add-rbac-role-for-helm","title":"Add RBAC Role For Helm","text":"<p>Helm can be installed and configured without RBAC but it is a preferred best practice for kubernetes installations and securing of helm specifically.</p> <p>Create the following yaml file to create the user account adn RBAC role</p> rbac-config.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: tiller\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: tiller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: tiller\n    namespace: kube-system\n</code></pre> <p>Apply the above configuration using kubectl with the following command: <pre><code>kubectl apply -f ./rbac-config.yaml\n</code></pre></p>"},{"location":"kubernetes/kubernetes-on-hyperv/#install-helm_1","title":"Install Helm","text":"<p>On your local machine run the following commands:</p> <pre><code>curl https://raw.githubusercontent.com/helm/helm/master/scripts/get &gt; get_helm.sh\n$ chmod 700 get_helm.sh\n$ ./get_helm.sh\n</code></pre> <p>This will download and install the helm client on your machine. To install the server portion of helm into your kubernetes cluster. Execute the following on your local machine aswell</p> <pre><code>helm init --service-account tiller\n</code></pre> <p>Note: 'tiller' is the name of the user account we created for RBAC earlier. Note that the install will still succeed if you do not include the <code>--service-account</code> parameter, but the installation will be less secure.</p> <p>After the command completes successfully helm will have been installed on your cluster. Helm works by using its local installation and the local kubectl configuration to find the location of your cluster and to map the corresponding piece. Uniquely, with this configuration, you can now also easily install and configure helm in your kubernetes cluster by calling helm on your local machine, just like as you can do with kubectl.</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#install-kubeapps","title":"Install Kubeapps","text":"<p>Source: https://github.com/kubeapps/kubeapps/blob/master/docs/user/getting-started.md</p> <p>Kubeapps is a UI page for browsing, installing and upgrading helm packages. It essentially works as a UI Helm package manager. Note that because of this, installing Helm is a prerequisite to installing Kubeapps</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#install-kubeapps_1","title":"Install Kubeapps","text":"<p>With Helm already installed, you can install kubeapps by running the following commands with helm: <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install --name kubeapps --namespace kubeapps bitnami/kubeapps\n</code></pre></p> <p>Note that this installation will take awhile, as several kubernetes pods have to be deployed, including a mongodatabase. To monitor the startup of kubeapps run the following <pre><code>kubectl get pods -w --namespace kubeapps\n</code></pre></p> <p>This will give you a live status of the kubeapps nodes as they are spawned and started up. If you see errors spawning, it is recommended to load the Kubernetes Dashboard to view startup errors. The kubernetes dashboard will also allow you to view pod status' and logs of each pod.</p> <p>An error that can occur is the mongodb pod will fail to start. The error is related to IPv6 as kubeapps assumes your kubernetes cluster supports it. If this is not the case, you will find your deployment failing. The solution is discussed further here: https://github.com/kubeapps/kubeapps/issues/479. From the Kubernetes Dashboard, find the deployment of the mongo database, edit the configuration and find the settings <code>MONGODB_ENABLE_IPV6</code>. By default this value is set to \"yes\", change this value to \"no\" and save. Kubernetes will automatically redeploy the mongodb node and after a few more minutes the deployment will heal. You can monitor this progress within the Kubernetes Dashboard or by calling</p> <pre><code>kubectl get pods -w --namespace kubeapps\n</code></pre>"},{"location":"kubernetes/kubernetes-on-hyperv/#configure-kubeapps","title":"Configure Kubeapps","text":"<p>The Kubeapps page will require an API token in order to access your kubernetes cluster. Create an account and api token with the following commands</p> <pre><code>kubectl create serviceaccount kubeapps-operator\nkubectl create clusterrolebinding kubeapps-operator --clusterrole=cluster-admin --serviceaccount=default:kubeapps-operator\n</code></pre> <p>Then retrieve the token for usage when accessing the dashboard with the following command:</p> <pre><code>kubectl get secret $(kubectl get serviceaccount kubeapps-operator -o jsonpath='{.secrets[].name}') -o jsonpath='{.data.token}' | base64 --decode\n</code></pre>"},{"location":"kubernetes/kubernetes-on-hyperv/#access-kubeapps","title":"Access Kubeapps","text":"<p>You can securely access the kubeapps dashboard by executing the following command:</p> <pre><code>export POD_NAME=$(kubectl get pods -n kubeapps -l \"app=kubeapps,release=kubeapps\" -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl port-forward -n kubeapps $POD_NAME 8080:8080\n</code></pre> <p>The above command will hang. You can access the kubeapps dashboard in your browser at http://127.0.0.1:8080. When the page loads you will be prompted to enter the kubernetes API token. Fetch the token as described earlier in \"Configure Kubeapps\" and enter it. Select Login and you will be brought to the Kubeapps home page</p> <p>you have now successfully installed and logged into Kubeapps! From here you can install additional tools and applications from within the GUI</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#install-metallb","title":"Install MetalLB","text":"<p>Resources: https://metallb.universe.tf/</p> <p>MetalLB is a loadbalancing software for barebone installations of kubernetes (This installation). The problem with kubernetes is that its loadbalancer service implementation is designed to work with cloud providers such as Azure or AWS and thus contains no functionality for barebone installations. kubernetes will allow you to create loadbalancer services, but it will never assign them a public IP, making it impossible to make your cluster application publicly accessible</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#installation","title":"Installation","text":"<p>MetalLB can be easily installed with Helm:</p> <pre><code>helm install --name metallb stable/metallb\n</code></pre>"},{"location":"kubernetes/kubernetes-on-hyperv/#configuration","title":"Configuration","text":"<p>MetalLB requires a ConfigMap to be setup before it will start working. The following is a minimum setup for MetalLB:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: metallb-system\n  name: config\ndata:\n  config: |\n    address-pools:\n    - name: default\n      protocol: layer2\n      addresses:\n      - 192.168.1.240-192.168.1.250\n</code></pre> <p>This implementation simply lets you specify the public ip range that is available on all of your kubernetes nodes. The above for example will make all ips from 240 to 250 available.</p> <p>When creating your services now simply use the <code>loadBalancerIP</code> parameter in the spec section of your LoadBalancer config yaml. Specify one of the IPs available in the range and create your service as usual. MetalLB comes with also error logging which will show up by typing <code>kubectl describe svc \"servicename\"</code>. It will print output to the logging output normaly presented in this command.</p> <p>Make sure to run the ConfigMap configuration above and then redeploy your LoadBalancers using the loadBalancerIP parameter and MetalLB will ensure they are assigned the specified IP. If you have LoadBalancers already deployed, you will have to redeploy them for MetalLB to notice.</p> <p>Note that MetalLB also does not need to be in the same namespace as the LoadBalancer in order to work</p>"},{"location":"kubernetes/kubernetes-on-hyperv/#resources","title":"Resources","text":"<ul> <li>http://vijayshinva.github.io/kubernetes/2018/07/28/setting-up-a-kubernetes-cluster-on-a-windows-laptop-using-hyper-v.html</li> <li>https://www.miniwebtool.com/mac-address-generator/</li> <li>https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-debian-9</li> <li>https://kubernetes.io/docs/setup/independent/install-kubeadm/</li> <li>https://serverfault.com/questions/684771/best-way-to-disable-swap-in-linux</li> <li>https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/storage_administration_guide/s2-disk-storage-parted-remove-part</li> </ul>"},{"location":"kubernetes/toolsuite/","title":"Tool Suite","text":"<p>A personal list of favourite tools to use and consider when working in the kubernetes ecosystem</p>"},{"location":"kubernetes/toolsuite/#essentials","title":"Essentials","text":"<p>Install these on your laptop for basically every day use, development, testing, debugging, etc</p> Name Description Links <code>kubectl</code> K8 API Tool <code>helm</code> Manifest packaging system <code>minikube</code> Local cluster environment service"},{"location":"kubernetes/toolsuite/#cli-addons","title":"CLI Addons","text":"<p>May be part of the essentials, but not necessary. Would not recommend if wanting to study for the CKA or CKD as you won't have these and they will make you lazy</p> Name Description Links <code>kubectx</code> Single command context switcher https://github.com/ahmetb/kubectx <code>kubens</code> Single command namespace switcher (bundled with kubectx) https://github.com/ahmetb/kubectx"},{"location":"kubernetes/toolsuite/#cluster-management","title":"Cluster Management","text":"<p>Tools for making cluster wide changes</p> Name Description Links <code>kube-ops-view</code> Terminal view of cluster activity <code>kubespray</code> Setup Production ready clusters https://github.com/kubernetes-sigs/kubespray/tree/master <code>kubeadm</code> Cluster Administration. CRUD nodes, etc. Does not create clusters"},{"location":"kubernetes/toolsuite/#validation-linting","title":"Validation / Linting","text":"<p>Tools for validating / verifying your Kubernetes manifests, helm charts, etc</p> Name Description Links <code>kubeval</code> Validate your kubernetes config files. Note: No longer maintained - <code>kubeconform</code> seems to be the replacement <ul><li>https://github.com/instrumenta/kubeval</li><li>https://github.com/yannh/kubeconform</li></ul> <code>tflint</code> Linting and validation for your Terraform. Includes plugins for cloud providers so it can validate things such as instance types and cloud specific configurations https://github.com/terraform-linters/tflint"},{"location":"kubernetes/toolsuite/#security","title":"Security","text":"<p>Security scanners and tools</p> Name Description Links <code>trivy</code> Works as a CLI tool but also can be installed as an operator in the cluster. Has a handful of scanning tools <code>kube-bench</code> CLI scanning tool to validate your cluster against CIS standard. Is integrated into <code>trivy</code> as well. https://github.com/aquasecurity/kube-bench"},{"location":"kubernetes/toolsuite/#cicd-tools","title":"CI/CD Tools","text":"Name Description Links ArgoCD Suite of CD tools including - ArgoCD, ArgoRollouts, ArgoWorkflows and ArgoEvents Flux Is a direct competitor with ArgoCD, however its not as actively maintained and the user base is more in favor of ArgoCD Keel A very simplified ArgoCD-like service. Uses annotations on <code>Deployments</code> to configure and track registry container changes and apply updates Jenkins The defacto CI tool. Java, large and clunky, but does do everything. Works well when linked with pipelines via Jenkinsfiles, and use ECS for spawning worker nodes https://keel.sh/ Drone Similar to Concourse, but appears even simpler. Has quite a bit of integration with Digital Ocean, Gitea, Github, Bitbucket etc. Looks very small and written all in go https://www.drone.io/ Concourse A self-hosted like CircleCI or Github Actions tool. Simple YAML based with container environments for building and deploying services. Doesn't have much best practices or abilities to handle running at scale (common libs)"},{"location":"kubernetes/toolsuite/#local-development-k8-tools","title":"Local Development K8 Tools","text":"<p>A number of tools for building clusters on your local machine. These are alternatives to <code>minikube</code></p> Name Description Links <code>kind</code> Builds k8 clusters all within docker. More designed for testing k8 then actually developing ontop of <code>micro-k8s</code> Made specifically to work with Canonical / Snap / Ubuntu. Working elsewhere is not available. Working with Snap, except for simple use cases between user &lt;-&gt; service, sucks in general <code>k3s</code> A mostly production ready k8 but slimmed down to run on fewer resources. TrueNAS uses this under the hood for self hosting service management. Have been told though working with it can create more verbose typing as all the tools are wrapped within the <code>k3s</code> CLI binary (ex: to use <code>kubectl</code> you can't configure and point <code>kubectl</code> to your <code>k3s</code> cluster, you have to go <code>k3s kubectl</code>)"},{"location":"security/crack-windows-passwords/","title":"Crack Windows Passwords","text":"<p>There is no such thing as forgetting your Windows Password or being locked out of your computer...</p>"},{"location":"security/crack-windows-passwords/#pre-requisites","title":"Pre-Requisites","text":"<ul> <li>Create a Kali Linux Bootable USB</li> </ul>"},{"location":"security/crack-windows-passwords/#process","title":"Process","text":"<ol> <li>plug in USB. Boot into Kali</li> <li>mount partition</li> <li><code>cd</code> to <code>/mnt/partition</code></li> <li><code>cd</code> to <code>/windows/System32/config</code></li> <li>Run <code>chntpw -l SAM*</code> to list all usernames on Windows</li> <li>Run <code>chntpw -u \u201cusername\u201d SAM</code> to view all options available to do with user's account.</li> <li>Choose 1 to remove password in prompt when executing previous steps command</li> </ol>"},{"location":"security/server-hardening/","title":"Server Hardening","text":"<p>Server hardening is the process of locking down a server so that the minimum amount of necessary components are allowed in and out of the server. This also includes applications operating within the server as well. In some cases, this procedure is simply creating a minimum version of the operating system and its tools for the servers specific purpose.</p> <p>Below is a continually evolving outline on an ideal practice of hardening different operating systems and different software components within each operating system.</p>"},{"location":"security/server-hardening/#packages","title":"Packages","text":"<p>To harden the server, removing as many packages as possible ensures that no surprising software is operating on the system. Additionally this reduces the chances of an exploit being discovered in a library you are not regularly keeping up to date. In some cases, installing a minimal OS build is easiest in this situation</p>"},{"location":"security/server-hardening/#debian","title":"Debian","text":"<pre><code>sudo apt-get --purge remove python3 python perl ruby \nsudo apt-get install deborphan\nsudo apt-get remove --purge `deborphan`\nsudo apt-get install localepurge\nsudo localepurge\n</code></pre>"},{"location":"security/server-hardening/#debian-smtp","title":"Debian SMTP","text":"<p>Debian Also comes preconfigured with a lightweight SMTP daemon. It can be quite a pain to remove it. It can be uninstalled though with the following commands <pre><code>apt-get remove exim4 exim4-base exim4-config exim4-daemon-light\n</code></pre> You can then delete all logging belonging to the daemon with the following command <pre><code>rm -r /var/log/exim4/\n</code></pre> This removal of course does not remove any configuration files. You can remove those by including the <code>--purge</code> flag in the remove call. See posting for details: http://stackoverflow.com/questions/12061358/how-to-cleanly-remove-exim4-mail-server-on-ubuntu</p>"},{"location":"security/server-hardening/#raspbian","title":"Raspbian","text":"<pre><code>sudo apt-get remove --purge libx11-6 xserver-xorg xserver-xorg-core xserver-common luajit java-common oracle-java8-jdk gdb gdbserver python-pifacecommon python-pifacedigitalio python-numpy python3 \npython3-minimal python3.2 python3.2-minimal debian-reference-common raspi-copies-and-fills hicolor-icon-theme raspberrypi-artwork omxplayer penguinspuzzle gnome-themes-standard-data lxde-icon-theme \ndesktop-file-utils libfreetype6-dev libept-dev smbclient aptitude aptitude-common libsysfs2 libident libboost-iostreams1.46.1 libept1.4.12 libboost-iostreams1.50.0 libboost-iostreams1.48.0 \nlibboost-iostreams1.49.0 libjbig0 libcwidget3 libsigc++-1.2-5c2 libsigc++-2.0-0c2a libapt-pkg-dev libtagcoll2-dev libraspberrypi-dev libraspberrypi-doc gcc-4.5-base libxapian-dev libwibble-dev \nzlib1g-dev libxapian22 gstreamer1.0-omx fbset menu-xdg hardlink udisks python-rpi.gpio gstreamer1.0-alsa python-serial xdg-utils cgroup-bin v4l-utils dphys-swapfile pkg-config gstreamer1.0-libav \nwireless-tools cifs-utils ncdu\n\nsudo apt-get autoremove --purge\n\nsudo apt-get install deborphan\nsudo apt-get remove --purge `deborphan`\n\nsudo apt-get install localepurge\nsudo localepurge\n</code></pre>"},{"location":"security/server-hardening/#users","title":"Users","text":"<p>User creation and credential management is important in securing your server. Always use long passwords for regular users, and even longer ones for the root user or sudo privileged users. An ideal strategy is either use a password generator or create sentences instead of words for your passwords. Note that length is more important then unique characters. Adding unique characters (eg: @#$.?), numbers, lowercase and uppercase will reduce the chances of the password being available in a dictionary or hybrid password attack. Length will though increase time during a brute-force attack. In any situation you want attackers to resort to a brute force attack, this way being the least efficient and most time consuming manner. A nice overview of this can be found here: https://www.youtube.com/watch?v=RtUvMJFP_IE</p> <p>On a fresh install 1. Change the root password with <code>sudo passwd root</code> 2. Create a new regular user with <code>adduser &lt;username&gt;</code>. DO NOT give this user sudo privileges 3. Remove sudo privileges from all users who can login to the machine</p>"},{"location":"security/server-hardening/#openssh","title":"OpenSSH","text":"<ul> <li>Configure the ssh daemon (typicaly in <code>/etc/sshd/sshd_config</code>) to not allow connections with root</li> <li>Configure RSA key based authentication. Once configured DISABLE PASSWORD AUTHENTICATION</li> <li>Configure Allow, AllowGroup, Deny, DenyGroup settings in ssh (http://www.cyberciti.biz/tips/openssh-deny-or-restrict-access-to-users-and-groups.html)</li> </ul> <p>Scroll to the bottom of the sshd_config and update or add the following:</p> <pre><code>KexAlgorithms curve25519-sha256@libssh.org,ecdh-sha2-nistp521,ecdh-sha2-nistp384,ecdh-sha2-nistp256,diffie-hellman-group-exchange-sha256\n\nCiphers chacha20-poly1305@openssh.com,aes256-gcm@openssh.com,aes256-ctr\n\nMACs hmac-sha2-512-etm@openssh.com,hmac-sha2-512\n</code></pre> <p>This will give the strongest Key Exchange, Ciphers and MAC Algorithms</p>"},{"location":"security/server-hardening/#iptablesfirewall","title":"IpTables/Firewall","text":"<ul> <li>See IpTables page for details - Create Firewall which only allows required services through<ul> <li>DHCP</li> <li>DNS</li> <li>SSH only in if it is being used</li> </ul> </li> <li>Ensure Kernel Forwarding (allows packets to be forwarded) is Disabled Unless this server is a Gateway Server</li> <li>Ensure IPTABLES default policy is DROP on INPUT, OUTPUT, and FORWARD</li> <li>Ensure IP6TABLES default policy is DROP on INPUT, OUTPUT, and FORWARD<ul> <li>Unless you are explicitly supporting IPv6 for a specific application, there should be no ACCEPTing rules in this firewall</li> </ul> </li> <li>Install and configure <code>iptables-persistent</code> to ensure firewall persists. See IpTables page again for details</li> </ul>"},{"location":"security/server-hardening/#rsyslog","title":"RSysLog","text":"<p>Always Configure RSysLog to do remote logging!. At minimum use a cloud service such as Papertrail to store extra copies of system logging. Ideally have a separate hardened server for this purpose.</p> <p>Configure as many components as possible (apache, lightppd, mysql) to log their data through RSysLog, thus allowing copies all to be sent to a remote logging service. Details on RSysLog can be found on the RSysLog page</p> <p>Tools such as Splunk, Sawmill, Loggly support mix of features to also retrieve RSyslogs and analyse them in real-time. These are also options that can be setup with a remote logging server</p>"},{"location":"security/server-hardening/#intrusion-detection-intrusion-prevention-systems","title":"Intrusion Detection / Intrusion Prevention Systems","text":"<p>Depending on your configuration an IDS or IPS may be ideal to have within the network. Systems such as Snort, Bro, Suricata can listen and run analysis on various traffic running through your network or system, providing useful insight along with RSysLog. See the Snort and Bro pages for documentation on those products</p>"},{"location":"security/server-hardening/#port-knocking","title":"Port Knocking","text":"<p>Depending on configuration, port knocking is an ideal method of hiding open ports on the server and to make connections more discrete. Numerous tools exist, most notably doorman is a tool that supports this functionality, and manipulates the IPtables rules to implement its functionality</p>"},{"location":"security/server-hardening/#sources","title":"Sources","text":"<ul> <li>Raspbian Minimal Install: https://www.raspberrypi.org/forums/viewtopic.php?t=109262</li> <li>Configure Accept and Deny for OpenSSH: http://www.cyberciti.biz/tips/openssh-deny-or-restrict-access-to-users-and-groups.html</li> </ul>"},{"location":"security/server-hardening/#ssh","title":"SSH","text":"<ul> <li>https://security.stackexchange.com/questions/179114/what-are-the-toughest-ssh-daemon-settings-in-terms-of-encryption-handshake-or</li> <li>https://www.linuxjournal.com/content/cipher-security-how-harden-tls-and-ssh</li> <li>https://infosec.mozilla.org/guidelines/openssh</li> <li>https://stribika.github.io/2015/01/04/secure-secure-shell.html</li> </ul>"},{"location":"services/apache/","title":"Apache","text":"<p>==File Locations==</p> <p>===Fedora/CentOS=== * HTML Document Root : /var/www/html * Main httpd config file: /etc/httpd/conf/httpd.conf * Apache Daemon Directory: /etc/rc.d/init.d/httpd ===Ubuntu=== * HTML Document Root : /var/www/html * Main httpd config file: /etc/apache2/apache.conf * Virtual Host Configuration file: /etc/apache2/sites-available/000-default.conf</p> <p>==Basic Install Apache==</p> <p>===Fedora/CentOS=== sudo dnf install httpd OR sudo yum install httpd depending on Fedora version </p> <p>====Apache Status==== sudo systemctl httpd status sudo apachectl status</p> <p>====Start Apache==== sudo systemctl start httpd.service sudo apachectl start</p> <p>====Stop Apache==== sudo systemctl httpd stop sudo apachectl stop</p> <p>====Set Apache To Start At Boot==== sudo systemctl enable httpd.service sudo apachectl enable</p> <p>===Ubuntu=== sudo apt-get install apache2</p> <p>====Apache Status====</p> <p>====Start Apache==== sudo service apache2 start</p> <p>====Stop Apache==== sudo service apache2 stop</p> <p>====Restart Apache==== sudo service apache2 restart</p> <p>==Create User Account Site== ===Fedora/CentOS===</p>"},{"location":"services/apache/#edit-file-etchttpdconfduserdirconf","title":"Edit File: /etc/httpd/conf.d/userdir.conf","text":""},{"location":"services/apache/#comment-out-the-userdir-disable-macro-and-uncomment-the-userdir-public_html-macro-in-userdirconf","title":"Comment out the \u201cUserDir disable\u201d macro and uncomment the \u201cUserDir public_html\u201d macro in userdir.conf","text":""},{"location":"services/apache/#create-the-useraccount-adduser-passwd","title":"Create the useraccount (adduser, passwd)","text":""},{"location":"services/apache/#login-to-the-account-avoids-some-known-errors","title":"Login to the account (avoids some known errors)","text":""},{"location":"services/apache/#logout-and-back-in-as-root","title":"LOGOUT AND BACK IN AS ROOT","text":""},{"location":"services/apache/#create-the-public_html-folder-inside-the-users-account-folder-if-you-logged-in-the-folder-will-have-documents-music-downloads-etc","title":"Create the public_html folder inside the users account folder (if you logged in the folder will have Documents, Music, Downloads, etc.)","text":"<p>==Add Password Access To Site== This is configuration guide in the appropriate conf file to cause a Basic Auth prompt to appear in the user's browser when viewing a page. Typically you will also want to generate a password file which will store the username and password to log into this area of your website. See the Create Password File section for steps on how to do that ===Fedora/CentOS=== Add this to /etc/httpd/conf.d/userdir.conf   &lt;Directory /home/     AllowOverride None     AuthUserFile      # Group authentication is disabled     AuthGroupFile /dev/null     AuthName test     AuthType Basic              require valid-user         order deny,allow         deny from all         allow from all      <p>If the basic auth does not appear, check in the userdir.conf the stanza at the bottom the page blocking access to userdir is commented out</p> <p>==Create A Password File==</p> <p>htpasswrd [-c]  <p>Eg: htpasswd [-c ] passwordfile username</p> <p>-c to create a new password file</p> <p>do not include '-c' to update a password file</p> <p>Note when generating this file, if using it for the above user profile site and with basic auth security, you may need to place this created file in the <code>public_html</code> folder of the user's profile site you are trying to access. This doesn't seem safe, but it works. The internet gives notions that the file should be named with a '.' so that it is hidden from the file system</p> <p>==Create A Self-Signed Certificate For Apache== ===Ubuntu=== 1. Create a folder create all of your self-signed certs in. cd into it 2. Execute:    sudo openssl req -new &gt; new.ssl.csr</p> <p>First your going to be prompted to create a password. Remember this password, you will need it in a later command</p> <p>At this point you will be prompted to enter some information looking something like this:   Generating a 1024 bit RSA private key ................++++++ ........................++++++ writing new private key to 'privkey.pem' Enter PEM pass phrase: Verifying - Enter PEM pass phrase: <p>You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank.</p> <p>Country Name (2 letter code) [AU]:  State or Province Name (full name) [Some-State]:  Locality Name (eg, city) []:  Organization Name (eg, company) [Internet Widgits Pty Ltd]:  Organizational Unit Name (eg, section) []:  Common Name (eg, YOUR name) []:  Email Address []: &lt;email (optional) <p>Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []:  An optional company name []:  <p>Basicaly for any setup most of the fields are optional except the Common Name field. If you are setting up apache with VirtualHost then you will need to make sure this is the fully qualified domain name (eg. wiki.bensoer.com) OR whatever value you are placing in the <code>ServerAlias</code> field of your VirtualHost entry for apache. This is important as apache will deny access if the value in the ServerAlias and the Common Name in the Self-Signed Certificate don't match</p> <p>Additionaly you should leave the challenge password blank, otherwise you will need to enter the password you put there everytime you reboot apache</p> <ol> <li>Execute the Following commands:   sudo openssl rsa -in privkey.pem -out new.cert.key sudo openssl x509 -in new.ssl.csr -out new.cert.cert -req -signkey new.cert.key -days 3600 <p>sudo mkdir /etc/ssl/self-signed sudo mkdir /etc/ssl/self-signed/certs sudo mkdir /etc/ssl/self-signed/private</p> <p>sudo cp new.cert.cert /etc/ssl/self-signed/certs/server.crt sudo cp new.cert.key /etc/ssl/self-signed/private/server.key</p> <p>After entering the first command in step 3 you will be prompted for a password. This is the password you created earlier in step 1</p> <p>Note the <code>-days</code> command used in the second command of step 3 is setting the number of days the self-signed certificate is valid. If you would like to renew your self-sgined certificate more often then change this number to something smaller</p> <p>After completing all commands in step 3 your self-signed certificates will be available in the <code>/etc/ssl/self-signed</code> folder. You can reference these directories in your VirtualHost configurations when setting up SSL for a VirtualHost</p> <p>==Notes==</p>"},{"location":"services/apache/#check-folderfile-permissions-sudo-chmod-777-with-caution","title":"Check Folder/File Permissions (sudo chmod 777 with caution)","text":""},{"location":"services/apache/#check-indexhtml","title":"* Check index.html","text":""},{"location":"services/apache/#check-the-account-was-created-correctly-loginlogout","title":"Check the Account was created correctly (login/logout)","text":"<p>==Sources== Create Self-Signed Certificates https://www.linux.com/learn/tutorials/392099-creating-self-signed-ssl-certificates-for-apache-on-linux</p>"},{"location":"services/bind9/","title":"Bind9","text":"<p>Bind9 is one of the most popular unix dns handling systems available. This documentation is primarily a reiteration of the included digital ocean source listed below with some personalised extra information to improve clarity</p> <p>==Debian 9== The debian 9 configuration instructions are setup for a single server hosted setup. Thus there is only one server with bind9 installed. This single server both operates and the name server and records server. With only a single server there is also no backup / secondary server as described in the digital ocean documentation</p> <p>===Installation=== To install Bind9 on Debian9 execute the following commands   sudo apt install bind9 bind9utils bind9-doc</p> <p>===Configure IPv4 Mode=== Configuring IPv4 Mode means bind9 will only handle requests over IPv4. This essentially just reduced configuration work as the IPv6 requires additional settings in bind9's config files.</p> <p>Edite the file <code>/etc/default/bind9</code> and add to the top of the file the following line:   OPTIONS=\"-u bind -4\" This will force bind9 to boot in IPv4 mode. You will need to restart bind9 for the change to take effect</p> <p>===Configure The DNS Server=== Open the file <code>/etc/bind/named.conf.options</code> and enter the following:   options {         directory \"/var/cache/bind\"; <pre><code>    recursion yes;                 # enables resursive queries\n    allow-recursion { any; };      # allows recursive queries from any clients\n    listen-on { &lt;privateip&gt;; };    # dns servers public IP address\n    allow-transfer { none; };      # disable zone transfers by default\n\n    forwarders {\n            8.8.8.8;\n            8.8.4.4;\n    };\n</code></pre> <p>}; Note that the above configuration has set <code>allow-recursion</code> to any which means any IP can make DNS look-ups requiring recursion. This is generally a security hazard but for an internal private network this may not be a huge deal. You can always change this value to <code>trusted</code>. </p> <p>IF you change the <code>allow-recursion</code> value to <code>trusted</code>, add the following section also to the top of the configuration file:   acl \"trusted\" {         ;     # dns servers public ip address         ;  # host1         ;  # host2 }; By doing this you are only allowing hosts with the IPs listed to make recursive queries. You will need to fill this list with the ip of the dns server and all clients that will be using this dns server! <p>Also, this configuration has been setup with forwarders to <code>8.8.8.8</code> and <code>8.8.4.4</code>. These are the IPs of Google's DNS servers and are used when our dns server does not have the record requested listed. This is useful if this DNS server will be referred to to resolve all domains - including those outside of the network. You can change this to a DNS server of your preference or remove the section if you do not want any forwarding of DNS requests to occur.</p> <p>===Configuring Zone Data=== Next edit the <code>/etc/bind/named.conf.local</code> file to add zone information. This file stores the name of the domains that will have their records stored on this server and where to find the zone file information.</p> <p>Copy the following into the <code>/etc/bind/named.conf.local</code> file:   zone \"\" {     type master;     file \"/etc/bind/zones/db.\"; # zone file path }; Replace <code> with your full domain. This could be myprivatedomain.local or bensoer.com if you wanted this dns server to resolve those domains <p>===Configuring Zone Files=== You now need to create the zone file which you have configured in the previous section to refer to for zone data. The folder and path listed above may not exist, so run the following commands:   sudo mkdir /etc/bind/zones Then create an open a file named <code>db.. Copy the following into it   $TTL    604800 @       IN      SOA     . root.. (                       YYYYMMDDV         ; Serial                          604800         ; Refresh                           86400         ; Retry                         2419200         ; Expire                          604800 )       ; Negative Cache TTL <p>; name servers - NS records     IN      NS      ns1.. <p>; name servers - A records ns1..          IN      A        root..         IN      A        <p>; A records Replace all locations of <code> with your full domain. This could be myprivatedomain.local or bensoer.com if you wanted this dns server to resolve it. Note also to replace the <code>YYYYMMDDV</code> in the Serial value with the current Year Month Date and Version. As of this writing this should be 201811201. Note that keeping this number up to date is crucial with every update as bind9 is only able to determine if changes have happened if this serial number is updated. Simply update it by updating the date OR increment the Version value if there are multiple updates within the same day. Reset version back to 1 if the date has changed. This system not only allows for easy number generation but gives a helpful reminder to other administrators of when the last change was made to the bind9 dns server. <p>All required configuration is now in place to resolve your domain. Now simply add A records in the zones file configured above. Add records under the A records comment in the same format as the nameserver records specified above. You can use the following as a template:   ..         IN      A        <p>Save your changes and restart the bind9 service</p> <p>==Notes==</p> <p>==Sources== * https://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-a-private-network-dns-server-on-debian-9#testing-clients</p>"},{"location":"services/cron/","title":"Cron","text":"<p>\"cron\" from the Greek word \"chronos\" meaning time.</p> <p>cron is an automation tool that periodicaly wakes up and executes \"cron jobs\" that can be stored in specific locations to do specific tasks. cron jobs are generaly written in shell, but are free to execute any command thus being expandable aswell to execute any other scripting language script</p> <p>==File Locations== Main cron file (crontab): <code>/etc/crontab</code>  Additional cron directory: <code>/etc/cron.d/</code> </p> <p>Default Cron Logs: <code>/var/log/cron</code>  NOTE: On some Fedora 22 this log may not exist. If you can not find it, check if you have <code>syslog</code> installed. If that is not installed, then your logs are being stored in journalctl. Lookup journalctl for more documentation. Alternatively you can also install syslog along with journalctl and it will have no effect on logging</p> <p>These directories may or may not exists depending on distro  Hourly cron directory: <code>/etc/cron.hourly</code>  Daily cron directory: <code>/etc/cron.daily</code>  Weekly cron directory: <code>/etc/cron.weekly</code>  Monthly cron directory: <code>/etc/cron.monthly</code> </p> <p>==Setup== When the daeomon wakes up every minute it first checks and executes the crontab file, then it checks and executes all files in the cron.d folder. When executing these scripts, any output is mailed to the owner of the crontab or the user name in the MAILTO environment variable that can be set in the crontab - if it exists.</p> <p>Fedora treats the files located in <code>/etc/cron.d/</code> as an extension of the <code>/etc/crontab</code> file and therefor inherits any settings as thier defaults from the crontab file. The intended purpose of this feature is to allow packages that require finer control of their scheduling then that available in <code>/etc/crontab</code></p> <p>For more general cron usage, some Unix/Linux distributions come with hourly, daily, weekly and monthy cron job folders. These folders contents are executed at such appropriate times. To use these prebuild cron jobs, add your shell script to the appropriate folder</p> <p>You should note you can only use Bourne Shell Command for Cron. Also Cron requires absolute paths for all commands (eg. \"/bin/ls\"  to use the list command)</p> <p>==Create A Cron Entry== An example of an entry in the <code>/etc/crontab</code> file looks like this:   *  *  *  *  *  root /bin/full-path/script.sh Note that SPACING is VERY IMPORTANT. There are 2 spaces between each star including between the last star and root, and then a single space between root and the script directory</p> <p>A template of what each item stands for:   <p>Each of the stars represent a time entry point. A \"*\" by itself means \"every\" of that category. It's easiest to read this as a sentence to decipher what is happening. Ranges for each entry is as follows</p> <p>{| class=\"wikitable\" ! Entry ! Range ! Additional Functionality |- |  || Value from 0-59. Entry means at what minute the cron job will run. || To have it run every increment of a certain amount of minutes use / |- |  || Value from 0-23. Entry means at what hour the cron job will run. || To have it run every increment of certain amount of hours use / |- |  || Value from 1-31. Entry means at what day of the month to execute the cron job || |- |  || Value from 1-12. Entry means at what month to execute the cron job. || To have it run every increment of a certain amount of months use */ |- |  || Value from 0-6. Sunday being 0. Entry means at what day of the week to execute the cron job || |- |  || What user to use when executing the cronjob. Typicaly this is root |- |  || The absolute path to the shell script to be executed || By adding \"run-parts\" after the user-permission section this can be an absolute path to a directory containing multiple scripts which will all be executed based on this cron entry |} <p>Whenever you make changes Cron does not need to be restarted. Everytime the crontab command executes it updates the modtime of the spool directory whenever it changes a crontab. By doing this the crontab checks if the spool directory's modtime (or modtime in /etc/crontab) has changed and if it has, cron will then examine the modtime on all crontabs and reload those which have changed.</p> <p>==Examples== ===Crontab Examples===   01 * * * * root run-parts /etc/cron.hourly 02 4 * * * root run-parts /etc/cron.daily 22 4 * * 0 root run-parts /etc/cron.weekly 42 4 1 * * root run-parts /etc/cron.monthly * The first line specifies that during the first minute, of every hour, of every day, of every week, and every month, all the scripts in the /etc/cron.hourly directory will be run. * The second line specifies that all of the scripts in the /etc/cron.daily will be executed daily at 0402 hrs (using the 24 hour clock). * The third line specifies that all of the scripts in the /etc/cron.weekly directory will be executed every week, on day 0 (Sundays - the days are numbered from 0 to 6, with 0 being Sunday) at 0422 hrs. * The last line specifies that all of the scripts in the /etc/cron.monthly will be executed on the first of very month at 0442 hrs.</p> <p>===Script Examples=== A script is an sh file that could contain something as simple as:   mail -c foo@domain.ca -s \"attachment\" bar@domain.ca &lt; /root/file.txt This is a script that sends an email to bar@domain.ca and CC's it to foo@domain.ca. The subject is \"attachment\" and the mail contents is injected via input redirection from a file stored in /root/file.txt</p> <p>==Storing Crontab Output== By defualt cron saves the output of a script in the user's mailbox (in most cases that is root)</p> <p>For user and custom application scripts, it is much better to save in a seperate logfile. Alter your cron job to look something liek this to reroute its output:   */10 * * * * /home/foo/ex1 2&gt;&amp;1 &gt;&gt; /var/log/script_output.log What is happening here is both <code>STDOUT</code> and <code>STDERR</code> are being output redirected and appended to var/log/script_output.log. The combination <code>2&gt;&amp;1</code> tells linux to merge both of these output streams and then <code>&gt;&gt;</code> is appending the output redirect to the specfied file.</p> <p>==Mailing Crontab Output== Additionaly modiifications can be made to actualy mail the output of the cron script. We can do this by changing the <code>MAILTO</code> environment variable defined in the <code>/etc/crontab</code> file or in your crontab scripts in <code>/etc/cron.d/</code> with an actual valid email.</p> <p>To set an email specificaly for 1 cronjob. This can be done by appending the email to the cron call like this:   */10 * * * * /home/foo/script.sh 2&gt;&amp;1 | mail -s \"cronjob ouput\" validemail@domain.com You can see the STDOUT and STDERR redirects here are being piped ( \"|\" ) into the mail command as mail content</p> <p>==Notes== If your crontab is not working check: * file permissions on the file being executed by cron * what user is being used by cron to execute the script * whether SELinux is interfering with the scripts execution * spacing may or may not matter depending on the machine</p> <ul> <li>Note cron requires absolute paths to run any system tool (/bin/ls for ls) ** This also means absolute paths to other files or resources the cron job script may need to access</li> <li>cron also only can use bourne shell commands (commands only available through the bourne shell, NOT Bourne-Again Shell)</li> </ul> <p>==Sources==</p>"},{"location":"services/iptables/","title":"Iptables","text":"<p>==Background== Problems related to security and access can come from outside or inside. These can be malicious in intent, the result of \"just looking around\", or accidental. It is important to verify the cause and intent of the intrusion by careful examination of log files. Once the intrusion has been characterized, steps can be taken to block the appropriate addresses and ports using a firewall. We will discuss the various techniques for controlling access to a network and protecting critical components using firewalls. The following are some of the more important methods of attack and ways of protecting yourself against them:</p> <p>===Unauthorized Access=== Unauthorized users outside your company attempt to connect to company file servers. Accessing an NFS server for example. These attacks can be avoided by carefully specifying who can gain access through services such as NFS and SAMBA</p> <p>===Exploitation of Known Weaknesses In Applications=== Some applications and network services were not originally designed with strong security features and are inherently vulnerable to attack. Examples are remote access services such as rlogin, rexec, etc. The best way to protect against this type of attack is to disable any vulnerable services or severely restrict them.</p> <p>===Denial Of Service=== Denial of service attacks are designed to overload a service and cause it to cease functioning or prevent others from accessing the service or program. These may be performed at the network layer by sending carefully crafted and malicious packets that cause network connections to fail. They may also be performed at the application layer, where carefully crafted application commands are given to a program that cause it to become extremely busy or stop functioning. Preventing network traffic from suspicious sources from reaching your hosts and preventing suspicious program commands and requests are the best ways of minimizing the risk of a denial of service attack.</p> <p>===IP Spoofing=== IP Spoofing is a security exploit where an Intruder attempts to send packets to a system which appear to originate from a source other than the Intruder's own. If the target system already has an authenticated TCP session with another system on the same IP network, and it mistakenly accepts a spoofed IP packet, then it may be induced to execute commands in that packet, as though they came from the authenticated connection. The attacker pretends to be an innocent host by following IP addresses in network packets. For example, a well-documented exploit of the BSD rlogin service can use this method to mimic a TCP connection from another host by guessing TCP sequence numbers. Verifying the authenticity of packets and commands will prevent this attack. Prevent routing with invalid source addresses. Introduce unpredictability into connection control mechanisms, such as TCP sequence numbers and the allocation of dynamic port addresses.</p> <p>===Eavesdropping=== A host is configured to \"listen\" to and capture data that is flowing on the network. Usernames and passwords are thus captured from user login network connections. Broadcast networks like Ethernet are especially vulnerable to this type of attack. One of the best ways to protect against this attack is to use of data encryption and secure connections (secure shell).</p> <p>===IP Firewalling=== Firewalls are very effective in preventing or reducing unauthorized access, network layer denial of service, and IP spoofing attacks. They are not very useful in avoiding exploitation of weaknesses in network services or programs and eavesdropping. These can be mitigated using other techniques. A firewall is a secure and trusted machine running specialized software that is inserted between a private network and a public network. The firewall software is configured with a set of rules that determine which network traffic will be allowed to pass and which will be blocked. The most sophisticated firewall arrangement involves a number of separate machines and is known as a perimeter network. Two machines act as \"filters\" called chokes to allow only certain types of network traffic to pass, and between these chokes reside network servers such as a mail gateway or a World Wide Web proxy server. This configuration can be very safe and easily allows quite a great range of control over who can connect both from the inside to the outside, and from the outside to the inside. Typically though, firewalls are single machines that serve all of these functions. These are a little less secure, but cheaper and easier to manage than the more sophisticated perimeter networks. The Linux kernel provides a range of built-in features that allow it to function quite effectively as an IP firewall. The network implementation includes code to do IP filtering in a number of different ways, and provides a mechanism to dynamically design and implement the sort of rules that the firewall will apply to packets entering and leaving the network.</p> <p>===IP Filtering=== This is simply a mechanism that decides which types of IP packets will be processed normally and which will be deleted and completely ignored, as if they had never been received.</p> <p>We can apply different criteria to determine which packets are to be filtered: * Protocol type: TCP, UDP, ICMP, etc. * Socket number (for TCP/UPD) * Datagram type: SYN/ACK, data, ICMP Echo Request, etc. * Datagram source address. * Datagram destination address.</p> <p>It is important to understand that IP filtering is a network layer facility. This means it doesn't understand anything about the application using the network connections, only about the connections themselves. For example, you may deny users access to your internal network on the default telnet port, but relying on IP filtering alone will not stop them from using the telnet program with a port that you do allow to pass through your firewall. Using proxy servers for each service that you allow across your firewall can prevent this. The proxy servers understand the application they were designed to proxy and can therefore prevent abuses, such as using the telnet program to get past a firewall by using the World Wide Web port. The IP filtering ruleset is made up of many combinations of the criteria listed previously. For example, say you wanted to allow web users within your network to have no access to the Internet except to use other sites' web servers. You would configure your firewall to allow forwarding of: * Packets with a source address on your network, a destination address of anywhere, and with a destination port of 80. * Packets with a destination address of your network and a source port of 80 from a source address of anywhere</p> <p>We have used two rules here. We have allowed our data to go out, but also the corresponding reply data to come back in. Linux simplifies this and allows us to specify both rules in one command.</p> <p>===Setting Up Linux Firewalling=== Most Linux distributions have high-level utilities to configure and deploy firewalls. As a means of acquiring an in-depth understanding of how a firewall functions, we will design and deploy firewalls using a low-level utility such as iptables. Linux kernels 2.3.15 and later support the fourth generation of Linux IP firewall called netfilter. The netfilter code is the result of a major redesign of the packet handling flow in Linux. The netfilter is a multifaceted utility, which can be configured using a utility called iptables.</p> <p>==Iptables== Iptables is actually only part of the mechanism that makes up the whole firewall system on Linux. There are two components that make it up in total: the kernel portion is known as netfilter and iptables is actually the extensible configuration tool that manages and runs netfilter.</p> <p>===Basic Structure=== There are 3 main chains that make up the netfilter system. INPUT, OUTPUT and FORWARD. Each of these chains is for a specific use: * INPUT chain is for packets that are destined FOR the host machine * OUTPUT chain is for packets that are coming FROM the host machine - the source address of the packet is the machine running iptables * FORWARD chain is for packets that are not destined for the iptables hosting machine that recieved it. This chain is used, as the name implies, for forwarding packets from the iptables machine to the given system. This is useful if iptables is being hosted on a router or gateway server that can then view the traffic going to and from the systems behind it and decide whether to let it through</p> <p>netfilter then also has 3 unique tables. filter, mangle and nat. Each table can has its own predefined chains. Each table can only apply certain actions on the packets that pass through it. filter is the primary table that interacts with the INPUT,OUTPUT and FORWARD chains. By default iptables will apply its commands to the filter table, explicit naming of the other tables is needed to apply actions to those tables.</p> <p>===The Filter Table===</p> <p>==Saving Changes Permanently== At restart, most systems will reset whatever has been put as the iptable rules with the system default. In order to keep the rules that were created we need to override the systems loading scripts. This can be done using iptables-save and iptables-persistent</p> <p>The below demonstrates how to use these programs on Ubuntu and Debian, additional steps are needed for CentOS and other RHEL based platforms. Please see the sources at the bottom of this page for a link to the full guidelines</p> <p>===Exporting/Restoring Iptable Rules=== iptables-save and iptables-restore are tools that allow you to export and restore the current iptables rules to a file. Exporting for IPv4 and Ipv6 can be done simply as this   iptables-save &gt; /etc/iptables/rules.v4 ip6tables-save &gt; /etc/iptables/rules.v6</p> <p>You can then do the inverse and restore the iptables from these files using iptables-restore with the following syntax   iptables-restore &lt; /etc/iptables/rules.v4 ip6tables-restore &lt; /etc/iptables/rules.v6</p> <p>===Configuring Persistence=== As of Ubuntu 10.0.4 and Debian 6, iptables-persistent is available and compatible with automatically restoring the iptable rules. iptables-persistent requires that the ip table rules be stored in /etc/iptables/rules.v4 for IPv4 rules used by iptables, and /etc/iptables/rules.v6 for IPv6 rules used by ip6tables</p> <p>You can install iptables-persistent with the following command   sudo apt-get install iptables-persistent This will install and configure the persistence module. During setup it will prompt as to whether to export the current iptables and ip6tables contents. If you have already done this via iptables-save, then you can skip this step. Otherwise select yes, and iptables-persistent will automatically export the current iptables entries and generate the required directories and files to operate</p> <p>==Sources== https://www.thomas-krenn.com/en/wiki/Saving_Iptables_Firewall_Rules_Permanently</p> <p>==Notes==</p>"},{"location":"services/linux-monitoring-tools/","title":"Linux Monitoring Tools","text":"<p>There are a number of tools in Linux that can be used to gather information about your system and determine if it is or has been hacked in some way</p> <p>==ifconfig/ip address== <code>ifconfig</code> or <code>ip address</code> can give a summary overview of the network cards on the system</p> <p>By default ifconfig will report the status of all active network cards. To view all cards, active or inactive include <code>-a</code></p> <p>An example of <code>ifconfig -a</code> printout could look like this:   eth0 Link encap:Ethernet HWaddr 00:01:02:45:45:5B    inet addr:192.168.1.20 Bcast:192.168.1.255 Mask:255.255.255.0    UP BROADCAST RUNNING PROMISC MULTICAST MTU:1500 Metric:1    RX packets:1449310 errors:0 dropped:0 overruns:0 frame:0    TX packets:19866 errors:0 dropped:0 overruns:0 carrier:0    collisions:232 txqueuelen:100    Interrupt:5 Base address:0xb400 lo Link encap:Local Loopback    inet addr:127.0.0.1 Mask:255.0.0.0    UP LOOPBACK RUNNING MTU:16436 Metric:1    RX packets:70 errors:0 dropped:0 overruns:0 frame:0     TX packets:70 errors:0 dropped:0 overruns:0 carrier:0    collisions:0 txqueuelen:0</p> <p>An important note is to make sure your card is not in <code>PROMISC</code> mode as the sample readout shows for eth0. PROMISC mode means that the card, instead of only listening for packets addressed to it, it is listening to all packets on the network. In terms of hacking, many hackers will do this so as to try to collect important data within your network that the system is connected to. Unless you are running an intrusion detection system on the local system, this mode should not be appearing</p> <p>==Checking Network Connectivity with ping== Ping is commonly disabled on servers as hackers have notoriously used it for simple DOS attacks. It's also used for reconnaissance in mapping out the network they may be interested in. It is not uncommon to ping a destination and get no results back. That being said ping is a useful tool for testing connectivity whether be the end host or interfaces in between</p> <p>You can execute ping by calling:   ping  <p>You can also limit how many hops ping will make between it and its destination (TTL - time to live) by using the <code>-t</code> flag followed by the number of hops to limit it by</p> <p>==Checking Network Connectivity with traceroute== Traceroute is a tool that shows all of the routers a request went through and thier latencies before arriving at its destination. It can be used to determine why a server may be slow, and if that may be caused by congestion at certain routers in the network</p> <p>Traceroute also allows use to get an interesting view of how ISP's and the target host are connected to eachother and connected to the internet</p> <p>You can execute traceroute like this:   traceroute  <p>http://www.visualroute.com/ is an interesting tool that lets you visualize with a GUI traceroute and placement of routers (although not 100% accurate)</p> <p>==Checking Network Prrocesses with netstat==</p> <p>netstat displays protocol statustics and current TCP/IP connections</p> <p>===Tags===</p> <p>{| class=\"wikitable\" !Tag !Use |- | -n || list all connections by IP rather then host names. This is useful if name resolutions are slow or are failing |- | -s || allows you to view per-protocol statistics for TCP, UDP, ICMP and IP |- | -p  || allows you to view information from a specific protocal |- | -a || lists all ports that are either in active use or being listened to by local servers |- | -A || specifies the adddress family reported. The listing uncludes the ports in use as they are associated with your network infterface cards. Local UNIX address family socket connections aren't reported, including local network-based connections in use by programs, such as any X Window program you might have running |} <p>Some examples with these tags include:   netstat -s -p icmp This call will show us statistics for ICMP   netstate -s -p ip 3 This example will show statistics about IP protocols. Additionaly it will also auto update itself every 3 seconds</p> <p>An extremely useful call to view all internet connections on your system is:   netstate -anp --ip Note that this only looks at internet connections using the <code>AF INET</code> socket. AF INET is a unix socket for TCP/IP sockets used across a network. The alternative <code>AF UNIX</code> is a socket type local to the kernel. It is primarily used for interprocess communication but nothing goes out to the network through it</p> <p>===Generated Report Notes=== When netstat prints out its  report there are a few interesting things to note about it:</p> <ul> <li>'*' under the Foreign or Local Address means the interface is listening on all network interfaces rather than on just a single interface. The port is either the symbolic or numberic service port identifier the server is using</li> <li>Send-Q and Recv-Q are the number of bits received through the connection but that have not been passed on to the local program</li> <li>State information is useful to identify the state of the connection. The possible states include : <code>LISTEN</code>, <code>SYN SENT</code>, <code>SYN REVC</code>, <code>ESTABLISHED</code>, <code>FIN WAIT</code>, <code>FIN SENT</code> ** If your netstat report returns with allot of connections in <code>TIME_WAIT</code> status, this is an indication that someone may be creating useless TCP connections and attempting a DDOS ** Typical TCP States Include: <code>SYN_SENT</code>, <code>ESTABLISHED</code>, <code>FIN_WAIT_1</code>, <code>FIN_WAIT_2</code>, <code>TIME_WAIT</code>, <code>CLOSED</code> In that order</li> </ul> <p>==Checking Processes with ps== ps reports on process status. As with netstat you should be familiar with every program running on your system and why it is running there. With a properly secured system, you should be able to identify every process listed in the ps report. You should not see any processes you can not identify</p> <p>===Tags=== {| class=\"wikitable\" ! Tag ! Use |- | -a || Selects all processes with controlling terminals, usually user programs running interactively in the foreground |- | -x || Select processes without controlling terminals, usually permanent system daemons running automatically in the background |- | -u || Optional parameter that produces additional user-oriented information including the user login name |}</p> <p>===Examples===   ps -aux | more This will show all processes, including <code>init</code> which is the parent of all processes and it always runs</p> <p>Note you may also see processes called <code>bdflushd</code> which is in charge of flushing modified file system buffers back to disk or <code>kswapd</code> which is a kernel thread that selects physical memory pages for swapping from memory to swap space on disk to free memory for other processes)</p> <p>==Monitor System Resources with lsof== lsof (Standing for LiSt Open Files) allows you to view current network connections and the files associated with them. Mainly you can interrogate a specific port with it to determine what exectly is running/connected/using that port on the system</p> <p>losof provides a very verbose output. calling <code>lsof </code> in console will printout all open files corresponding to every active process on the host. This is quite lengthy so its best to narrow the list down with a few filtering tags</p> <p> isof -i Will print out all connections through the internet in a similar format to <code> netstat -a -p</code>. This is particularly useful when wanting to secure a Linux system as it allows us to view all open ports. Also lsof will list with the programs name, allowing us to find and shutdown unwanted programs easily.</p> <p>You can narrow lsof down with the following filters   lsof -i:587 lsof -i:smtp lsof -i:@somewhere.remote.net</p> <ul> <li>The first example will generate a report looking at whatever is connected or using port 587</li> <li>The second example will generate a report looking at what connections the well know service <code>smtp</code> is making</li> <li>The third example will generate a report for connections going to and from the host somewhere.remote.net</li> </ul> <p>Additionaly you can further filter by protocol and port with the following call with lsof:   lsof -i tcp:5555 This will generate a report of all connections using TCP and port 5555</p> <p>You can even use lsof to view all connections of a specific PID:   lsof -p 505 This will print out a report of all the connections being made by that process, including the file descriptors (listed under the column FD)</p> <p>The file descriptors are defined as follows {| class=\"wikitable\" !Descriptor !Use |- | cwd || Variable represents the current working directory of the process |- | txt || Defines the program text, which is executable itself |- | mem || Is a file held in memory |- | 4 OR 21 || Represents a file in use by a particular process. If these values are followed by a 'u' it means they have both read and write access |}</p> <p>Using all of this together you can determine whether something physically exists on the system, or is simply being held in memory</p> <p>==Using RSysLogs== See [[RSysLog]]</p> <p>==Top and HTop== top and htop are essentialy identical programs that show the top running process on a machine. They are simply augment tools for ps. htop adds extra functionality to top though in adding also live stat information about CPU usage, Memory consumption and Swap space usage along with the top programs.</p> <p>top is included in most linux distributions, htop is avalable in both dnf and apt-get package managers</p> <p>==Notes==</p> <p>==Sources==</p>"},{"location":"services/linux-user-management/","title":"Linux user management","text":"<p>==Summary== Basic user creation and account manipulation is relatively easy with a number of simple commands</p> <p>{| class=\"wikitable\" !Command !Purpose !Syntax Use |- |adduser||Creates a user account|| adduser  |- |passwd||Assigns a password to a user account || passwd  |- |userdel||Deletes user account. Does not delete content || userdel  |- |} <p>==File Locations== Linux stores its user account and password hashes in two seperate files located on the file system.</p> <p>The <code>/etc/passwd</code> file stores all of the user accounts on the system  The <code>/etc/shadow</code> file stores all of the hashes of the passwords to each account on the system</p> <p>==Commands== ===adduser=== <code>adduser</code> adds a user to the system. This user account though is unaccessible as no password has been assigned to it, but can be identified as a valid user through ssh login or virtual login when logging into linux in non-gui mode. Note this step will create the user accounts home directory under <code>/home/ <p>Create a user by entering:   adduser  <p>Creating a user who also has sudo capabilities can be done simply by entering:   adduser  sudo Adding users to any group can be done in the same way. To remove a user from a group, see userdel <p>===passwd=== After adding a user, you should immediately add a password, thus giving access to the account</p> <p>Add a user by entering:   passwd  <p>You will then be prompted to enter a password and possibly warned if it is not strong enough, you can choose to ignore this warning if you would like</p> <p>===userdel=== <code>userdel</code> will delete the user's account but does not delete any of the user's data or content stored in their home folder. This function acts as a method of disabling the user account. </p> <p>To disable/delete a user account enter:   userdel  Sometimes if the user is currently active or a program is using that user, userdel will prompt an error from the disable/delete. You can confirm and check this by running this command   w  This will list all processes that are being used by the user or using the user for executing their commands. By either killing all processes using the user, or using -f flag to force the user to be deleted, userdel will then disable the account <p>To fully delete an account you will need to delete the user's data folder in <code>/home/&lt;username</code></p> <p>This can be done cautiously with the following command   cd /home rm -rf  <p>Note we are using the remove function but also have navigated to the <code>/home</code> folder. All we are doing here is forcing and recursively deleting the user's data folder from the <code>/home</code> folder</p> <p>Alternatively, userdel can also be used to remove users from groups. You can list all groups a user is in with the following command   groups  Simply, remove the user from a group with the following command   userdel  <p>==Notes==</p> <p>==Sources==</p>"},{"location":"services/linux/","title":"Linux","text":"<p>The open-source OS.</p> <p>==Linux Monitoring Tools== See [[Linux_Monitoring_Tools]]</p> <p>== SELinux == See [[SELinux]]</p> <p>== RSysLog == See [[RSysLog]]</p> <p>== User Management == See [[Linux_User_Management]].</p> <p>== Package Installer ==  See [[apt-get]].</p> <p>== Favourite Commands ==</p> <p>=== <code>find</code> and <code>grep</code> - search and sort for a string === *will recursively search / for ''string'' printing the results to the terminal; the -i flag will ignore case sensitivity :<code>sudo find / | grep -i 'string'</code></p> <p>=== <code>tail</code> - live view of file === *will give you a scrolling view of the logfile; as new lines are added to the file, they will show up in your console screen :<code>tail -f 'path to file'</code></p> <p>=== <code>ls</code> - list directory contents === long list format with owner and permission details :<code>ls -l</code> show hidden folders and files as well :<code>ls -al</code></p> <p>=== <code>ps</code> - report a snapshot of the current processes === for current user only :<code>ps -u</code> for all users :<code>ps -au</code> *for all processes even those not associated with a terminal :<code>ps -aux</code></p> <p>See Also [[Linux_Monitoring_Tools]] for details and analysis on using ps</p> <p>=== <code>rm</code> - remove files or directories === for a single file :<code>rm \"''file''\"</code> removes directory and all contents '''USE CAUTION''' :<code>rm -r \"''directory''\"</code> *forces the operation to delete unwriteable files '''USE EXTREME CAUTION''' :<code>rm -rf \"''directory''\"</code></p> <p>=== <code>chown</code> - change the user and/or group ownership === change the file owner :<code>chown \"''Owner''\" \"''file''\"</code> change the file owner and group :<code>chown \"''Owner''\":\"''Group''\" \"''file''\"</code> *change a folder and contents  '''USE CAUTION''' :<code>chown -R \"''Owner''\":\"''Group''\" \"''directory''\"</code></p> <p>=== <code>chmod</code> - change access permissions === readable, writable, and executable by owner, group, and world :<code>chmod 777 \"''file''\"</code> make a file executable :<code>chmod +x \"''file''\"</code> *change a folder and recursively apply to subfolders and subfiles '''USE CAUTION''' :<code>chmod -R 744 \"''directory''\"</code></p> <p>==How to Determine chmod Code== chmod is a tricky function in it allows you to alter file and folder permissions based on numbers, but these numbers are actually not just nonsense and are based on a pattern that can be easily recognized to produce all file permissions desired by the implementer.</p> <p>chmod uses 3 numbers to set its permissions (eg. 777). Each number is an octal value that maps  to read,write and execute permissions for 3 user groups. Each of the 3 numbers is the permission for each group.</p> <ul> <li>The first number sets permissions for the owner/creator of the file or folder. This may be root, but if user implemented is typically that user.</li> <li>The second number sets permissions for the group that the file or folder is part of. For example files created in the /etc/apache or /var/www folders are typicaly part of the www-data or apache usergroup</li> <li>The third number sets permissions for all other users. These can be anyone who is not the owner or part of the group that the folder or file belongs to. Typicaly for most user created files this is the field you will be editing because usually it is for other programs needing access to the file you have created that is not part of any groups</li> </ul> <p>So each number then is an octal number (0-7) but this can be represented in binary needing 3 bits (0-7 in base10 = 000-111 in base2). These 3 bits that make up the number for the group then determine he permission of the group. Each bit represents the following permission:  * First bit is Read permission. 1 means it is enabled 0 means disabled * Second bit is Write permission. 1 means it is enabled 0 means disabled * Third bit is Execute permission. 1 means it is enabled 0 means disabled</p> <p>===Examples=== Lets give a file  Read/Write/Execute to itself, Read/Execute to the group and Read to all others.</p> <ul> <li>Read/Write/Execute = 111 = 7. First number is 7</li> <li>Read/Execute for the group = 101 = 5. Second number is 5</li> <li>Read for all others = 100 = 4. Third number is 4</li> </ul> <p>Therefor to set this permission we need <code>chmod 754"},{"location":"services/mongodb/","title":"Mongodb","text":""},{"location":"services/mongodb/#install-mongodb","title":"Install MongoDB","text":""},{"location":"services/mongodb/#linux-mint","title":"Linux Mint","text":"<p>A walkthrough tutorial is available on the blog site: https://blog.bensoer.com/install-mongodb-on-linux-mint/</p> <p>A brief breakdown of the steps will be listed here</p> <ol> <li>Run the Following Command: <code>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10</code></li> <li>Edit the Follow Command:  <pre><code>echo \"deb http://repo.mongodb.org/apt/ubuntu \"$(lsb_release -sc)\"/mongodb-org/3.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.0.list\n</code></pre> </li> <li>Replace <code>$(lsb_release -sc)</code> with the appropriate ubuntu version the Mint version you are installing MongoDB on is based off of. You can find a full list here : http://www.linuxmint.com/oldreleases.php</li> <li> Update dependencies: <code>sudo apt-get update</code></li> <li> Install Latest Version of MongoDB: <code>sudo apt-get install -y mongodb-org</code></li> </ol>"},{"location":"services/mongodb/#ubuntu","title":"Ubuntu","text":"<p>To install on Ubuntu, the procedure is very simple. Executes the following commands in the listed order:</p> <ol> <li><code>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10</code></li> <li> <pre><code>echo \"deb http://repo.mongodb.org/apt/ubuntu \"$(lsb_release -sc)\"/mongodb-org/3.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.0.list\n</code></pre> </li> <li><code>sudo apt-get update</code></li> <li><code>sudo apt-get install -y mongodb-org</code></li> </ol> <p>Note: that this will install the latest version of mongo. To install a specific version see MongoDB Docs in the sources</p>"},{"location":"services/mongodb/#resources","title":"Resources","text":"<ul> <li>https://blog.bensoer.com/install-mongodb-on-linux-mint/ </li> <li>http://glenngeenen.be/untitled/ </li> <li>https://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/ </li> </ul>"},{"location":"services/mysql/","title":"Mysql","text":""},{"location":"services/mysql/#install-mysql","title":"Install MySQL","text":"<p>There is a bug where resetting the root password at the wrong point during the installation will cause MySQL to be installed on Ubuntu in a bad state. The following steps circumvent this issue.</p>"},{"location":"services/mysql/#ubuntu","title":"Ubuntu","text":"<ol> <li>Run <code>sudo apt-get install mysql-server</code> and let the install execute. During this install you will be prompted to reset the root password. Do Not Reset The Password At This Point. Leave the field blank and carry on with the installation. MySQL will use its default password at this point.</li> <li>Run <code>sudo mysql_install_db</code>. If the previous step worked, this will run regardless of what directory you are located in</li> <li>Run <code>sudo mysql_secure_installation</code>. This will prompt you through a number of steps to secure your MySQL database. One of these steps includes resetting the root password. Reset the Root Password When Prompted During This Command Execution</li> </ol>"},{"location":"services/mysql/#create-a-user-for-a-database-with-full-access","title":"Create A User For A Database With Full Access","text":"<p>The following allows you to create a new user and give them full permissions to either a specific table in a database or a whole database. This assumed you have already installed MySQL</p> <ol> <li>Login to MySQL through terminal:      <pre><code>mysql -u root -h localhost -p\n</code></pre>     Press enter and mysql will prompt for your password</li> <li>Execute      <pre><code>CREATE USER &lt;username&gt; IDENTIFIED BY &lt;password&gt;;\n</code></pre>     This will create your user</li> <li> <p>Execute one of the following lines depending on what permissions you wish to give:</p> Command Description <code>GRANT ALL PRIVILEGES ON &lt;database&gt;.* TO &lt;username&gt;;</code> Give a user full privileges to any table of a specific database <code>GRANT ALL PRIVILEGES ON &lt;database&gt;.&lt;tablename&gt; TO &lt;username&gt;;</code> Give a user full privileges to a specific table in a specific database <code>GRANT ALL PRIVILEGES ON *.* TO &lt;username&gt;;</code> Give a user full privileges to any table of any database. This essentially gives the user the equivalent permissions as the root user </li> </ol> <p>Some additional calls that may be of use when creating these users:</p> Command Description <code>SELECT User, Host, Password FROM mysql.user</code> List all the known users and their host access. '%' Host means only over localhost <code>SHOW GRANTS FOR &lt;username&gt;</code> List all grant permissions given, this is useful to double check the privileges assigned have gone through"},{"location":"services/mysql/#resources","title":"Resources","text":"<ul> <li>Digital Ocean LAMP tutorial: https://www.digitalocean.com/community/tutorials/how-to-install-linux-apache-mysql-php-lamp-stack-on-ubuntu-14-04</li> <li>MySQL CREATE USER Docs: https://dev.mysql.com/doc/refman/5.1/en/create-user.html</li> <li>MySQL GRANT Docs: https://dev.mysql.com/doc/refman/5.1/en/grant.html</li> </ul>"},{"location":"services/nfs/","title":"Nfs","text":"<p>NFS is essentially the predecessor of SAMBA. It is strictly limited to UNIX and LINUX systems, and has some major security flaws. It is still popularity used though due to its integration with mount and ability to treat network drives like local mounted drives. Also its simplistic yet powerful configurations make it a much simpler alternative to SAMBA</p> <p>==Install NFS== ===Fedora/CentOS=== dnf install nfs-utils</p> <p>Note: That this is to install nfs on the server whose folders will be shared after setup</p> <p>==Setup NFS== BEFORE DOING ANYTHING, after installing NFS create an exports file under <code>/etc</code> so as to not to accidentaly give full access to the file system.   sudo touch /etc/exports The exports file is where we place our configuration information for nfs</p> <p>On each line enter    An Example:   /home/shareFolder 192.168.0.0/24(rw,no_root_squash) <p>Note you can specify an explicit address as the second parameter, or specify a range using decimal subnet mask notation (ip number \\ valid bits). The above written example would give anybody with a prefix of 192.168.0 access to the /home/shareFolder and is allows read/write access and if accessed with root, the user permissions of the client will not be squashed while in the drive</p> <p>See permissions chart for common permission options and settings. Man is also your friend here</p> <p>==Permissions== Referre to man documents for all options available {| class=\"wikitable\" !Permission !Value |- |ro||Give Read-Only Access |- |rw||Give Read/Write Access |- |no_root_squash|| Allows the root user on the client to have root access on the share folder mounted. By default this is disabled and the client user has rights equal to the nobody user |- |no_subtree_check|| NFS will validate that the requested directory the client is within the correct directory. This is useful if only part of the volume is shared, If all of the volume is shared this will slow down transfers |- |sync|| This effects the exports command. Changes to run synchronizations synchronously vs. the default asynchronous. Warning though that this may cause data corruption if the server reboots or turns off |}</p> <p>==Start NFS Server== ===Fedora/CentOS=== Enabled the nfs service with:   systemctl enable nfs-server.service Then start the nfs server:   systemctl start nfs-server You can also then stop the server at any time with   systemctl stop nfs-server</p> <p>Note that there is a bug in nfs with the restart command as it does not always fully restart nfs. It is in this case best practice to stop and then start the server instead of restarting</p> <p>Alternatively you can also use this command while the server is running to update your changes...but I have never had it work correctly. To gaurantee the NFS server to update with new changes. stop and start the server   exportsfs -v</p> <p>==Connecting to an NFS Share== Using NFS as a client is very easy, just use the mount command as if you were mounting a local drive   mount -t nfs : <p>The <code>ip-of-server</code> is the IP of the server with nfs installed and sharing files. <code>dir-of-share</code>is the directory on the server that is being shared. This is the exact same dir as specified in the <code>/etc/exports</code> file configured on the server side. The <code>local-system-dir</code> is then the location on your local computer you would like to mount the location. This will be the directory you will access the share folders on your computer. A common location is <code>/mnt</code></p> <p>An Example:   mount -t nfs 192.168.0.123:/home/sharefolder /mnt/shareserver</p> <p>==Notes==</p> <p>==Sources==</p>"},{"location":"services/nmap/","title":"Nmap","text":"<p>Nmap or \"Network Mapper\" is a bulk network mapper giving in-depth details using various types of packets (including broken/invalid/corrupt ones). In simple essence, it is a port scanning tool that is able to determine and accurately guess vulnerabilities in a host that nmap is run on.</p> <p>By default nmap performs a SYN scan against any complaint TCP stack. This avoids an platform specific issues. Nmap can be used to scan from 1 to thousands of hosts and determine the states of any of those systems</p> <p>Nmap can be execute in console using the command <code>nmap</code> or with a GUI by entering <code>nmapfe</code> into a terminal</p> <p>==Installation== Nmap can be installed on most linux distros through the package manger   sudo apt-get install nmap #Ubuntu/Debian <p>sudo dnf install nmap #Fedora/CentOS</p> <p>You can download a version for windows here: https://nmap.org/download.html</p> <p>==Port States== After a scan, nmap prints out a report of its findings along with the state of each port it has scanned. Each state has a specific meaning {| class=\"wikitable\" !State !Description |- |OPEN||An application on the target host is listening for connections/packets on that port |- |FILTERED||There is a firewall, or something else that is blocking the port and nmap cannot detect if it is open or closed |- |UNFILTERED||A port is classified as unfiltered when nmap cannot detect if it is open or closed; However, it is responsive to probes |- |CLOSED|| There is no application that is listening on that port |}</p> <p>Filtered vs Unfiltered is simply determined as to whether the packets sent by nmap are being dropped or not. It is a calculation based on response time and type of packets sent to the server</p> <p>==Configuring Nmap Ports== By default nmap will scan all ports inclusively up to 1024, and then higher numbered ports that are listed in the nmap-services file. We can specify what ports to scan using the <code>-p</code> flag Additionaly by using <code>U&lt;/code and <code>T</code> syntax we can specify what protocol to use for each port as well.   nmap -p U:53,T:10-111 www.host.com  The above example will scan port 53 on the UDP protocol and ports 10 - 111 on the TCP protocol. The end system that will be scanned is www.host.com <p>==Hiding From An IDS== The main problem with nmap is that it is an exceptionally noisy program. Many intrusion detection systems (IDS) can pick up nmap scans immediatly. In the interest of stealthy reconnaisance, nmap has a number of features it mitigate if not remove chances of being detected by an IDS.</p> <p>===Timing=== nmap scans can be timed to run at slower intervals. If the server is highly active, the mixing in of nmap traffic will be lost. An IDS may either not pickup on it due to the irregularity of the packets, or it may be identified as a false posotive by the network administrator</p> <p>nmap has the following parameters to configure timing of a scan {| class=\"wikitable\" !Timing Value (T) !Type !Description |- |1||Paranoid||Sends a packet every 5 seconds |- |2||Sneaky||Sends a packet every 15 seconds |- |3||Polite||Sends a packet every 0.4 seconds |- |4||Normal||Automatically, as quick as possible |- |5||Aggressive||Wait 1.2 seconds for a response before skipping |- |6||Insane||Waits for 0.3 seconds for a response before skipping |} Needless to say to avoid detection, using the Aggressive and Insane modes are highly not recommended. Additionally, Aggressive and Insane modes will skip ports if there is no response within the wait time, thus making the end resulting reports to contain errors.</p> <p>Using the Paranoid and Sneaky options will create a highly reliable report and will reduce detection, but will take much longer to complete</p> <p>An example of using the Paranoid mode in an nmap scan would look like this   nmap -T1 www.host.com</p> <p>===Decoy Packets=== Decoy scanning is another option with nmap where it will send multiple spoof packets along with its own to mitigate IDS detection.</p> <p>The syntax looks like this   nmap -D  <p>Where decoy1 and decoy2 are explicitly selected IPs that will be used as decoys. The keyword \"ME\" can then be used to represent nmaps own IP. By placing \"ME\" in the list of decoys, nmap will send the packets in the order specified with its own legitimate packet always in the same position relative to the decoys. If the keyword \"ME\" is not included, nmap will simply place its own IP in random positions in the decoy list.</p> <p>Note that in order for decoy packets to work, the decoy IPs must belong to a legitimate system. If they are not, the host being scanned is likely to become a target of a SYN Flood and be bombarded with SYN/ACKS</p> <p>The decoy packet option additionally has the ability to generate a random number of decoy packets to be used. These IPs are randomly calculated on the source host. As noted already above though, this option is strongly not recommended as you will be unable to determine whether the randomly generated IPs are valid or not. The syntax for nmap to generate random IP decoy packets is as follows   nmap -D RND:[numberofips] [host]</p> <p>nmap fortunately though offers some tools for ensuring legitimate IPs are selected   nmap -iR 100 www.host.com This will have nmap scan www.host.com but use up to 100 randomly chosen legitimate IP addresses to use as decoys during its scan. nmap will automaticaly randomize itself in with the packets</p> <p>==Scanning Ping Dropping Hosts== Many end systems have configured their firewalls to drop all ICMP ping requests. This poses a problem for nmap which by default, before each scan, attempts to ping the host system to determine if it is up. This allows it to skip hosts when scanning a network that may not have all of its IPs assigned. If the host blocks ping, nmap will end up skipping that host and not scanning it. Even by giving an explicit IP, by default if the ping request fails, the scan is skipped. To avoid this problem pass the -Pn parameter. This tells nmap to not ping the host initialy to determine if it exists, and to scan it regardless. When scanning a host you know exists, this is exceptionaly useful</p> <p>An example of a scan with the initial ping removed may look like this:   nmap -Pn www.host.com</p> <p>==Other Notable Flags==</p> <p>{| class=\"wikitable\" !Flag !Description !Example |- | -O ||Enables OS Fingerprint Scanning||nmap -O www.host.com |- | -A ||Enable OS Fingerprint Scanning along with version detection, script scanning and traceroute||nmap -A www.host.com |- | -S ||Spoof The Source IP. Not recommended flag as nmap will then not receive the response||nmap -S 192.168.0.1 www.host.com |- | -6 ||Enable IPv6 Scanning||nmap -6 www.host.com |- | -sn ||Executes a Ping scan. No port scanning occurs. Useful for determining what hosts in a network are up||nmap -sn 192.168.0.1/24 |}</p> <p>Full listing of flags can be found in the man pages aswell by typing man nmap in the terminal</p> <p>==Notes==</p> <p>==Sources== http://linux.die.net/man/1/nmap</p>"},{"location":"services/openvpn/","title":"Openvpn","text":"<p>OpenVPN is a open source VPN application that uses a unique security protocol to implement virtual private networks. Read more about it at: [https://en.wikipedia.org/wiki/OpenVPN Wikipedia] [https://openvpn.net/index.php/open-source/documentation.html OpenVPN Documentation]</p> <p>== Install OpenVPN == OpenVPN comes with Easy-RSA, a lightweight package for using the RSA encryption method. The install needs to be run as root so execute <code>sudo su</code> ===Adding apt repositiories=== ONLY DO THIS IF YOU ARE RUNNING A i386 OR AMD64 ARCHITECTURE! Lets add OpenVPN's apt repository to our own list of repositories so that we can get a more up-to-date release than what is in your distribution's repositories. Run the following commands: :<code>wget -O - https://swupdate.openvpn.net/repos/repo-public.gpg|apt-key add -</code> :<code>echo \"deb http://build.openvpn.net/debian/openvpn/stable  main\" &gt; /etc/apt/sources.list.d/openvpn-aptrepo.list Where  depends your distribution: squeeze (Debian 6.x) wheezy (Debian 7.x) jessie (Debian 8.x) lucid (Ubuntu 10.04) precise (Ubuntu 12.04) trusty (Ubuntu 14.04) <p>===Install===  Now run <code>apt-get install openvpn</code> and let the install execute. Easy-rsa should be installed along with it, otherwise [https://wiki.bensoer.com/index.php/Apt-get#Usage update and install your dependencies].</p> <p>OpenVPN installs to /etc/openvpn.</p> <p>== Setting Up SSL Certificates == Lets get cryptographic. === Preparing Easy-RSA ===</p>"},{"location":"services/openvpn/#create-an-easy-rsa-folder-in-your-openvpn-install-directory-mkdir-etcopenvpneasy-rsa","title":"Create an Easy-RSA folder in your OpenVPN install directory <code>mkdir /etc/openvpn/easy-rsa</code>","text":""},{"location":"services/openvpn/#move-your-easy-rsa-install-to-your-openvpn-install-directory-cp-r-usrshareeasy-rsa-etcopenvpn","title":"Move your Easy-RSA install to your OpenVPN install directory <code>cp -r /usr/share/easy-rsa /etc/openvpn/</code>","text":""},{"location":"services/openvpn/#navigate-to-the-easy-rsa-folder-we-just-created-cd-etcopenvpneasy-rsa","title":"Navigate to the Easy-RSA folder we just created <code>cd /etc/openvpn/easy-rsa</code>","text":""},{"location":"services/openvpn/#edit-the-easy-rsa-variables-file-to-generate-keys-in-our-new-etcopenvpneasy-rsa-directory-nano-vars-change-easy_rsa-variable-to-export-easy_rsaetcopenvpneasy-rsa-and-change-key_size-variable-to-export-key_size4096-also-change-the-default-values-for-key_countryus-key_provinceca-key_citysanfrancisco-key_orgfort-funston-key_emailmemyhostmydomain-and-key_oumyorganizationalunit-so-you-dont-have-to-do-it-multiple-times-later-in-the-key-generation-process","title":"Edit the Easy-RSA variables file to generate keys in our new /etc/openvpn/easy-rsa directory <code>nano vars</code> change EASY_RSA variable to: <code>export EASY_RSA=\"/etc/openvpn/easy-rsa\"</code> and change KEY_SIZE variable to: <code>export KEY_SIZE=4096</code> also change the default values for KEY_COUNTRY=\"US\", KEY_PROVINCE=\"CA\", KEY_CITY=\"SanFrancisco\", KEY_ORG=\"Fort-Funston\", KEY_EMAIL=\"me@myhost.mydomain\", and KEY_OU=\"MyOrganizationalUnit\" so you don't have to do it multiple times later in the key generation process.","text":"<p>=== Building and Signing you Certificate Authority ===</p>"},{"location":"services/openvpn/#build-your-certificate-authorityca-source-varsclean-allbuild-ca","title":"Build your certificate authority(CA): <code>source ./vars</code><code>./clean-all</code><code>./build-ca</code>","text":""},{"location":"services/openvpn/#during-the-build-process-it-will-prompt-you-for-some-name-values-for-the-ca-if-you-modified-and-sourced-the-vars-file-correctly-these-values-should-default-to-the-ones-you-entered-earlier","title":"During the build process it will prompt you for some name values for the CA. If you modified and sourced the vars file correctly these values should default to the ones you entered earlier.","text":""},{"location":"services/openvpn/#sign-your-ca-build-key-server-server_name-it-will-prompt-you-for-those-same-values-common-name-must-be-the-same-as-server_name-you-just-picked-you-must-leave-the-challenge-password-blank","title":"Sign your CA: <code>./build-key-server \"''Server_Name''\"</code> It will prompt you for those same values. Common Name must be the same as \"''Server_Name''\" you just picked. You MUST leave the Challenge Password blank.","text":"<p>=== Creating and Signing Clients === For desktop clients use: <code>./build-key-pass \"''UserName''\"</code> For mobile clients use: <code>./build-key-pkcs12 \"''UserName''\"</code> When prompted to Enter PEM pass phrase enter a password that will be used to encrypt your private key. Make sure to remember it as you will need it each time you want to connect to your VPN. Again you MUST leave the Challenge Password blank. You could only generate one client key and use the same certificate for every device you wanted to connect to the VPN, however each device connected to the VPN concurrently needs to have a unique certificate and username.</p> <p>=== Build the DH parameters === [https://wiki.openssl.org/index.php/Diffie_Hellman Diffie Hellman] parameters must be generated for the OpenVPN server. Run the following commands: :<code>cd /etc/openvpn/easy-rsa/</code> :<code>./build-dh</code> This process will take the longest time to compute as it is using random numbers and looking for some specific relationships.</p> <p>=== OpenVPN's DoS Protection === OpenVPN has build in protection against a hacker finding out your server\u2019s address, and generating such a large number of access requests that your server crashes. By generating a [https://en.wikipedia.org/wiki/Hash-based_message_authentication_code HMAC] key, your server wont attempt to authenticate unless it sees this static key first. Generate one by: :<code>openvpn \u2013-genkey \u2013-secret keys/ta.key</code></p> <p>=== Understanding your Keys === Now we will find our newly-generated keys and certificates in the keys subdirectory. Here is an explanation of the relevant files: {| class=\"wikitable\" !Filename !Needed By !Purpose !Secret |- |ca.crt||server + all clients||Root CA certificate||NO |- |ca.key||key signing machine only||Root CA key||YES |- |dh{n}.pem||server only||Diffie Hellman parameters||NO |- |server.crt||server only||Server Certificate||NO |- |server.key||server only||Server Key||YES |- |client1.crt||client1 only||Client1 Certificate||NO |- |client1.key||client1 only||Client1 Key||YES |- |client2.crt||client2 only||Client2 Certificate||NO |- |client2.key||client2 only||Client2 Key||YES |- |client3.crt||client3 only||Client3 Certificate||NO |- |client3.key||client3 only||Client3 Key||YES |- |ta.key||server + all clients||TA Key||YES |}</p> <p>The final step in the key generation process is to copy all files to the machines which need them, taking care to copy secret files over a secure channel like <code>scp</code>.CONGRATULATIONS! You have now created all required SSL certificates needed for your VPN.</p> <p>==Config Files== It would take too long to go into details about everything in these files. You can find details in the resources mentioned above. ===Server Config=== Edit /etc/openvpn/server.conf to contain the following:   port 1194 proto udp dev tun <p>ca ca.crt cert server.crt key server.key</p> <p>dh dh4096.pem</p> <p>tls-auth ta.key 0</p> <p>topology subnet server 10.8.0.0 255.255.255.0</p>"},{"location":"services/openvpn/#add-route-for-local-subnet","title":"Add route for local subnet","text":"<p>push \"route 192.168.0.0 255.255.255.0\"</p>"},{"location":"services/openvpn/#all-clients-redirect-their-default-network-gateway-through-the-vpn","title":"all clients redirect their default network gateway through the VPN","text":"<p>push \"redirect-gateway def1\"</p>"},{"location":"services/openvpn/#dns-servers-provided-by-opendnscom","title":"DNS servers provided by opendns.com.","text":"<p>;push \"dhcp-option DNS 208.67.222.222\" ;push \"dhcp-option DNS 208.67.220.220\"</p>"},{"location":"services/openvpn/#or-google-developers-public-dns","title":"Or Google Developers Public DNS","text":"<p>;push \"dhcp-option DNS 8.8.8.8\" ;push \"dhcp-option DNS 8.8.4.4\" push \"dhcp-option DNS 192.168.0.1\"</p> <p>client-to-client</p> <p>keepalive 10 120</p> <p>cipher AES-256-CBC</p> <p>comp-lzo</p> <p>max-clients 5</p> <p>user nobody group nogroup</p> <p>persist-key persist-tun</p> <p>status /var/log/openvpn/openvpn-status.log log /var/log/openvpn/openvpn.log verb 3</p> <p>===Client Config=== The following is used in a client.conf by the openVPN application on a client computer.   client proto udp dev tun <p>remote my-openVPN-server-IP 1194 resolv-retry infinite</p> <p>nobind</p> <p>user nobody group nobody</p> <p>persist-key persist-tun</p> <p>ca ca.crt cert client.crt key client.key</p> <p>remote-cert-tls server</p> <p>tls-auth ta.key 1</p> <p>cipher AES-256-CBC</p> <p>auth-nocache</p> <p>comp-lzo</p> <p>verb 3</p> <p>===Server Firewall Setup=== Choose between a simple or more secure firewall script. ====Basic==== Since in this setup we are routing all of the clients internet traffic through the VPN we need to be configured to deal with this traffic somehow, such as by NATing it to the internet. This can be accomplished by: :<code>iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE</code> Note that this solution will only get things up and running. It provides no security whatsoever to your VPN Server.</p> <p>====Advanced==== Below is a link to a sample IPTables firewall that can be tweaked to control traffic passing through the VPN. This configuration includes the masquerading rules mentioned above along with additional security measures for your VPN firewall. This script includes rules to only allow systems located in the local network of the VPN can access the VPN via SSH. Additionally, VPN users cannot access the VPN Server or the Gateway Router, but do have access to all other machines in the local network, and the internet outside of the network. A commented section has been highlighted in the script for users to add additional rules to control what data is allowed through the network.</p> <p>https://gist.github.com/bensoer/9fec3386a9460c42511bf8fd49b4a6d8</p> <p>To use this script:</p> <ol> <li>Download and copy it to your VPN server with IPTables installed.</li> <li>Open the script using a text editor and update the variables listed in the USER CONFIGURATION section to match that of your network the VPN is being hosted in (see script comments for details).</li> <li>Execute the script with the following command: <code>sudo ./openvpnfirewall.sh</code></li> </ol> <p>Status messages will print out as each section of the firewall is applied. To change the firewall, simply update the script and rerun it. The script contains logic to clear the previous firewall settings before applying the newly listed ones. If there is an error, follow the prompts given in the console with their locations in the script (a printout in the shell script is preceded with the command 'echo') and adjust the code as needed.</p> <p>====Script Execution and Firewall Persistance==== =====Using The Network Interface===== In order for this iptables rule to be applied every time you turn on the VPN, you can turn it into a shell script and modify /etc/network/interfaces so it will execute the script every time it initializes the network interface eth0.   ... iface eth0 inet dhcp         pre-up script.sh ... =====Using iptables-save===== You can also use iptables-save a tool available for Debian, Ubuntu, CentOS and RHEL based distros. After executing your script run the following command to export it:"},{"location":"services/openvpn/#debian-ubuntu","title":"Debian / Ubuntu","text":"<p>iptables-save &gt; /etc/iptables/rules.v4 ip6tables-save &gt; /etc/iptables/rules.v6</p>"},{"location":"services/openvpn/#rhel-centos","title":"RHEL / CentOS","text":"<p>iptables-save &gt; /etc/sysconfig/iptables iptables-save &gt; /etc/sysconfig/ip6tables You may need to mkdir the directory if this is the first use. Ensure also that these directories only have read and write permissions for sudo, and only read for others (sudo chmod 777)</p> <p>Then install iptables-persistent  sudo apt-get install iptables-persistent The installation will prompt to re-export the current iptables rules again. You can say no at this step since we just did it. The iptables-persistance will then ensure that everytime the system boots up the the iptables rules we exported above are impported and set. To update the persistence rules at a later time, simply re-export them as done initialy</p> <p>To explore the iptables-save its co-tool iptables-restore and iptables-persistent. See the [[Iptables]] page for details</p> <p>====Enabling Forwarding==== If you are running a Raspbian VPN you have to modify /etc/sysctl.conf to allow packet forwarding. Uncomment the line <code>net.ipv4.ip_forward=1</code> Apply the changes we just made by executing: <code>sysctl -p</code></p> <p>==Revoking Certificates== ===Using Easy-RSA=== Easy-RSA handles revoking client certificates. Navigate to your Easy-RSA directory in your OpenVPN folder: :<code>cd /etc/openvpn/easy-rsa/</code> Run the following to revoke the certificate for the client called 'unwanted-client': :<code>./revoke-full unwanted-client</code> The index.txt file in you keys directory will be updated. You\u2019ll see an \u2018R\u2019 (for Revoked) on the first column from the left for unwanted-client. View this with: :<code>cat keys/index.txt</code> You can also examine the CRL (certificate revocation list) file with: :<code>openssl crl -in keys/crl.pem -text</code> ===Configure OpenVPN=== Now we need to configure our OpenVPN server to enable CRL verification. Add the following line to your server.conf file: :<code>crl-verify /etc/openvpn/easy-rsa/keys/crl.pem</code> Reload the OpenVPN config file by: :<code>service openvpn reload</code> When the crl-verify option is used in OpenVPN, the CRL file will be re-read any time a new client connects or an existing client renegotiates the SSL/TLS connection (by default once per hour). This means that you can update the CRL file while the OpenVPN server daemon is running, and have the new CRL take effect immediately for newly connecting clients.</p> <p>==Adding LDAP for Active Directory Authentication== LDAP is the underlying protocol of Active Directory. In an organization context where there may be multiple servers, controlling authentication in a single place may be desirable. This can also be applied to OpenVPN. OpenVPN can be configured to authenticate with both Certificates and LDAP or LDAP alone. The following instructions will configure for both, as it is the more secure option</p> <p>===Installation=== On your linux machine install :<code>apt-get install openvpn-auth-ldap</code> Depending on whether you are RPM or Debian based will change where the module is installed. On Debian the module will be installed in /etc/openvpn/openvpn-auth-ldap.so. On RPM it is installed in /etc/openvpn/plugin/lib/openvpn-auth-ldap.so</p> <p>You will also need to create the following directory to store your ldap configuration :<code>mkdir /etc/openvpn/auth</code></p> <p>===Configuration=== Copy the following into ldap.conf inside your newly created directory:"},{"location":"services/openvpn/#ldap-server-url","title":"LDAP server URL","text":"<p>URL ldap://dc-test-1.test.com:389</p>"},{"location":"services/openvpn/#bind-dn-if-your-ldap-server-doesnt-support-anonymous-binds","title":"Bind DN (If your LDAP server doesn't support anonymous binds)","text":""},{"location":"services/openvpn/#binddn-uidadminouusersdctestdccom","title":"BindDN uid=admin,ou=Users,dc=test,dc=com","text":"<p>BindDN admin@test.com</p>"},{"location":"services/openvpn/#-for-domain-users-also-works","title":"- for domain users \\ also works","text":""},{"location":"services/openvpn/#bind-password","title":"Bind Password <p>Password humus</p>","text":""},{"location":"services/openvpn/#-this-is-the-password-to-login-to-the-ldap-server-to-authenticate-connecting-users","title":"- this is the password to login to the LDAP server to authenticate connecting users","text":""},{"location":"services/openvpn/#network-timeout-in-seconds","title":"Network timeout (in seconds) <p>Timeout 15</p>","text":""},{"location":"services/openvpn/#enable-start-tls","title":"Enable Start TLS <p>TLSEnable no</p>","text":""},{"location":"services/openvpn/#follow-ldap-referrals-anonymously","title":"Follow LDAP Referrals (anonymously) <p>FollowReferrals yes</p>","text":""},{"location":"services/openvpn/#-only-add-the-following-if-you-are-connecting-securely-to-the-adldap-server","title":"- Only Add The Following if you are connecting securely to the AD/LDAP server","text":""},{"location":"services/openvpn/#tls-ca-certificate-file","title":"TLS CA Certificate File <p>TLSCACertFile /usr/local/etc/ssl/ca.pem</p>","text":""},{"location":"services/openvpn/#tls-ca-certificate-directory","title":"TLS CA Certificate Directory <p>TLSCACertDir /etc/ssl/certs</p>","text":""},{"location":"services/openvpn/#client-certificate-and-key","title":"Client Certificate and key","text":""},{"location":"services/openvpn/#if-tls-client-authentication-is-required","title":"If TLS client authentication is required <p>TLSCertFile /usr/local/etc/ssl/client-cert.pem TLSKeyFile /usr/local/etc/ssl/client-key.pem</p>","text":""},{"location":"services/openvpn/#cipher-suite","title":"Cipher Suite","text":""},{"location":"services/openvpn/#the-defaults-are-usually-fine-here","title":"The defaults are usually fine here","text":""},{"location":"services/openvpn/#tlsciphersuite-alladhstrength","title":"TLSCipherSuite ALL:!ADH:@STRENGTH  <p>","text":""},{"location":"services/openvpn/#base-dn","title":"Base DN","text":""},{"location":"services/openvpn/#basedn-cnusersdctestdccom","title":"BaseDN \"CN=Users,DC=test,DC=com\" <p>BaseDN \"CN=Users,DC=test,DC=com\"</p>","text":""},{"location":"services/openvpn/#-basedn-sets-the-base-point-to-start-searching-for-users-to-search-everywhere-in-the-domain-remove-the-cn-portion-each-dc-is-a-part-of-the-domain-split-by-the-dots","title":"- BaseDN sets the base point to start searching for users. To search everywhere in the domain, remove the CN portion. Each DC is a part of the domain split by the dots","text":""},{"location":"services/openvpn/#user-search-filter","title":"User Search Filter","text":""},{"location":"services/openvpn/#searchfilter-uiduaccountstatusactive","title":"SearchFilter \"(&amp;(uid=%u)(accountStatus=active))\"","text":""},{"location":"services/openvpn/#searchfilter-samaccountnameumsnpallowdialintrue","title":"SearchFilter \"(&amp;(sAMAccountName=%u)(msNPAllowDialin=TRUE))\" <p>SearchFilter \"(&amp;(sAMAccountName=%u))\"</p>","text":""},{"location":"services/openvpn/#-searchfilter-is-the-search-query-made-within-the-basedn-location-the-above-one-searchs-for-matching-usernames","title":"- SearchFilter is the search query made within the BaseDN location - the above one searchs for matching usernames","text":""},{"location":"services/openvpn/#require-group-membership","title":"Require Group Membership","text":""},{"location":"services/openvpn/#-add-this-if-you-want-to-control-the-ad-users-needing-to-belong-to-a-group-comment-out-if-not","title":"- Add this if you want to control the AD users needing to belong to a group. Comment out if not <p>RequireGroup true</p>","text":""},{"location":"services/openvpn/#add-non-group-members-to-a-pf-table-disabled","title":"Add non-group members to a PF table (disabled)","text":""},{"location":"services/openvpn/#pftable-ips_vpn_users","title":"PFTable ips_vpn_users","text":""},{"location":"services/openvpn/#-this-section-is-only-required-if-requiregroup-is-true-however-it-can-still-be-here-even-if-it-is-removed","title":"- This section is only required if RequireGroup is true, however it can still be here even if it is removed  BaseDN \"CN=Users,DC=test,DC=com\" # - BaseDN sets the base point to start searching for users. To search everywhere in the domain, remove the CN portion and replace each DC with each part of the domain SearchFilter \"(cn=vpn-users)\" # - vpn-users is the name of the group that users must be part of to join MemberAttribute \"member\" # Add group members to a PF table (disabled) #PFTable ips_vpn_eng   <p>After saving and configurating the above contents in ldap.conf, add the following line to the bottom of the main server.conf   plugins /etc/openvpn/openvpn-auth-ldap.so \"/etc/openvpn/auth/ldap.conf\"  This tells openvpn to use the ldap plugin and load the just created plugin</p> <p>Finaly, include the following line in the server.conf depending on what you would like to do: * To force Certificate and LDAP authentication add <code>auth-user-pass</code> below the plugin line * To only require LDAP authentication add <code>client-cert-not-required</code> below the plugin line</p> <p>Finally give openvpn a restart to load the plugin. Check the logs to ensure configuration is correct (Authentication for LDAP typically is an issue if it is not the proper format)</p> <p>For full details on this configuration see: https://www.allcloud.io/how-to/configure-openvpn-authentication-using-active-directory/ for details</p> <p>== Notes == easy-rsa might not have installed to /usr/share/easy-rsa. It should be somewhere in /usr/share, but you might have to search to find the exact path. The mathematical relation between a 4096 bit SSL key pair takes too long to generate on a small computer like a RaspberryPi, use 2048. *the n in dh{n}.pem is the size of your SSL keys (2048 or 4096)</p> <p>==Sources==</p> <p>https://openvpn.net/index.php/open-source/documentation/howto.html http://readwrite.com/2014/04/10/raspberry-pi-vpn-tutorial-server-secure-web-browsing https://www.thomas-krenn.com/en/wiki/Saving_Iptables_Firewall_Rules_Permanently  https://openvpn.net/index.php/open-source/documentation/miscellaneous/77-rsa-key-management.html https://jamielinux.com/docs/openssl-certificate-authority/introduction.html  https://www.allcloud.io/how-to/configure-openvpn-authentication-using-active-directory/ https://github.com/threerings/openvpn-auth-ldap/blob/master/auth-ldap.conf</p>","text":""},{"location":"services/rsyslog/","title":"Rsyslog","text":"<p>rsyslog is the linux logging tool for all system activity. There are a number of useful things you can do with it to evaluate and debug problems with your server with the logs</p> <p>==Directories== * Main system logs are located: <code>/var/log/messages</code>  This is a catchall log for any logging on the system * rsyslog configuration file: <code>/etc/rsyslog.conf</code> * rsyslog daemon: <code>/sbin/rsyslogd</code></p> <ul> <li>Root logins, user logins, su attmpts are located: <code>/var/log/secure</code></li> <li> <p>Mail traffic is logged to: <code>/var/log/maillog</code></p> </li> <li> <p>Error Message from uuvp and news server (innd) daemons: <code>/var/log/spooler</code>  (This one is not used by most systems anymore)</p> </li> </ul> <p>Some versions of Fedora don't have any of these log files aswell and instead use the <code>journalctl</code> command to access and filter through the logs. rsyslog though can be installed aswell to work side by side with journalctl and will not interfere</p> <p>==Configure Remote Logging== Remote logging is an important security functionality so that hackers can not alter the log files and remove their presence on the machine. By sending copies of your logs to a machine completely dedicated and locked down to only archive logs will ensure that logs will not be tampered with</p> <p>You can configure a system to send its logs to a remote server by editing the <code>/etc/syslog.conf</code> file. Add an entry to the bottom of the conf file that looks something like this   . @:514 <p>Facility and prioiryt are used to specify and filter what log information is sent to the log server. See the man pages for full documentation on the different facilites and priorities are available to filter the send logs. Note that priority is used as a threshhold and logs from that threshold and up are sent to the server.</p> <p>To send all logs from each facility and each priority level you can just use <code>.</code></p> <p>Example:   . @144.272.38.38:514 Note also the importance of port 514. This is the typical default port number most loggings servers are configured to use. You can though of course configure this on your logging server.</p> <p>==Configure Logging Server== With rsyslog you can easily configure your logging server to recieve external log files with a few commands to rsyslog on startup</p>"},{"location":"services/rsyslog/#open-etcrsyslogconf","title":"Open <code>/etc/rsyslog.conf</code>","text":""},{"location":"services/rsyslog/#uncomment-or-add-the-following-lines","title":"Uncomment or add the following lines:","text":"<p> $ModLoad imudp $UDPServerRun 514 <p>$ModLoad imtcp $InputTCPServerRun 514 If you know what transportation protocol your server uses you only need to add/uncomment the corresponding lines.</p> <p>If you wanted to change the port number to receive log files, change the value assigned to the <code>$UDPServerRun</code> variable</p> <p>For the changes to take effect, restart rsyslog   sudo systemctl restart rsyslog #Fedora/CentOS sudo service rsyslog restart #Ubunt</p> <p>==Configure RSysLog To Use A Different Conf Directory== RSysLog can be configured to load its configuration details form a different directory. Unfortunately this implementation so far has to be done manually and does not persist past a restart.</p> <p>1) copy the rsyslog.conf file (by default in /etc/rsyslog.conf) to the new location you would like it to be referenced by 2) Shutdown the currently running rsyslog service with the following command   sudo service rsyslog stop 3) Startup the rsyslog daemon manualy with the following command   rsyslogd -n -f /new/conf/directory/rsyslog.conf &amp;  4)Press Enter Twice. Once Loads the program, Second releases terminal from the process Note the '&amp;' at the end is important to ensure the daemon executes in its own process</p> <p>This setup may appear a bit obvious as someone may login and find rsyslog turned off, attempt to turn it on, and recieve errors. To resolve this issue execute step 3 with the following command instead   rsyslogd -n -i 1447 -f /new/conf/directory/rsyslog.conf &amp;  By default rsyslogd uses PID 1446 to loadup. If you are to run multiple instances of rsyslog on your system, you will need to provide each new instance with their own PID. This can be done simply with the -i parameter when loading rsyslogd</p> <p>==Facilities== {| class=\"wikitable\" ! Facility ! Message Category |- | auth or security || Security / Authorization |- | authpriv || Private Security / Authorization |- | cron || Cron Daemon Messages |- | daemon || System Daemon-Generated Messages |- | ftp || FTP Server Messages |- | kern || Kernel Messages |- | lpr || Printer SubSystem |- | news || Network News Subsystem |- | syslog || Syslog-Generated Messages |- | user || User Program-Generated Messages |- | UUCP || UUCP SubSystem |- | mail || Mail SybSystem |} ==Priorities== {| class=\"wikitable\" ! Priority ! Message type |- | debug || Debug messages |- | info || Informational status messages |- | notice || Normal but important conditions |- | warning or warn || Warning Messages |- | err or error || Error Messages |- | crit || Critical Conditions |- | alert || Immediate Attention Required |- | emerg or panic || System is unusable |}</p> <p>==Configure Remote Logging to A Database== RSysLog can log to a database. Documentation online is poor and there are a number of careful steps that must be met. But the procedure is possible. This procedure will install and configure remote logging to a MySQL database</p> <p>===Configure MySQL Database===</p>"},{"location":"services/rsyslog/#find-the-version-of-your-rsyslog-on-your-logging-server-this-can-be-done-by-restarting-rsyslog-and-then-checking-the-varlogsyslog-log-file-when-rsyslog-initializes-it-first-prints-its-version","title":"Find the version of your RSysLog on your Logging Server. This can be done by restarting RSyslog and then checking the /var/log/syslog log file. When RSysLog initializes it first prints its version","text":""},{"location":"services/rsyslog/#then-go-to-the-rsyslog-website-and-search-through-the-archives-for-a-download-of-the-same-version-as-your-rsyslog-server-download-it-onto-the-server-where-you-mysql-database-is-hosted","title":"Then go to the RSysLog website and search through the archives for a Download of the same version as your RSysLog server. Download it onto the server where you MySQL database is hosted","text":""},{"location":"services/rsyslog/#extract-the-folder-and-get-the-createdbsql-script-roughly-located-in-rsyslog-842pluginsommysqlcreatedbsql-run-this-script-on-mysql-can-either-from-console-or-by-import-in-mysql-toolbox-this-script-typicaly-includes-code-to-build-the-database-so-nothing-needs-to-exist-before-running-it-make-sure-to-create-a-user-account-that-can-access-the-database","title":"Extract the folder and get the 'createDB.sql' script roughly located in /rsyslog-8.4.2/plugins/ommysql/createDB.sql. Run this script on MySQL (can either from console or by import in MySQL Toolbox). This script typicaly includes code to build the database, so nothing needs to exist before running it. Make sure to create a user account that can access the database.","text":"<p>===Configure Logging Server===</p>"},{"location":"services/rsyslog/#apt-get-install-rsyslog-mysql-on-the-server-that-will-be-logging-to-your-mysql-database-skip-any-prompted-automated-setup-procedure","title":"apt-get install rsyslog-mysql on the Server that will be logging to your MySQL database. Skip any prompted automated setup procedure.","text":""},{"location":"services/rsyslog/#cd-to-etcrsyslogd-within-this-folder-during-the-install-will-be-a-mysqlconf-file-open-it-with-nano","title":"cd to /etc/rsyslog.d/ within this folder during the install will be a 'mysql.conf' file. Open it with nano","text":""},{"location":"services/rsyslog/#update-or-copy-into-the-file-the-following-information-substituting-your-own-information-where-necessary","title":"Update or copy into the file the following information, substituting your own information where necessary","text":"<p> $ModLoad ommysql . :ommysql:,,,  For my setup this looks something like this. Note that the host could either be localhost, in which case the logging server also is hosting the MySQL database, or it can be a domain or IP of another system hosting the MySQL database. RSysLog will resolve hostnames so a domain is acceptable aswell   $ModLoad ommysql . :ommysql:192.168.20.100,Syslog,username,supersecret  <ol> <li>Once configured restart rsyslog on the logging server with service rsyslog restart -r</li> <li>Login to your MySQL server and you should see events now arriving</li> </ol> <p>==Raspberry Pi Example== My router has the ability to log remotely to a Syslog server on its network so I decided to set one up on my Pi. It took a total of 15 minutes.</p> <ol> <li>Edit <code>/etc/rsyslog.conf</code> to listen for a connection on the default port of 514 (see above)</li> <li>Create your log file:</li> <code>sudo touch /var/log/router.log</code> <li>Configure rsyslog to use the log file you jut created</li> *Under <code>/etc/rsyslog.d/</code> create a file with the extension <code>.conf</code> :<code>sudo nano /etc/rsyslog.d/router.conf</code> *Add the following lines:   $template NetworkLog, \"/var/log/router.log\" :fromhost-ip, isequal, \"192.168.0.1\" -?NetworkLog &amp; ~ <li>Restart rsyslog (see above)</li> <li>Enable remote logging on your web interface of your router</li> <li>Configure logrotate</li> It is a good idea to configure the new log file you just created in <code>logrotate</code> to compress and remove the log file when it gets too big. *Create a file in <code>/etc/logrotate.d/</code> :<code>sudo nano router</code> *Add the following lines:   /var/log/router.log {         rotate 7         size 500k         notifempty         compress         postrotate                 invoke-rc.d rsyslog rotate &gt; /dev/null         endscript } </ol> <p>==Notes== If RSyslog is not recieivng logs try the following: * On the recieving host check and entry for <code>udp/514</code> exists under <code>/etc/services</code> * RSysLog may need to be started with the <code>-r</code> flag to allow remote logging. This may need to be done on the sender and/or reciever ** Example: <code>systemctl restart rsyslog -r</code> * Clear the Firewall with: #<code>iptables -F</code> #<code>iptables -X</code></p> <p>==Sources== https://linux.die.net/man/8/rsyslogd  https://linux.die.net/man/5/rsyslog.conf  https://linux.die.net/man/8/syslogd  http://serverfault.com/questions/542379/how-to-change-rsyslog-configuration-file-directory http://opensourceforu.com/2015/10/remote-logging-using-rsyslog-and-mysql/ http://www.rsyslog.com/doc/v8-stable/tutorials/recording_pri.html</p>"},{"location":"services/samba/","title":"SAMBA","text":"<p>==Install SAMBA== dnf install samba ==Configure SAMBA== For the most basic configuration and security setup, append the following to the bottom of the file   [NFSHARE]     comment=      path =      public =      writable =      guest ok =      printable =  Comment is just a comment that can be used to help identify the share.  Path is the directory that is being shared through SAMBA  Public is to enable public access to the share folder  Writable is to enable writing to the share folder  Guest Ok enabled anonymous access to the share folder  Printable is whether SAMBA will share printer access through this share folder  <p>==Start SAMBA== ===Fedora/CentOS===</p>"},{"location":"services/samba/#start-the-smb-service-systemctl-start-smbservice","title":"Start the SMB service: <code>systemctl start smb.service</code>","text":""},{"location":"services/samba/#then-start-the-samba-server-systemctl-start-smb","title":"Then start the SAMBA server: <code>systemctl start smb</code>","text":"<ul> <li>You can check if your mount has worked with the following command: <code>smbclient -L localhost</code></li> </ul> <p>Note that at this point no user accounts have been configured for SAMBA so the only authetnication available is anonymous access. When you run the above command it will prompt to enter a password. Do not enter a password and hit enter. This will attempt to then connect as if you are anonymous.</p> <p>==User Permissions On SAMBA== By default SAMBA is configured with 'user' security. For this report were going to configure further this default setup. To add security, were going to add a user and password. With how SAMBA is built, these credentials must match a user and password on the linux system.</p> <p>Add a SAMBA user with this command:   smbpasswd -a   Press enter and you may be prompted to enter a password for the account. Add the same password as the account password is to login to the linux system. Note: If the user account has its password changed at a later time, this password will have to be updated for the account to work To update a SAMABA user's pasword use this command:   smbpasswd  <p>==SAMBA with Xinetd== You can configure Xinetd to manage SAMBA aswell for additional security.</p> <p>The following entries in the <code>/etc/xinetd.conf</code> will add it:</p> <p> server netbios-ns {     socket_type = dgram     protocol = udp     wait = yes     user = root     only_from = 192.168.0.1 192.168.0.2     group = root     server = /urs/local/sama/bin/smbd } <p>server netbios-ssn {     socket_type = stream     protocol = tcp     wait = no     user = root     only_from = 192.168.0.1 192.168.0.2     group = root     server = /usr/local/samba/bin/smbd }</p> <p>To see details on configuring Xinetd see the appropriate page</p> <p>==Notes==</p> <p>==Sources==</p>"},{"location":"services/selinux/","title":"SELinux","text":"<p>SELinux is a firewall program on Fedora and a number of REHL linux distributions. It can be a bit of a pain with permissions and giving programs on your local machine external access. Here are some commands to help you out</p> <p>==Directories== SELinux Root Directory: <code> /etc/selinux </code></p> <p>==Temporary Enable/Disable SELinux== Good for Testing is SELinux is the problem</p> <p> sudo setenforce 0 #puts SELinux in permissive mode. It prompts issues but does not stop them sudo setenforce 1 #puts SELinux in enforce mode. Will prompt and will block</p> <p>You can always check what the status of SELinux is with this command:   sudo sestatus</p> <p>==SELinux Is Interfering with Apache and PHP== This is a common issue with the Laraval framework</p> <p>To disable SELinux from interfering in a directory use the following command:</p> <p> su -c \"chcon -R -h -t httpd_sys_script_rw_t \" <p>Note that it will do this recursively, so any files and folders below the passed in folder will be ignored aswell. Should note aswell that this should only be used on local development environments. Production configuration please do more googling</p> <p>==Notes==</p> <p>==Sources== http://stackoverflow.com/questions/17954625/services-json-failed-to-open-stream-permission-denied-in-laravel-4/27377624#27377624</p>"},{"location":"services/sftp/","title":"Sftp","text":"<p>==What is SFTP== SFTP is the SSH File Transfer Protocol which is an extension of the Secure Shell protocol (SSH) to provide secure file transfer capabilities. It is not to be mistaken for FTP or FTPS. Unlike FTP, SFTP encrypts both commands and data, preventing passwords and sensitive information from being transmitted openly over the network. It cannot interoperate with FTP software. FTPS is an extension to the FTP standard that allows clients to request FTP sessions to be encrypted. This is done by sending the \"AUTH TLS\" command.</p> <p>==Setting Up a Simple Server== The goal is to setup a SFTP server where the users are chrooted to their home directory, and have limited system powers. ===Setting up the Server=== Assuming you already have SSH installed, we need to edit the SSH server config file.</p>"},{"location":"services/sftp/#open-the-config-file-sudo-nano-etcshhsshd_config","title":"Open the config file <code>sudo nano /etc/shh/sshd_config</code>","text":""},{"location":"services/sftp/#comment-out-the-following-line-with-a-at-the-beginning-subsystem-sftp-usrlibopensshsftp-server","title":"Comment out the following line with a # at the beginning <code>#Subsystem sftp /usr/lib/openssh/sftp-server</code>","text":""},{"location":"services/sftp/#add-the-following-at-the-end-of-the-file-subsystem-sftp-internal-sftp-match-group-sftpusers-chrootdirectory-h-forcecommand-internal-sftp-x11forwarding-no-allowtcpforwarding-no-passwordauthentication-yes","title":"Add the following at the end of the file: <pre>Subsystem sftp internal-sftp\n\nMatch Group sftpusers\n    ChrootDirectory %h\n    ForceCommand internal-sftp\n    X11Forwarding no\n    AllowTCPForwarding no\n    PasswordAuthentication yes</pre>","text":""},{"location":"services/sftp/#restart-ssh-sudo-service-ssh-restart","title":"Restart SSH <code>sudo service ssh restart</code>","text":"<p>===Creating the sftpusers Group=== <code>sudo groupadd sftpusers</code></p> <p>===Create SFTP Users===</p>"},{"location":"services/sftp/#create-user-sudo-adduser-username","title":"Create user <code>sudo adduser username</code>","text":""},{"location":"services/sftp/#prevent-ssh-login-assign-user-to-sftp-group-sudo-usermod-g-sftpusers-usernamesudo-usermod-s-usrsbinnologin-username","title":"Prevent SSH login &amp; assign user to SFTP group <code>sudo usermod -G sftpusers username</code><code>sudo usermod -s /usr/sbin/nologin username</code>","text":""},{"location":"services/sftp/#chroot-user-limit-them-to-their-home-directory-sudo-chown-rootroot-homeusernamesudo-chmod-755-homeusername","title":"Chroot user (limit them to their home directory) <code>sudo chown root:root /home/username</code><code>sudo chmod 755 /home/username</code>","text":""},{"location":"services/sftp/#give-the-user-a-folder-to-upload-to-sudo-mkdir-homeusernamesharesudo-chown-usernamesftpusers-homeusernameshare","title":"Give the user a folder to upload to <code>sudo mkdir /home/username/share</code><code>sudo chown username:sftpusers /home/username/share</code>","text":"<p>==Notes==</p> <p>==Sources==</p>"},{"location":"services/shell-scripting/","title":"Shell scripting","text":"<p>Shells provide an interface for controlling programs and their input and output.</p> <p>There are 4 different shells</p> <ul> <li>Bourne (sh)</li> <li>C (csh)</li> <li>Korn (ksh)</li> <li>Bash (Bourne-Again)</li> </ul> <p>The Bourne shells are ideal for string and word manipulation where as C shell is best used for mathematical scripting. Korn shell was an attempt at merging these two shells at a point but was later dropped and Bash was created.</p> <p>All of the above shells are line-oriented, with features dating back to the earliest UNIX shells for Teletype terminals. All shells operate in a loop:</p>"},{"location":"services/shell-scripting/#display-a-prompt","title":"Display a prompt","text":""},{"location":"services/shell-scripting/#ready-input-command-line","title":"Ready input command line","text":""},{"location":"services/shell-scripting/#perform-substitutions-in-command-line","title":"Perform substitutions in command line","text":""},{"location":"services/shell-scripting/#execute-command","title":"Execute command","text":""},{"location":"services/shell-scripting/#loop-to-step-1","title":"Loop to step 1","text":"<p>The substitutions in step 3 may be of : variables, filenames, command outputs, aliases, depending on the shell. For the following documentation we will be dealing with primarily Bash shell, the currently most common shell on Linux distributions</p> <p>==Command Substitution==</p> <p>Many commands that take arguments ignore standard input (eg. ls, file, echo, cd, rm). This is a problem for passing around results between commands</p> <p>A conundrum: ls prints the list of file names to standard output, file though takes filenames as arguments and determines the file type. So how do we get the output of ls as an input for file ?</p> <p>This won't work:   ls | file</p> <p>So how do we do this ? When the shell encounters a string between backquotes such as <code> <code>cmd</code> </code> it executes cmd and replaces the backquotes string with the standard output of cmd, with any trailing newlines deleted. So for out ls and file problem we can solve it like this:   file <code>ls</code> In this scenario ls will be executed first, which will then be passed as parameters to the file command, getting file information about the current directory</p> <p>Standard output of any command can be made into arguments to any other command like this:   $ date"},{"location":"services/shell-scripting/#will-print-thu-may-10-113346-pdt-2012","title":"will print: Thu May 10 11:33:46 PDT 2012","text":"<p>$ echo The date is <code>date</code></p>"},{"location":"services/shell-scripting/#will-print-the-date-is-thu-may-10-113433-pdt-2012","title":"will print: The date is Thu May 10 11:34:33 PDT 2012","text":"<p>$ echo The date is <code>data| awk '{print $2, $3 \",\", $6}'</code></p>"},{"location":"services/shell-scripting/#will-print-the-data-is-may-10-2012","title":"will print: The data is May 10, 2012","text":"<p>==Defining Variables== Shell variables can hold single string values, created with = and accessed $. NOTE they are ONLY string variables. Also spacing matters here. There can be not spaces between the variable name the '=' sign and the value being assigned.</p> <p>Variable concatenation is also possible using a special syntax. You can concatenate one variable into the other by surrounding the one variable in {}</p> <p>Examples  $ ub=/usr/bin $ cd $ub;pwd This example assigned the <code>/usr/bin</code> folder to the variable ub and then cd's into the variable and then asks for the current directory. Note here that we are assigning variables directly in the console. You can create and use variable both in script files and in the console. You variables will only last in console until the shell terminal is closed.</p> <p>Note in the above aswell the commands are seperated with a semi-colon. This allows you to make multiple sequential commands on the same line / same command execution</p> <p> $ curse='@$&amp;&gt;##! is a legalized virus!' $ echo $curse @$&amp;&gt;##! is a legalized virus! The above example shows how to include whitespace or matecharacters into assignments. Note that the difference is surrounding the string with single quotes</p> <p> $ dir=<code>pwd</code> Using the techniques in the previous section here we can store the output of a command into a variable</p> <p> $ one=1 <p>$ echo $onetwo  #does not work - variable does not exist</p> <p>$ echo ${one}two #value of variable one, text two</p>"},{"location":"services/shell-scripting/#will-print-1two","title":"will print: 1two","text":"<p>==Metacharacter Suppression== With shell using so many characters with secondary special meanings (eg /, $,`, ) what do you do if you want literaly these items in your strings ? Using Metacharacter Suppression you can cancel out these second meanings and use these characters for thier literal use</p> <p>The following rules work for character suppression: * Single quotes suppress the special meaning of ALL metacharacters * Double quotes suppress the special meaning of all metacharacters EXCEPT \\ (backslash), $ (dollar sign) and ` (backquote) * Backslash suppresses the special meaning of the immediately following character (except between single quotes - because then it aswell is cancelled out)</p> <p>Basically you can think of it as the same rules to PHP. Single quotes mean whatever is between them will be taken literally. Double quotes will attempt to resolve any variables or special meaning characters it finds within them, and backslash will escape the character, unless that creates a meaning within itself</p> <p>Note that &gt;, &lt;, |, spaces and tabs are also metacharacters as they seperate arguments.</p> <p>Examples  $ echo hello | wc -c         # one argument to echo"},{"location":"services/shell-scripting/#will-print-6","title":"will print: 6","text":"<p>$ echo hello '|' wc -c       # four arguments to echo</p>"},{"location":"services/shell-scripting/#will-print-hello-wc-c","title":"will print: hello | wc- c","text":"<p>$ echo '.....hello | wc -c'  # one argument to echo</p>"},{"location":"services/shell-scripting/#will-print-hello-wc-c_1","title":"will print: .....hello | wc -c","text":"<p>Patterns for grep and sed often contain metacharactersl its a good habit to surround arguments with single quotes   $ grep '|' chap2 <p>$ sed 's?&gt;/&gt;&gt;/g' foo/bar/readme</p> <p>==Shell Functions== A shell function associates a name with a list fo commands. When the name is given as a command, the list of commands is executes. All functions must be declared before they can be used. The syntax is:   function_name(){     Commands } <p>OR</p> <p>function_name(){     Commands     return $TRUE }</p> <p>Example  $ mydate() { date | awk '{ print $2, $3 \",\", $6 }'; } $ mydate"},{"location":"services/shell-scripting/#will-print-may-10-2012","title":"will print: May 10, 2012","text":"<p>==Function Arguments== You can't pass arguments to shell functions. The actual () doesn't mean anything other then help shell identify a function. You can actualy put any parameters in it. You can pass parameters though through positional parameters using <code> $1, $2, .... , $9 </code> like syntax.</p> <p>Example  nd(){     cd $1     PS1=\"<code>pwd</code> $ \" }"},{"location":"services/shell-scripting/#calling-the-function-in-shell","title":"calling the function in shell","text":"<p>$ nd /bin</p>"},{"location":"services/shell-scripting/#will-print-bin","title":"will print: /bin $","text":"<p>==Positional Parameters and Special Variables== Arguments to a shell script are accessed by position parameters. There are some additional ways to access these parameters</p> <ul> <li>$1, $2, ...., $9   -   Arguments 1 through 9</li> <li>$0                 -   Will be the script file's name</li> <li>$*                 -   All Positional paramters (starting from 1)</li> </ul> <p>Some other special variables include * $#                 -   Number of positional parameters to the script * $$                 -   Shell prcess ID number  * &amp;?                 -   Exit status of last command</p> <p>==Shell Scripts== A sequence of commands stored in an ordinaty text file is called a shell script. The .(dot) build-in causes the shell to execute commands from a script</p> <p>Example  $ cat f_defs echo Defining nd and mydate nd() { cd $1; PS1=\"<code>pwd</code> $ \"; } mydate() { date | awk ' { print $2, $3 \",\", $6 }'; } <p>$ . f-defs Defining nd and mydate $ nd /bin /bin $</p> <p>Alternatively, shell scripts can be executed by setting the execute bit on the file</p> <p>==The awk Programming Language== awk is named after is authors: Al Aho, Peter Weinberger and Brian Kernighan.</p> <p>It is a full programming language supporting: * build-in and user-defined variables * arithmetic and string operations * loops and branches * build-and user-defined functions</p> <p>awk's simplest and most common use is to select and change the order of fields. awk teats each input line as a sequence of fields delimited with whitespace. Fields can be selected by number and printed:   $ awk '{ print $3 }' UNIX is Linux Linux Linux is UNIX UNIX ^d</p> <p>Using single quotes makes the print action test a single argument to awk, with no metacharacter interpretation</p> <p>awk can be used to reformat output of other commands:   $ date"},{"location":"services/shell-scripting/#will-print-thu-may-10-132857-pdt-2012","title":"will print: Thu May 10 13:28:57 PDT 2012","text":"<p>$ date | awk '{ print $2 $3 $6 }'</p>"},{"location":"services/shell-scripting/#will-print-may102012","title":"will print: May102012","text":"<p>$ date | awk '{ print $2, $3, $6 }'</p>"},{"location":"services/shell-scripting/#will-print-may-10-2012_1","title":"will print: May 10 2012","text":"<p>$ date | awk '{ print $2, $3, \",\", $6 }'</p>"},{"location":"services/shell-scripting/#will-print-may-10-2012_2","title":"will print: May 10 , 2012","text":"<p> $ echo ipsos custodies? | awk '{ print \"Quis custodiet ipsos \" $2 }' Quis custodiet ipsos custodies?</p> <p>awk's field seperator can be changed with the -F options. For example <code> -F: </code> will make colon the identified field seperator. Then we could use this  to print out all of the login names of all users in the passwd file on linuz as follows:</p> <p> $ awk -F: '{ print $1 }' /etc/passwd"},{"location":"services/shell-scripting/#prints-list-of-all-users","title":"prints list of all users","text":"<p>$ grep /home /etc/passwd | awk -F: '{ print $1 }'</p>"},{"location":"services/shell-scripting/#prints-all-users-who-have-home-as-part-of-thier-login-directory-essentialy-finding-all-actual-users","title":"prints all users who have /home as part of thier login directory (essentialy finding all actual users)","text":"<p>==shift and set Build-In Commands== The shift command shifts position parameters</p> <p>"},{"location":"services/shell-scripting/#build-shift_test-script","title":"build shift_test script","text":"<p>$ cat shift_test echo $0 $1 $5 $9 \" Args:\" $, No: $# shift echo $0 $1 $5 $9 \" Args:\" $, No: $#</p>"},{"location":"services/shell-scripting/#use-shift_test-script","title":"use shift_test script","text":"<p>$ shift_test A B C D E F G H I J shift_test A E I Args: A B C D E F G H I J, No: 10 shift_test B F J Args: B C D E F G H I J, No: 9</p> <p>Note that by shifting it is truncating off the first letter, like JavaScript arra's shift function. Note that the indexs though have not changed, only the value they point to has</p> <p>The set command sets the positional parameters to its arguments. Essentially it overwrites the positional parameters with the returned values from whatever command is passed to set:</p> <p>"},{"location":"services/shell-scripting/#build-set_test-script","title":"build set_test script","text":"<p>$ cat set_test date set <code>date</code> echo The date is $2 $3, $6</p>"},{"location":"services/shell-scripting/#execute-set_test-script","title":"execute set_test script","text":"<p>$ set_test Thu May 10 13:42:46 PDT 2012 The date is May 10, 2012</p> <p>==The for Loop==</p> <p>General Form:   for var [in val_list] do     command1     command2     commandN done</p> <p>For example to display a string 10 times:"},{"location":"services/shell-scripting/#binbash","title":"!/bin/bash","text":"<p>for i in 1 2 3 4 5 do echo \"Hello $i World\" done</p> <p>Shell also allows you to set a step value, allowing you to count by 2 or 3 or even backwards. The following example has default setp value of 1"},{"location":"services/shell-scripting/#binbash_1","title":"!/bin/bash","text":"<p>for i in {1..5} do echo \"Hello $i World\" done</p> <p>We can also specifty step arguments using the <code> {START...END...INCREMENT} </code> syntax:"},{"location":"services/shell-scripting/#only-works-for-version-40","title":"Only works for Version 4.0+","text":""},{"location":"services/shell-scripting/#binbash_2","title":"!/bin/bash","text":"<p>echo \"Bash version ${BASH_VERSION}...\" for i in {0..10..2} do echo \"Hello $i World\" done See in this example it will start at 0, end at 10 and increment by 2 as specified in the <code>{0..10..2}</code> of the for loop of the above example</p> <p>If no value list is given, $* (the positional parameter list) is assumed:   for varname do echo $varname; echo \u201c${varname}: printed\u201d done</p> <p>Output:"},{"location":"services/shell-scripting/#sh-varprintsh-foo-bar-foobar","title":"sh varprint.sh foo bar foobar","text":"<p>foo foo: printed bar bar: printed foobar foobar: printed</p> <p>==The case Branch== Simply, this is the shell equivelent of Case or a Switch statement</p> <p>General form:   case word in pattern) commands ;; esac Note: the double semi-colons are NOT a typo. Nor is the single ')' closing bracket</p> <p>In the above example 'word' is usual a variable substitution. It uses filename expansion-like metacharacters with | for \"OR\"</p> <p>A case branch can be the body of a for loop, for example:   for i do     case ${i} in         win[0-7])    echo \"${i}: Windows\" ;;         linux|unix)  echo \"${i}: Operating System\" ;;         *)           echo \"${i}: Unknown\" ;;     esac done</p> <p>Output:"},{"location":"services/shell-scripting/#sh-word_namessh-win0-unix-mac","title":"sh word_names.sh win0 unix mac","text":"<p>win0: Windows unix: Operating System mac: Unknown</p> <p>==The if/else Branch== General Form   if commands then      commands <pre><code>#else if condition\n[elif commands \n then \n    commands]\n\n# else condition\n[else \n    commands]\n</code></pre> <p>fi Note all of the else if and else commands are inside the if command which is closed with <code>fi</code></p> <p>The commands following if are executed. An exit status of 0 from the last command means true, non-zero means false. The if construct is typically used in conjunction with the test command which is covered in detail in the next section</p> <p>==The test Command== The test command evaluates an expression and yield an exit code of 0 for true, and non-0 for false</p> <p>General form:   test expression <p>OR </p> <p>[expression]</p> <p>There is also a new form used on Bash and Korn shells referred to as \"new test\" and use double brackeds ( [[expression]] ) instead of single given in the above example</p> <p>test implements the old, portable syntax of the command. In almost all modtern shells '[' is a synonym for test (but requires a final argument of ']')</p> <p>'[[' is a new improved version of it, and is a keyword, not a program. This makes it easier to use, as shown in the examples below:</p> <p>Examples  if [ -z \"$variable\" ] then     echo \"variable is null!\" fi <p>if [ ! -f \"$filename\" ] then     echo \"not a valid, existing filename: $filename\" fi</p> <p>if [[ ! -e $file ]] then     echo \"directory entry does not exist: $file\" fi</p> <p>if [[ $file0 -nt $file1 ]] then     echo \"file $file0 is newer than $file1\" fi</p> <p>In the case of [[ glob expansion (pattern matching) will be done and therefor arguments need not be quotes. Because [[ is built in on to the shell and does not have legacy requirements, you don't need to worry about word splitting on variables that evaluate to a string with spaces. Therefor, there is not need to put the variables in double quotes.</p> <p>Test has numerous primitives for determining features of files and strings</p> <p>File Comparisons {| class=\"wikitable\" ! Syntax ! Use |- | -f  || True if the file exits, is regular file |- | -d  || True if the file exists and is a directory |- | -x  || true if the file exists and is executable |- | -s  || True if the file exists and has a file size large then 0 |- |} <p>String Comparisons {| class=\"wikitable\" ! Syntax ! Use |- | s1 = s2 || True if String 1 (s1) is identicle to String 2 (s2) |- | s1 != s2 || True if String 1 (s1) is not identicle to String 2 (s2) |- |}</p> <p>Integer Comparisons {| class=\"wikitable\" ! Syntax ! Use |- | n1 -eq n2 || True if Integer 1 (n1) is equal to Integer 2 (n2) |- | n1 -ne n2 || True if Integer 1 (n1) is not equal to Integer 2 (n2) |- | n1 -gt n2 || True if Integer 1 (n1) is greater then Integer 2 (n2) |-  | n1 -ge n2 || True if Integer 1 (n1) is greater then or equal to Integer 2 (n2) |- | n1 -lt n2 || True if Integer 1 (n1) is less then Integer 2 (n2) |- | n1 -le n2 || True if Integer 1 (n1) is less then or equal to Integer 2 (n2) |}</p> <p>Logical Operators {| class=\"wikitable\" ! Syntax ! Use |- | thing1 -a thing2 || True if thing1 AND thing2 both return True |- | thing1 -o thing2 || True if thing1 OR thing2 return True |- | thing1 !  thing2 || Inverses the meaning of the . Is logical equivelent for NOT |} <p>There are also short-circuited Logical Operators. These can be used with test but order matters. These operators can also be used outside of if statements as short-hand of executing commands based on the short-circuited logic aswell</p> <p>Short-Circuited Logical Operators {| class=\"wikitable\" ! Syntax ! USe |- | command1 &amp;&amp; command2 || True if both commands return True (exit status 0). If command1 does not return exit status 0, command2 will not be run |- | command1 || command2 || True if both commands return True (exit status 0). If command1 returns with exit status 0 (true), command2 will not be run |}</p> <p>A good reference for all of the operator options is here : http://tldp.org/LDP/abs/html/comparison-ops.html</p> <p>Combination Examples"},{"location":"services/shell-scripting/#example-with-and","title":"example with AND","text":"<p>test -f temp -a -x temp</p>"},{"location":"services/shell-scripting/#example-with-or","title":"example with OR","text":"<p>[ -if temp -o -d temp]</p>"},{"location":"services/shell-scripting/#example-with-not","title":"example with NOT","text":"<p>test ! -d temp</p>"},{"location":"services/shell-scripting/#example-with-groupings-note-whitespacing","title":"example with groupings. NOTE WHITESPACING","text":"<p>[ ${i} = 'ready' -a ( -f temp -o -d temp -o -d tempt ) ]</p> <p> Examples</p> <p>Simple example using the if and test commands   if [ -f ${i} ] then     echo ${i} is a regular file     else         echo ${i} is not a regular file fi</p> <p>The next example shows a complete shell program that illustrates the use of \"if\", \"test\" and shell functions:"},{"location":"services/shell-scripting/#binbash_3","title":"!/bin/bash","text":"<p>usage(){     echo \"Usage: $0 username\"     exit 1 }</p> <p>userlogged(){     if (who | grep $1 &gt; /dev/null)     then          echo $1 is logged on         else             echo $1 is not logged on     fi }</p>"},{"location":"services/shell-scripting/#call-usage-function-if-filename-not-supplied","title":"call usage() function if filename not supplied","text":"<p>if test $# != 1 then      usage     else          userlogged $1 fi</p> <p>Using short-circuited logical operators we can simulate if and else statements. See the if else statement below:   if [ -f unixfile ] then     rm unixfile     else         echo \"unixfile was not found, or is not a regular file\" fi</p> <p>We can simulate this with short-circuited logical operators as:   [ -f unixfile ] &amp;&amp; rm unixfile || echo \"unixfile was not found, or is not a regular file\"</p> <p>In addition, multiple commands can be executed based on the results of 1 command by incorporating braces and semi-colons:   command1 &amp;&amp; { command2 ; command3 ; command4 ; } If the exit status of command1 is true (zero), commands 2,3 and 4 will be performed</p> <p>==while and until Loops==</p> <p>General sytnax for the while loop is:   while commands do     commands done</p> <p>Example  while true do     sleep 60     who done</p> <p>The General syntax for the until loop is:   until commands do     commands done</p> <p>Example  until who | grep Bill do      sleep 30 done echo Bill is logged in!</p> <p>==Filtering with Loops and Branches==</p> <p>Loops and branch output may be filtered.</p> <p>Example  $ cat sys_hogs for user in <code>awk -F: '{ print $1 }' /etc/passwd</code> do     if [ ${user} != 'root' -a ${user} != 'aman' ]     then         home=<code>awk -F: \"/^${user}:/ { print \\\\$6 }\" /etc/passwd</code>         usage=<code>du -s ${home} | awk '{ print $1 }'</code>         echo ${usage} ${user}     fi done | sort -nr | head | awk '{ print $2 \":\", $1 }'</p> <p>Complex scripts often require debugging</p> <p> $ sh -x sys_hogs  # trace execution</p> <p>==Redirecting Standard Error==</p> <p>The standard input, output and error streams are associated with file descriptors : * standard input - 0  * standard output - 1  * standard error - 2</p> <ul> <li>&lt; redirects 0 (standard input)</li> <li> <p>and &gt;&gt; redirect 1 (standard output)</p> </li> <li>Preceding &gt; or &gt;&gt; with 2 redirects standard error   myprog &lt; foo &gt; foo.out 2&gt; foo. err</li> <li>Standard error may be combined with standard output   myprog &lt; foo &gt; foo.allout 2&gt;&amp;1</li> </ul> <p>==The read Command==</p> <p>The read command is a simple command that allows for user input. The read command waits for input and then assigns it to variables passed as parameters to the read command</p> <p> $ read line This is a test <p>$ echo $line This is a test</p> <p>$ read word rest This is another test</p> <p>$ echo $word This</p> <p>$ echo $rest is another test</p> <p>Note that read typical will read in the phrase and then the return key as a second input. If you have multiple reads after one another you may end up with the return key as the input to the second read. The best way to avoid this is to assign two variables to the read command. 1 will take the input, the 2nd will take the return key, which at that time you can discard</p> <p>You should also note that when read is given multiple variables as parameters to store the input text, read will put each individual word in its own variable using spaces to delimit the start and end of a word, until the last variable where it will place the rest of the read input.</p> <p>==Arithmetic with expr==</p> <p>The shell has no built-in arithmetic operations. The expr command provides arithmetic and other facilities for use in shell scripts or other contexts. expr takes an arithmetic expression as arguments and prints the value to standard output; metacharacters must be suppressed:   $ expr 7 * ( 2 + 3 ) 35 $ a=1; a=<code>expr $a + 1</code>; echo $a 2</p>"},{"location":"services/snort/","title":"Snort","text":"<p>Snort is an IDS that does real-time analysis of incoming traffic. It can be configured to work both for networks and also single systems. An idealistic configuration follows the following tutorial: https://s3.amazonaws.com/snort-org-site/production/document_files/files/000/000/090/original/Snort_2.9.8.x_on_Ubuntu_12-14-15.pdf</p> <p>This includes how-to on setup of also barnyard and other components.</p> <p>In addition to this, the community rules do not cover all aspects of intrusion detection. Thus using emerging threats rulesets is also recommended. These can be found here:</p> <p>https://rules.emergingthreats.net/open/snort-2.9.0/rules/</p> <p>==Notes==</p> <p>==Sources== https://s3.amazonaws.com/snort-org-site/production/document_files/files/000/000/090/original/Snort_2.9.8.x_on_Ubuntu_12-14-15.pdf https://rules.emergingthreats.net/open/snort-2.9.0/rules/</p>"},{"location":"services/tcp-wrappers/","title":"TCP Wrappers","text":"<p>TCP Wrappers are one of the most common ways to control access on a Unix or Linux System. The wrapper wraps around existing daemons and interfaces between clients and the server. This provides a layer of security between the external world and your service provided by your Linux/Unix system.</p> <p>Xinetd is the fundamental program that started this movement. Any service managed by xinetd (aswell any preogram with build-in support for libwrap) can use TCP wrappers to manage access</p> <p>==Directories== Hosts.Allow: <code>/etc/hosts.allow</code>  Hosts.Deny: <code>/etc/hosts.deny</code> </p> <p>Xinetd Super Server Configuration: <code>/etc/xinetd.conf</code></p> <p>==Xinetd== xinetd can use the hosts.allow and hosts.deny files to configure access to system services. hosts.allow lists hosts allowed to access services controlled by xinetd and hosts.deny lists hosts denied from access to services controlled by xinetd.</p> <p>hosts.allow has precedence over hosts.deny. There-for a host listed in both files or neither files will be given access. Hosts can be based on individual IP addesses or hostnames, or on a pattern of clients. The wrapper <code>tcpd</code> checks these hosts.allow and hosts.deny files to determine whether to allow access. As a system administrator you can configure the rules in these files to allow or deny each xinetd-managed service to specific hosts, domains or subnets</p> <p>When a connection is initiated on one of the ports xinetd is bound to, xinetd looks up the service name by its port number in <code>/etc/services</code>, then the configuration of that service in <code>/etc/xinetd.conf</code>. If <code>tcpd</code> is listed as the server executable to run, its argument (the daemon's executable name) is looked up first in hosts.allow. If access is permitted by rules in that files, access to the service is allowed. Then, if access is denied by the rules in hosts.deny the connection is dropped. If no rule matches in either file, access to the service is allowed.</p> <p>The parsing daemon stops at the first match by inspecting the files in this order:</p>"},{"location":"services/tcp-wrappers/#hostsallow","title":"hosts.allow","text":""},{"location":"services/tcp-wrappers/#hostsdeny","title":"hosts.deny","text":""},{"location":"services/tcp-wrappers/#default-allow","title":"default: Allow","text":"<p>Because the default configuration is to allow all server requests, you must edit the hosts.access and hosts.deny files to restrict access. A typical and very simple configuration is to deny ALL service to ALL, but allow ALL service to hosts on your local subnet</p> <p>===Permissive Strategy=== Leave hosts.allow empty and restrict access in the hosts.deny</p> <p>===Paranoid Strategy=== In hosts.deny deny ALL (ie. ALL:ALL) and then configure hosts.allow to permit access</p> <p>The following are important points to consider when using TCP wrappers to protect network services: * Because access rules in hosts.allow are applied first, they take precedence over rules specified in hosts.deny. * Therefore, if access to a service is allowed in hosts.allow, a rule denying access to that same service in hosts.deny is ignored. * Since the rules in each file are read from the top down and the first matching rule for a given service is the only one applied, the order of the rules is extremely important. * If no rules for the service are found in either file, or if neither file exists, access to the service is granted. * TCP wrapped services do not cache the rules from the hosts access files, so any changes to hosts.allow or hosts.deny take effect immediately without restarting network services.</p> <p>===Formatting Access Rules=== The format for both the hosts.allow and hosts.deny files are identical. Any blank lines or lines that start with a hash (#) are ignored and each rule must be on a new line. A rule basicily looks like a colon separated  rule with the daemon name and the user's who have permission</p> <p>The basic syntax looks like this:   :  [: : : ...] <p>{| class=\"wikitable\" ! Attribute ! Value / Definition |- |  || A comma separated list of process names (not service names) or the ALL wildcard |- |  || A comma seperated list of hostnames, host IP address, special patterns or special wildcars that identify the hosts effected by the rule |- |  || An optional action or colon separated list of actions performed when the rule is triggered. Option fields support expansions), launch shell commands, allow or deny access, and alter logging behavior. |} <p>===Examples===   vsftpd : .example.com This rule instructs TCP wrappers to watch for connections to the FTP daemon (vsftpd) from any host in the example.com domain. If this rule appears in hosts.allow, the connection will be accepted. If this rule ppears in hosts.deny, the connection will be rejected.   sshd : .example.com : spawn /bin/echo <code>/bin/date</code> access denied&gt;&gt;/var/log/sshd.log : deny This example uses two options fields</p> <p>The above rule states that if a connection to the SSH daemon (sshd) is attempted from a host in the example.com domain, execute the echo command (which will log the attempt to a special file), and deny the connection. Because the optional deny directive is used, this line will deny access even if it appears in the hosts.allow file.</p> <p>===Wildcards===</p> <p>Wildcards allow TCP wrappers to more easily match groups of daemons or hosts. They are used most frequently in the client list field of access rules.</p> <p>The following wildcards may be used: {| class=\"wikitable\" ! Wildcard ! Use |- | ALL || Matches everything. It can be used for both the daemon list and the client list. |- | LOCAL || Matches any host that does not contain a period (.), such as localhost. |- | KNOWN || Matches any host where the hostname and host address are known or where the user is known. |- | UNKNOWN || Matches any host where the hostname or host address are unknown or where the user is unknown. |- | PARANOID || Matches any host where the hostname does not match the host address. |}</p> <p>===Patterns===</p> <p>Patterns can be used in the client list field of access rules to more precisely specify groups of client hosts. These are mainly alternate varying shorthands to specify groups of client hosts.</p> <p>====Period==== Placing a period (.) at the beginning of a hostname, matches all hosts sharing the listed components of the name.</p> <p> ALL : .example.com This example will match any host within the example.com domain</p> <p> ALL : 192.168. You can also place the period at the end. In the above example all IP's that start with <code>192.168</code> will be matched with this rule. Note that alternatively to this, netmask expressions are also acceptable and work the exact same. The following example is equivelent to the above example:   ALL : 192.168.0.0/255.255.254.0</p> <p>Note though that prefix length style is not supported. The following example WILL NOT work:   ALL : 192.168.0.0/16</p> <p>====Asterix==== The asterix (*) can be used to match entire groups of hostnames or IP address, as long as they are not mixed in a client list containing other types of patterns</p> <p>The following example will match any host within the example.com domain   ALL : *.example.com</p> <p>====Slash==== The slash (/) is specificaly used to identify a file name. This can be useful if rules specifying large numbers of hosts are necessary. The following example refers TCP wrappers to the <code>/etc/telnet.hosts</code> file for all Telnet connections:</p> <p> in.telnetd : /etc/telnet.hosts</p> <p>This way you can essentially segregate out permissions to other files</p> <p>===Operators=== Currently there is only one operator <code>EXCEPT</code> which can be used in both the daemon list and the client list of a rules.</p> <p>This <code>EXCEPT</code> rule allows specific exceptions to broader matches within the same rule.</p> <p>In the following example from a hosts.allow file, all example.com hosts are allowed to connect to all services except cracker.example.com:   ALL: .example.com EXCEPT cracker.example.com In the another example from a hosts.allow file, clients from the 192.168.0.x network can use all services except for FTP:   ALL EXCEPT vsftpd: 192.168.0.</p> <p>===Logging===</p> <p>Xinetd has option fields, and these can also be used to change the logging priority of a given rule. This can be done using the <code>severity</code> directive.</p> <p>The following example, connections to the SSH daemon from any host in the example.com domain are logged to the default authpriv facility (because no facility value is specific) with a priority of emerg  sshd : .example.com : severity emerg</p> <p>As noted you can also specify a facility group by specificying it in the <code>severity</code> directive. The following example logs any SSH connection attempts by hosts from example.com domain to the local0 favility with a priority of alert:   sshd : .example.com : severity local0.alert</p> <p>Note though in practice, this example will not work until the syslog daomon (syslogd) is configured ot loc to the local0 facility. Some additional configuration is needed for this to work as expected</p> <p>===Access Control===</p> <p>Option fields also allow administrators to explicitly allow or deny hosts in a single rule by adding the allow or deny directive as the final option.</p> <p>For instance, the following two rules allow SSH connections from client-1.example.com, but deny connections from client-2.example.com:   sshd : client-1.example.com : allow sshd : client-2.example.com : deny By allowing access control on a per-rule basis, the option field allows administrators to consolidate all access rules into a single file: either hosts.allow or hosts.deny. This is a useful consideration to make when using a Permissive or Paranoid security strategy with the hosts.allow and hosts.deny files</p> <p>===Shell Commands=== Option fields allow access rules to launch shell commands through the following two directives: * spawn \u2014 Launches a shell command as a child process. This option directive can perform tasks like using /usr/sbin/safe_finger to get more information about the requesting client or create special log files using the echo command.</p> <p>In the following example, clients attempting to access Telnet services from the example.com domain are quietly logged to a special file. Note the use of the <code>%h</code> expansion which is explained in the next section:   in.telnetd : .example.com : spawn /bin/echo <code>/bin/date</code> from %h&gt;&gt;/var/log/telnet.log : allow</p> <ul> <li>twist \u2014 Replaces the requested service with the specified command. This directive is often used to set up traps for intruders (also called \"honey pots\"). It can also be used to send messages to connecting clients. The twist command must occur at the end of the rule line.</li> </ul> <p>In the following example, clients attempting to access FTP services from the example.com domain are sent a message via the echo command:   vsftpd : .example.com : twist /bin/echo \"Rabbit Hole Ahead, Turn Back!\" </p> <p>===Expansions===</p> <p>Expansions, when used in conjunction with the spawn and twist directives provide information about the client, server, and processes involved.</p> <p>The following is a list of supported expansions: {| class=\"wikitable\" ! Expansion ! Use |- | %a || The client's IP address. |- | %A || The server's IP address. |- | %c || Supplies a variety of client information, such as the username and hostname, or the username and IP address. |- | %d || The daemon process name. |- | %h || The client's hostname (or IP address, if the hostname is unavailable). |- | %H || The server's hostname (or IP address, if the hostname is unavailable). |- | %n || The client's hostname. If unavailable, unknown is printed. If the client's hostname and host address do not match, paranoid is printed. |- | %N || The server's hostname. If unavailable, unknown is printed. If the server's hostname and host address do not match, paranoid is printed. |- | %p || The daemon process ID. |- | %s || Various types of server information, such as the daemon process and the host or IP address of the server. |- | %u || The client's username. If unavailable, unknown is printed. |}</p> <p>The following sample rule uses an expansion in conjunction with the spawn command to identify the client host in a customized log file. It instructs TCP wrappers that if a connection to the SSH daemon (sshd) is attempted from a host in the example.com domain, execute the echo command to log the attempt, including the client hostname (using the %h expansion), to a special file:   sshd : .example.com : spawn /bin/echo <code>/bin/date</code> access denied to %h&gt;&gt;/var/log/sshd.log : deny</p> <p>Similarly, expansions can be used to personalize messages back to the client. In the following example, clients attempting to access FTP services from the example.com domain are informed that they have been banned from the server:   vsftpd : .example.com : twist /bin/echo \"%h has been banned from this server!\"</p> <p>'''***Note that in practice one should not antagonise a hacker. The user should not be informed about anything of thier status on the server '''</p> <p>==Xinetd Super Server==</p> <p>The exenteded internet services daemon (xinetd) has a number of extended capabilities the extend further then the TCP Wrappers and portmapper tools explained in the previous section. The Xinetd Super Server is also substantially easier to manage and more user friendly</p> <p>===Advantages of the Xinetd Super Server=== * xinetd provides access control in a way that is quite similar to TCP_wrappers or the portmapper. * It provides access control for TCP, UDP, and RPC services. * Implements access limitations based on time. * Extensive logging capabilities for both successful and unsuccessful connections. * Provides transparency to both the client host and the wrapped network service. Both the connecting client and the wrapped network service are unaware that TCP wrappers are in use. * Legitimate users are logged and connected to the requested service while connections from banned clients fail. * Centralized management of multiple protocols. \u2014 TCP wrappers operate separately from the network services they protect, allowing many server applications to share a common set of configuration files for simpler management. * Provides for hard reconfiguration by killing services that are no longer allowed. * Provides numerous mechanisms to prevent DoS attacks. ** Provides a compile-time option to include libwrap, the TCP_wrappers library. ** Limit on the number of daemons of a given type that can run concurrently. An overall limit of processes forked by xinetd. ** Limits on log file sizes. * Provides a compile-time option to include libwrap, the TCP_wrappers library. ** Causes /etc/hosts.allow and /etc/hosts.deny access control checks in addition to xinetd access control checks. Provides for the invocation of tcpd. ** All TCP-wrappers functionality is available. * Services may be bound to specific interfaces. * Services may be forwarded (proxied) to another system.</p> <p>===Disadvantages of Xinetd Super Server=== * The configuration file, /etc/xinetd.conf, is incompatible with the older /etc/inetd.conf. ** You can however use a conversion utility, xtoa, that is included with the distribution. * Time-outs and other problems occur for RPC services, especially on busy systems. ** However, xinetd and portmap may coexist, allowing RPC through the portmapper. * The configuration file for xinetd is /etc/xinetd.conf, but the default file only contains a few defaults and an instruction to include the /etc/xinetd.d directory. * To enable or disable a xinetd service, edit its configuration file in the /etc/xinetd.d directory. * If the disable attribute is set to yes, the service is disabled. If the disable attribute is set to no, the service is enabled. * If you edit any of the xinetd configuration files or change its enabled status using Serviceconf, ntsysv, or chkconfig, you must restart xinetd with the command service xinetd restart before the changes will take effect.</p> <p>===Configuration=== The xinetd configuration file is, by default, /etc/xinetd.conf. Its syntax is quite different than, and incompatible with, /etc/inetd.conf. Essentially, it combines the functionality of /etc/inetd.conf, /etc/hosts.allow and /etc/hosts.deny into one file. Each entry in /etc/xinetd.conf is of the form:</p> <p> service  {       } <p>Note that spacing and indentation has been known to matter on some systems</p> <p>{| class=\"wikitable\" ! Attribute ! Use |- |  || Arbitrary name for the service, though typically the name of the standard network service being configured. Additional and completely nonstandard services may be added, as long as they are invoked through a network request, including network requests from the localhost itself. |- |  || There are a number of attributes which available which are listed in the next section |- |  || The value assigned to the specified attribute |- |  || Valid operators include : =, += and -= . Most attributes will assign values using the = operator, but some attributes allow the adding and removing of the values assigned to them thus using the += and -+ operators |} <p>Note: If you are transfering a service from one that is usualy managed by itself to one being managed by Xinetd Super Server, you do not need to turn on the service. Xinetd will manage the starting and stopping of the service all within itself. When setting up the first time, setup the service in Xinetd in the following order:</p>"},{"location":"services/tcp-wrappers/#shutdown-the-service-on-the-host-system","title":"Shutdown the service on the host system","text":""},{"location":"services/tcp-wrappers/#configure-the-service-in-the-xinetd-config-file","title":"Configure the service in the Xinetd config file","text":""},{"location":"services/tcp-wrappers/#restart-xinetd-service-do-not-touch-the-old-service","title":"Restart Xinetd service. DO NOT TOUCH THE OLD SERVICE","text":""},{"location":"services/tcp-wrappers/#check-the-status-of-the-xinetd-service-fedora-systemctl-status-xinetd-for-any-errors-in-the-logs-if-there-are-erros-about-port-binding-failures-it-is-likely-the-service-is-still-running-on-its-own-or-has-not-released-the-port-yet-upon-it-being-shutdown","title":"Check the status of the Xinetd service (Fedora: systemctl status xinetd) for any errors in the logs. If there are erros about port binding failures, it is likely the service is still running on its own or has not released the port yet upon it being shutdown","text":"<p>===Attributes &amp; Values===</p> <p>====socket_type==== The type of TCP/IP socket used. Acceptable values are stream (TCP), dgram (UDP), raw, and seqpacket (reliable, sequential datagrams).</p> <p>====protocol==== Specifies the protocol used by the service. Must be an entry in /etc/protocols. If not specified, the default protocol for the service is used.</p> <p>====server==== Daemon to invoke. Must be absolutely qualified.</p> <p>====server-args==== Specifies the flags to be passed to the daemon.</p> <p>====port==== Port number associated with the service. If listed in /etc/services, it must match.</p> <p>====wait==== There are two possible values for this attribute. If yes, then xinetd will start the requested daemon and cease to handle requests for this service until the daemon terminates. This is a single-threaded service. If no, then xinetd will start a daemon for each request, regardless of the state of previously started daemons. This is a multithreaded service.</p> <p>====user==== Sets the UID for the daemon. This attribute is ineffective if the effective UID of xinetd is not 0.</p> <p>====nice==== Specifies the nice value for the daemon.</p> <p>====id==== Used to uniquely identify a service when redundancy exists. For example, echo provides both dgram and streams services. Setting id=echo_dgram and id=echo_streams would uniquely identify the dgram and streams services, respectively. If not specified, id assumes the value specified by the service keyword.</p> <p>====access_times==== Sets the time intervals for when the service is available. Format is hh:mmhh:mm; for example, 08:00-18:00 means the service is available from 8 A.M. through 6 P.m.</p> <p>====only_from==== Space-separated list of allowed clients. The syntax for clients is as follows:</p> <p>{| class=\"wikitable\" ! Value ! Description |- | hostname || A resolvable hostname. All IP addresses associated with the hostname will be used. |- | IPadress || The standard IP address in dot decimal form. |}</p> <p>====net_name==== A network name from /etc/networks.</p> <p>=====x.x.x.0, x.x.0.0, x.0.0.0, 0.0.0.0===== The 0 is treated as a wildcard. For example, an entry like 88.3.92.0 would match all addresses beginning with 88.3.92.0 through and including 88.3.92.255. The 0.0.0.0 entry matches all addresses. </p> <p>=====x.x.x. {a, b, ....}, x.x.{a, b, ...}, x. {a, b, ...}===== Specifies lists of hosts. For example, 172.19.32.{1, 56, 59} means the list of IP addresses, 172.19.32.1, 172.19.32.56, and 172.19.32.59.</p> <p>=====IPaddress/netmask===== Defines the network or subnet to match. For example, 172.19.16.0/20 matches all addresses in the range 172.19.16.0 through and including 172.19.31.255. If this attribute is specified without a value, it acts to deny access to the service..</p> <p>====no_access==== Space separated list of denied clients. The syntax for clients is given previously. Note that placement of no_access matters in the stanza as placing it above a setting allowing users in will block it. The stanzas on xinetd are read in procedurally from top to bottom</p> <p>====redirect==== This attribute assumes the syntax, redirect= IPaddress port. It has the effect of redirecting a TCP service to another system. The server attribute is ignored if this attribute is used.</p> <p>====bind==== Space separated list of denied clients. The syntax for clients is given previously. Binds a service to a specific interface. Syntax is bind = IPaddress. This allows hosts with multiple interfaces (physical or logical), for example, to permit specific services (or ports) on one interface but not the other.</p> <p>====log_on_success==== Specifies the information to be logged on success. Possible values are: {| class=\"wikitable\" ! Value ! Use |- | PID || PID of the daemon. If a new daemon is not forked, PID is set to 0. |- | HOST || Client host IP address. |- | USERID || Captures UID of the client user through an RFC1413 call. Available only for multithreaded, streams services. |- | EXIT || Logs daemon termination and status. |- | DURATION || Logs duration of session. By default, nothing is logged. This attribute supports all operators. |}</p> <p>====log_on_failure==== Specifies the information to be logged on failure. A message indicating the nature of the error is always logged. Possible values are: {| class=\"wikitable\" ! Value ! Use |- | ATTEMPT || Records a failed attempt. All other values imply this one. |- | HOST || Client host IP address. |- | USERID || Captures UID of the client user through an RFC1413 call. Only available for multithreaded, streams services. |- | RECORD || Records additional client information such as local user,remote user, and terminal type. |} By default, nothing is logged. This attribute supports all operators.</p> <p>====disabled==== It has the same effect as commenting out the service entry in the /etc/xinetd.conf file. Syntax is disabled = yes. If left out, the service is enabled.</p> <p>==Examples==</p> <p>===Basic Examples=== Consider a simple example. The attributes (everything inside the braces and to the left of the = symbol) are very straightforward in their meaning as are the associated values (everything inside the braces and to the right of the = symbol):   service ftp {     socket_type = stream     protocol = tcp     wait = no     user = root     server =/usr/sbin/in.ftpd     server-args = -l -a }</p> <p> service telnet {     socket_type = stream     protocol = tcp     wait = no     user = root     server = /usr/sbin/in.telnetd }</p> <p>===Access Control Examples===</p> <p> defaults {     log_type = SYSLOG local4.info     log_on_success = PID HOST EXIT DURATION     log_on_failure = HOST     instances = 8 } <p>service login {     socket_type = stream     protocol = tcp     wait = no     user = root     flags = REUSE     only_from = 142.0.0.0     no_access = 142.100.0.0     log_on success += USERID     log_on failure += USERID     server /usr/sbin/in.ftpd     server_args -l -a }</p> <p>The configuration specifies a login service entry for a server (milliways for example) that allows access from any system whose IP address begins with 142 except for those whose address begins with 142.100.</p> <p>Note here also <code>no_access</code> is blow <code>only_from</code>. Because this configuration file is read from top to bottom. If <code>no_access</code> appears first, everyone will be blocked out and not exception will be made to allow users in the <code>only_from</code> attribute.</p> <p>===Using bind Examples=== The bind attribute allows for associating a particular service with a specific interface's IP address. Suppose we have an internal ftp server that supplies read-only resources for company employees via anonymous ftp. This ftp server has two interfaces, one that attaches to the corporate environment and the other that connects to a private internal network that is generally accessible only to employees who work in that particular group. We can implement the desired functionality using the following entries in /etc/xinetd.conf:</p> <p> defaults {     log_type = SYSLOG local4.info     log_on_success = PID HOST EXIT DURATION     log_on_failure = HOST     instances = 8 } <p>service ftp {     id = ftp     socket_type = stream     protocol = tcp     wait = no     user = root     only_from 172.17.0.0 172.19.0.0/20     bind = 172.17.1.1 #widget     log_on_success += USERID     log_on_failure += USERID     server = /usr/sbin/in.ftpd     server_args = -l -a }</p> <p>service ftp {     id = ftp_chroot     socket_type = stream     protocol = tcp     wait = no     user = root     bind = 24.170.1.218 #widget server     log_on_success += USERID     log_on_failure += USERID     access_times = 8:30-1:30 13:00-18:00     server /usr/sbin/anon/in.aftpd } Each of the two ftp service entries has a unique id attribute. There is no limit to the number of services with the same name as long as each has a unique identifier. In this case, we set the id attribute to ftp for the internal ftp server and ftp-chroot for the external anonymous server. Note that in the latter case the daemon invoked is /usr/sbin/anon/in.aftpd (this is the effective equivalent to twist for TCP-wrappers), which is different than the former service. The use of bind in each case will allow packets destined only to that interface to invoke the indicated daemon. Thus, we can reach the anonymous server by executing ftp widget server. Since there are no access controls for that service, everyone has access. However, access is granted only between 8:30 A.M. through 11:30 A.M. and I P.M. through 6 P.m. due to the access_times attribute. On the other hand, executing ftp widget will be successful only if the first two octets of the client IP address begin with 172.17 or the request comes from an address in the range 172.19.0.0 through 172.19.15.255 because of the only_from attribute.</p> <p>===Using redirect Examples===</p> <p>The redirect attribute provides a method for proxying a service through a server. In other words, the user may telnet to a particular server running xinetd, and that server would open another connection to a different system. This can implemented this with the following entries in an /etc/xinetd.conf file:</p> <p> service telnet {     socket_type = stream     wait = no     flags = REUSE     user = root     bind = 172.17.33.111     log_on_success = PID HOST EXIT DURATION USERID     log_on_failure = RECORD HOST } <p>service telnet {     socet_type = stream     wait = no     flags = REUSE     user = root     bind = 201.171.99.99     redirect = 172.17.1.1 23     log_on_success = PID HOST EXIT DURATION USERID     log_on_failure = RECORD HOST }</p> <p>The REUSE attribute is essential since the server will be terminating and restarting continuously. The bind attribute has the following effect. If the command telnet 172.17.33.111 is used, the connection will be made to the server itself. If, on the other hand, the command telnet 201.171.99.99 is used, then the connection will be forwarded to 172.17.1.1 on port 23 (the telnet port).</p> <p>==Incorporating hosts.deny and hosts.allow into Xinetd Super Server== Including TCP_wrappers functionality in /etc/xinetd.conf is very simple. Wherever /usr/sbin/tcpd is set as the value for the attribute server, that service will be wrapped. The following examples illustrate this:   defaults {     log_type = SYSLOG local4.info     log_on_success = PID HOST EXIT DURATION     log_on_failure = HOST     instances = 8 } <p>service ftp {     id = ftp_chroot     socket_type = stream     protocol = tcp     wait = no     user = root     only_from = 172.17.0.0     log_on_success += USERID     log_on_failure += USERID     access_times = 8:30-6:30     server = /usr/sbin/tcpd     server_args = /usr/sbin/in.ftpd -l -a }</p> <p>service telnet {     socket_type = stream     wait = no     flags = NAMEINARGS REUSE     user = root     bind = 172.17.33.111     server = /usr/sbin/tcpd     server_args = /usr/sbin/in.telnetd     log_on_success = PID HOST EXIT DURATION USERID     log_on_failure = RECORD HOST }</p> <p>The xinetd hosts access control differs from the method used by TCP wrappers. While TCP wrappers places all of the access configuration within two files, /etc/hosts.allow and /etc/hosts.deny, each service's file in /etc/xinetd.d can contain its own access control rules.</p> <p>The following hosts access options are supported by xinetd: * only_from \u2014 Allows only the specified hosts to use the service. * no_access \u2014 Blocks listed hosts from using the service. * access_times \u2014 Specifies the time range when a particular service may be used. The time range must be stated in 24-hour format notation, HH:MM-HH:MM.</p> <p>The only_from and no_access options can use a list of IP addresses or host names, or can specify an entire network. Like TCP wrappers, combining xinetd access control with the enhanced logging configuration can enhance security by blocking requests from banned hosts while verbosely record each connection attempt.</p> <p>For example, the following /etc/xinetd.d/telnet file can be used to block Telnet access from a particular network group and restrict the overall time range that even allowed users can log in:</p> <p> service telnet {     disable = no     flags = REUSE     socket_type = stream     wait = no     user = root     server = /usr/sbin/in.telnetd     log_on_failure += USERID     no_access = 10.0.1.0/24     log_on_success = PID HOST EXIT     access_time = 09:45-16:15 } In the above example, when a client system from the 10.0.1.0/24 network, such as 10.0.1.2, tries access the Telnet service, it will receive the following message:   Connection closed by foreign host. In addition, their login attempt is logged in /var/log/secure as follows:   May 15 17:38:49 boo xinetd[16252]: START: telnet pid=16256 from=10.0.1.2 May 15 17:38:49 boo xinetd[16256]: FAIL: telnet address from=10.0.1.2 May 15 17:38:49 boo xinetd[16252]: EXIT: telnet status=0 pid=16256</p> <p>==Creating a 'defaults' Entry== The purpose of the defaults entry in the /etc/xinetd.conf file is to specify default values for all services in the file. These default values may be overridden or modified by each individual service entry. The following is an example of defaults entry as it might appear in /etc/xinetd.conf.</p> <p> defaults {     log_type = SYSLOG local4 info     log_on_success = PID HOST EXIT DURATION     log_on_failure = HOST     instances = 8     disabled = in.tftdp in.rexecd }</p> <p>The above defaults specify that for all services, log messages will be sent to the syslogd daemon via the local4. info selector. Successful connections to services will cause the PID, client IP address, termination status, and time of connection to be logged. Unsuccessful connection attempts will have the client IP address logged. The maximum number of instances for any one service is set to eight. Two services, in.tftpd and in.rexecd are disabled. The Red Hat distribution puts a link in the xinetd.conf file to a directory where all the individual service files are configured and kept</p> <p>If the xinetd.conf file's default section is configured with an <code> includeidr</code> tag then individual service files will be configured within those directories. Example:   defaults {     instances = 60     log_type = SYSLOG authpriv     log_on_success = HOST PID     log_on_failure = HOST RECORD } includedir /etc/xinetd.d This example would store indicidual configuration files for each service in <code>/etc/xinetd.d</code>. Note that configurations in these files will override the defaults if they differ</p> <p>==Notes==</p> <p>==Sources==</p>"},{"location":"services/tcpdump/","title":"TCPDump","text":"<p>tcpdump is a network monitoring application that can capture packet traffic on a broadcast network. The traffic of interest can be specified by command line arguments when tcpdump is run and the utility will only dump the required traffic. It supports a variety of output options but by default it outputs packet headers in a user readable format to the standard output. The size of data from a packet to be dumped is 68 octets by default. This can be altered by specifying the option <code>-s snaplen</code>, where snaplen is the length of a packet to be dumped. It records network traffic by the use of a host computer's packet filter. A packet filter is an operating system service for selectively recording network packets. This is referred to as packet capture. tcpdump is implemented using the Packet Capture library (libpcap), which recognizes a variety of packet capture systems on different operation system platforms. An example of a packet capture system is the Berkeley Packet Filter (BPF). The link-level device driver calls BPF when a packet arrives at the network interface before it makes a decision whether the packet is addressed to the local host and should be passed up to the system protocol stack. The BPF feeds the packet to the tcpdump process's filter that is specified by the user and is responsible for deciding whether a packet is to be accepted. If the packet is to be accepted, BPF copies the requested amount of data to the buffer associated with the filter and timestamps it. Timestamps measured by tcpdump are closer to packets' wire times than those acquired in the user-space by a process such as ping, since they are captured by BPF just above the link-layer device driver which directly communicates with the network interface card. Furthermore, since ping timestamps an ICMP request packet earlier than BPF and timestamps the ICMP reply packet later than BPF, the round-trip time reported by ping is always larger than that reported by the tcpdump running on the same machine</p> <p>==Using TCPDump== The tcpdump program is very useful tool that can be used to capture and read TCP/IP traffic on the network. Specifically it captures and prints out the headers of packets on a network interface. By default it captures and prints all the traffic the local subnet in a standard format. Command line options are provided for modifying the default behavior, either by collecting specified records, printing in a more verbose mode (-v), printing in hexadecimal (-x) or writing records as \"raw packets\" to a file (-w) instead of printing as standard output.</p> <p>{| class=\"wikitable\" ! Tag ! Use |- | -v || Verbose Mode |- | -x || Print in Hexadecimal |- | -w || Write Records as \"Raw Packets\" to file instead of Standard Output |}</p> <p>We can also design tcpdump filters and specify packets to be captured. Rather than gather all traffic passing on the local subnet, tcpdump can be instructed to capture packets with very specific characteristics. Examples such filters would be to capture only TCP packets, or packets to a given port, say telnet, port 23. We can also limit the scope of packets captured to a specific IP address or hostname. Combinations of protocol details can be used to selectively capture packets. Just about any field in an IP datagram, including the actual data payload, can be used to select the packets that are captured. tcpdump has a default standard output based on the protocol (TCP, UDP, ICMP) of the record that is displayed</p> <p>==Examples== The following is an example of the command: <code> tcpdump udp </code>     Timestamp       source.port            dest.port    udp bytes 21:49:18.485000 ithaca.olympus.728 &gt; valhalla.bcit.111: udp 56 <pre><code>timestamp: hour:minutes:seconds.fractions of seconds\nsource.port: source IP/hostname.source port\ndest.port: destination IP/host.destination port\nudp: may or may not expressly label the udp protocol\nbytes: number of bytes of udp data (payload)&lt;/nowiki&gt;\n</code></pre> <p>The source information includes the source host name, ithaca.olympus, or IP number depending upon whether the IP can be resolved. To save dns query delays, tcpdump can be run with the <code>-n</code> paramter. The hostname is followed by a period and the source port, in this case 728.\uf0b7 Immediately following the greater than sign is the destination host or IP, valhalla.bcit followed by a period, and then the destination port, in this case port 111 (this happens to be the portmapper or sunrpc port). Next we see the word \"udp\", specifying the protocol. Note that not all udp records will be specifically labeled as such. DNS, or port 53, is an exception. The final field indicates the number of bytes found in the packet payload.</p> <p>The following is an example of the command: <code> tcpdump tcp </code></p> <p> timestamp source.port dest.port flags beginning:ending   bytes options seq#   seq# 21:49:18.485000 zeus.net.1173 &gt; dns.net.21: S 62697789:62697789(0) win 512 <pre><code>flags: tcp flags ( PSH, RST, SYN, FIN)\nbeginning seq #: for the initial connection, this is the initial\nsequence number (ISN) from the source IP\nending seq #: this is the beginning sequence number + data bytes\nbytes: data bytes (payload) in the tcp packet\noptions: options that the source host advertises to the destination host&lt;/nowiki&gt;\n</code></pre> <p>The tcp output is identical to the udp record as far as timestamp, source and destination host and port. What distinguishes the tcp format from the others are the tcp flags, sequence numbers, acknowledgements, acknowledgement numbers, and tcp options. In this trace the flag SYN or S is set following the destination port of 21 (ftp). The SYN flag indicates a request to begin a tcp session. Other possible flag values are P for PUSH (sends remaining data), R for RESET (abort a connection) and F for FIN (gracefully terminates a connection). A period in the flag field simply indicates that none of the PUSH, RESET ,SYN or FIN flags are set. Next is the starting sequence number. Sequence numbers are used by tcp to achieve reliable packet delivery. In this case, since this is an initial connection, it is known as the initial Sequence Number or ISN. The ending sequence number is the sum of the initial sequence number plus the number of tcp data bytes sent in this tcp segment. A SYN connection sends no data bytes, as represented by the zero in parentheses. Data will not be sent until the two hosts complete the three-way handshake.</p> <p>Finally, there is a tcp options field. In this record, we see that zeus.net is advertising a sliding window size of 512 bytes. A sliding window is used by one host to inform a remote host that it has maximum receive buffer size of 512 bytes. If dns.net is a more powerful, larger host, it will have to slow itself down and regulate the data it transmits so as to not overrun the buffer size of zeus.net. The next few slides provide examples that illustrate the basic details displayed by tcpdump.</p> <p>==Writing TCPDump Filters== One of the most powerful features of tcpdump is its ability to use filters to narrow down the type of packet captures to be displayed. Often, we will want to examine certain fields in the IP datagram for signs of malicious activity directed at our network. If we want wanted to capture specific types of packets, all we have to do is write a filter that uniquely specifies that type of packet. Then we execute tcpdump with the filter option, and tcpdump will use the filter as part of the packet selection criteria. Filters need to specify an item of interest, such as a field in the IP datagram, for record selection. Such items might be part of the IP header (the IP header length, for example), the TCP header (TCP flags, for example), the UDP header (the destination port, for example), or the ICMP message (message type, for example). Also provided are some macros for commonly used fields, such as \"port\" to indicate a source or destination port or \"host\" to indicate an IP numberor name of a source or destination host. </p> <p>Sometimes the fields we are interested in do not have macros, and so we must use the format of referencing a field by the protocol and displacement in terms of bytes into that protocol. tcpdump assigns a designated name for each type of header associated with a protocol. \"ip\" is used to denote a field in the IP header or data portion of the IP datagram, \"tcp\" for a field in the TCP header or data of the TCP segment, \"udp\" for a field in the UDP header or data of the UDP datagram, and \"icmp\" for a field in the ICMP header or data of the ICMP message. This gives us a reference a field in a given protocol by its displacement in bytes from the beginning of the protocol header. For instance, ip[0] indicates the first byte of the IP datagram, which happens to be part of the IP header (remember to count starting at 0). tcp[13] is byte 14 into the TCP segment, which is also part of the TCP header, and icmp[0] is the first byte of the ICMP message, which is the ICMP message type.</p> <p>The format for filters is the following:    [:]   The relational operators that tcpdump recognizes are: =, &lt;, &gt;, &lt;=, &gt;=,!=, as well as the logical operators and (&amp;), or (|), not (!).  A simple example to capture all telnet traffic would be:   tcpdump \u2013i eth0 \u2013x \u2018tcp[2:2] = 23\u2019 This tells tcpdump to look at the tcp header. We skip bytes zero, and one (the source port), and look at bytes two and three (the destination port). If this two-byte field contains the value 23, this is a telnet packet. To simplify things, tcpdump also includes some built in macro\u2019s that make writing filters even simpler. The following filter accomplishes the same thing as the filter above:   tcpdump \u2013i eth0 \u2013x \u2018tcp and dst port 23\u2019 The following would filter to capture NFS traffic:   tcpdump \u2013i eth0 \u2013x \u2018ip and udp port 2049\u2019 If you just want to capture tcp packets you could use 1 of these 2 filters:   tcpdump \u2013i eth0 \u2013x \u2018tcp\u2019 #uses macros  tcpdump \u2013i eth0 \u2013x \u2018ip[9:1] = 6\u2019 #parces ip header If you wanted to be more specific and capture SYN packets we could use the filter:   tcpdump \u2013i eth0 \u2013x \u2018tcp[13] &amp; 2 != 0\u2019 This filter tells tcpdump to look at byte 13 of the tcp header (remember to start counting at zero). Once you find byte 13, logically AND the value with 2 (0010), and  compare the result of the AND to the value 1.  Note there is also a bug in this filter. SYN/ACK packets will also be captured ?  Here is another example that is a complete filter to capture a signature of an IP datagram that has IP options set:   tcpdump -i eth0 -x 'ip[0] &amp; 0x0f &gt; 5' It is possible to string several filters together using parenthesis and logical operators to create very powerful and effective filters. Note that this is not recommended    # Capture any packets that are ip and                                                  #IP packets, AND ( (ip[12:4] = ip[16:4])                                   #the source IP == the destination IP or                                                      #OR     ((not src net 192.168) and                          #if the source net is not 192.168 AND     (         (ip[19] = 0xff) or                              #the destination is all !'s broadcast OR         (ip[19] = 0x00) or                              #the destination is all 0's broadcast OR         ((ip[6:1] &amp; 0x20 != 0 ) and                     #if MF is set, AND         (ip[6:2] &amp; 0x1fff = 0)) or                      #this is the first fragment OR             (net 0 or net 127 or net 1) or              #if the destination net is 0, 127 or 1                                                         #OR         (ip[12] &gt; 239) or                               #if the destination IP is class D or E OR         ( ((ip[0:1] &amp; 0xf) &gt; 5) )                       #if the packet contains IP options     ))                                                  #(more than five 32 bit words in the IP header means options is in use) )  As you can see typing in such filters is complex and tedious and also not recommended. We can though put these filters into a filter file and call it for example 'ipfilter' and call tcpdump with the <code> -F </code> filter option:    tcpdump \u2013i eth0 \u2013x \u2013F ipfilter It is important to note that the filter files must not contain comments. The comments above are there just for the purposes of explanation and must be removed before invoking tcpdump with the file.  Some of the telltale indications in the IP header that you might be a target of reconnaissance include traffic sent to your broadcast address, fragmentation, and the presence of IP options. You should never see legitimate traffic directed to your broadcast address from outside your network, and that should be blocked at the external firewall.  Fragmentation is a natural enough byproduct of a datagram destined to your network via a network that permits a larger MTU than one of the routes it took to get to your network. But, fragmentation can also be used for denial-of-service attacks or to try to bypass notice by an IDS or routers that cannot keep track of state  ==Detecting Traffic to the Broadcast Addresses== A broadcast address is defined as one with a final octet of 255 or 0. This includes most broadcast addresses subdivided on classic byte boundaries. The destination address is found in bytes 16 through 19 (32 bits) of the IP header. Of concern to us is the final octet, or byte 19. We can describe the broadcast addresses as follows:   ip[19] = 0xff ip[19] = 0x00 Or as a combined filter as follows:   ip[19] = 0xff or ip[19] = 0x00 As a matter of course hexadecimal notation is used. But, the above can also be expressed in decimal as follows:   ip[19] = 255 or ip[19] = 0 Depending on where the host running the tcpdump filter is located, you might pick up broadcast traffic inside your network. Assume that the inside network is 192.168.x.x. To further refine this filter to examine only traffic directed toward your network from a foreign source, the filter as follows:   not src net 192.168 and (ip[19] = 0xff or ip[19] = 0x00) The macro operator not, is used to negate; and src, to indicate the traffic originated from this source; and net, to indicate a subnet. The above filter will capture any traffic that originates from a source network other than your own that is destined for the broadcast addresses.  ==Detecting Fragmentation== All fragments in a fragmented packet sequence except the last one have the more fragments (MF) bit set. If we locate this field and test the MF bit to see if it is set, we can find most of the fragmented traffic directed to our network. Specifically, if you count into the IP header, you will find it in the sixth byte. It is the third bit from the left of the high order-bit. The mask needs to be 0010 0000, which is a hexadecimal 0x20. Thus our filter becomes <code>ip[6] &amp; 0x20 != 0</code>.  Another filter to detect the MF bit would match the fragmented datagrams but will miss the the last fragment (which has the 2nd bit set to 0):   tcpdump -i eth1 'ip[6] = 32' The last fragment has the first 3 bits set to 0, but contains values in the fragment offset field.  The following filter will match all the fragments, including the last fragment:   tcpdump -i eth1 '((ip[6:2] &gt; 0) and (not ip[6] = 64))'  * <code>ip[6:2] &gt; 0</code> - returns anything with a value of at least 1 * <code>not ip[6] = 64</code> - omit datagrams with the DF bit set  Note that because fragmentation is not always malicious, you are likely to generate false positives with this filter.  ==UDP Filters== Many backdoors and Trojans use UDP ports. UDP is a very simple header to deal with as far as filter design is concerned. To detect UDP traffic, we select UDP ports to be monitored for suspicious activity.For example, to scan for traffic to port 31337, the filter is as follows:   udp and dst port 31337 The main effort is not so much designing the filter but in deciding which ports you want include, adding them to the filter, and keeping the filter current with the real world of ever-expanding UDP exploits. Consider a popular UDP application, traceroute. This tool maps a path to a destination host by attempting to send UDP datagrams to high-numbered ports of the destination host.If a host on your network is that destination host, you want to be alerted of the attempted or successful traceroute. Start by looking at UDP activity to ports in the 3000-33999 range for most of the traceroute activity.   Note that the Windows version of traceroute use ICMP echo requests and replies, so this signature does not detect that activity.   Also note that some versions of traceroute enable the user to provide command-line options, one of which is a destination port. Therefore, this filter might not capture all traceroute activity, but it will find most of the conventional activity. The UDP destination port number is found in bytes 2 and 3 of the UDP header.   Question: \"why don't we use the port macro rather than byte displacements?\" For instance, why can't we use this filter:   dst port &gt;= 33000 and dst port &lt; 34000 The problem is that when tcpdump uses a range such as this and not one exact value, you have to express that field in terms of the primitive protocol and displacement and forgo the use of macros. The correct syntax to discover traceroute then becomes this:   udp[2:2] &gt;= 33000 and udp[2:2] &lt; 34000 The length option [2:2] will span bytes. We need to examine 2 consecutive bytes starting at byte 2.   We can further limit the amount of traffic that this filter extracts by examining the TTL value along with the destination port. traceroute operates by manipulating the TTL value found in the IP header. It records the routers that it traverses and does so using an incrementing TTL value. Frequently you will see a TTL of 1 on the sensor host running tcpdump before it crosses a router that will expire it. This is a classic traceroute signature. Therefore, we can further refine the traceroute filter to include the TTL value to eliminate some of the noise associated with discovering traceroutes. The TTL field is found in the IP header; it has no macro to reference it, but it is located in the eighth byte. The new filter is as follows:   udp[2:2] &gt;= 33000 and udp[2:2] &lt; 34000 and ip[8] = 1  ==TCP Filters== Filters for TCP traffic are mostly concerned with initial SYN connections and other types of anomalous flag combinations that might indicate some kind of reconnaissance or mapping efforts. We want to look for initial SYN connections because they inform us of attempted connections to a TCP port. This doesn't necessarily mean that they were successful. If the sensor is located outside a packet-filtering device that blocks access to the TCP destination port, it will never reach the host. We can glean a lot of intelligence by detecting this activity, the least of which is discovering rogue TCP ports that hosts on your network might be offering. ===Filters for Examining TCP Flags=== The TCP flag bits are located 13 bytes into the TCP header. Because we are looking for individual bits in the bytes, some bit-masking must be performed to select the flag or flags to be examined. We start by writing a filter to extract records with the SYN flag alone set:   tcp[13] &amp; 0xff = 2 The mask consists of all 1's. Why not use a mask of 0's in all fields except the SYN flag, i.e., (tcp[13] &amp; 0x02 = 2)?  By masking a bit with a 0, the resulting (byte) value is necessarily 0. The value bit could be 1, however, and the 0 mask would discard it. Suppose that you want to look at TCP segments with the SYN flag alone set. Now suppose that you have a TCP flag byte with both the SYN and ACK flags set. The binary value that you would see for the TCP flag byte would be 0001 0010. If that were masked with 0000 0010, you would end up with a result of 0000 0010,which is 2. This is a misleading result because the intent here is to capture those packets with only the SYN bit set. Therefore, masking with 0's in fields other than the SYN flag selects TCP segments with other flags set along with the SYN flag. To prevent this from occurring, you use the original filter and look for an exact value. This filter does not select records with other flags set along with the SYN flag. The following are some other TCP flag combinations that might be very useful to be aware of:    tcp[13] = 0 This indicates null scans with no flags set. This condition should never occur.   tcp[13] = 3 This indicates activity where both the SYN and FIN flags are set simultaneously. This is definitely an anomalous condition. Alter the filter to tcp[13] &amp; 0x03 = 3, to detect any activity with both the SYN and FIN flags set, as well as any other flags set. In this case, you don't necessarily want to limit this to SYN and FIN alone.   tcp[13] = 0x10 and tcp[8:4] = 0 Indicates activity with the ACK flag set, but with an acknowledgement value of 0. Any TCP segment with the ACK flag on should have a minimum acknowledgement value of 1 that occurs during the three-way handshake. At least 1 sequence number has to be consumed to elicit a valid acknowledgement; otherwise no acknowledgement would be returned.  This filter captures nmap operating system fingerprinting scans that sends TCP traffic to various destination ports with the ACK flag alone set, but a 0 value in the acknowledgement field.   tcp[13] &gt;= 64  The two high-order bits in the TCP 8-bit flag byte are labeled reserved bits. These 2 bits should be 0's; if they are not, something is amiss. The first reserved bit is found in the 2, (64) position, and the second is found in the 27 (128) position. If either or both bits are set, the value for the TCP flag byte is greater than or equal to 64. nmap sometimes sets one or both of these bits to perform operating system fingerprinting. Most hosts reset these values to 0's, but some leave the set value. This is used by nmap to help classify the operating system behavior.  ==Capturing HTTP Traffic== Let us say we want to design a filter that will capture any packets containing HTTP GET requests. The HTTP request will contains a starting string similar to (which is 16 bytes counting the carriage return but not the backslashes)   GET / HTTP/1.1\\r\\n  Assuming no IP options are set, the GET command will use the bytes 20, 21 and 22. Options usually will take up to 12 bytes (12th byte indicates the header length, which should indicate 32 bytes). Therefore, in our filter we match bytes 32, 33 and 34 (remember to count from 0). The string \"GET \" in hex is: 47455420. Thus our filter becomes:   tcpdump -i eth1 'tcp[32:4] = 0x47455420'  ==Configure TCPDump As A Daemon== TCPDump has a number of tools that make setting it up as a monitoring tool quite easy. The only trouble begins when u want to be able to have TCPDump run in the background and also persist upon restart of the machine. the implementation below has been implemented on Raspbian and likely will work on Debian. It uses a mix of cron and init.d to operate.  This implementation assumes everything working as root which is not the greatest idea, but further experimentation can narrow this setup down  Note this configuration requires tcpdump, init.d, and zip all to be installed in order for all scripts to work  ===Setup Logging Directories=== These will be the places TCPDump will dump all of its findings and will apply archiving. Make the following directories and file persmissions:   sudo mkdir /var/log/tcpdump sudo mkdir /var/log/tcpdump/tcpdump_archive sudo chmod 777 -R /var/log/tcpdump sudo chmod 777 -R /var/log/tcpdump_archive    ===Create CRON Manager Scripts=== In the /home directory create the files tcpdump-startup.sh and tcpdump-cleanup.sh. Put the following into each file  ====tcpdump-startup.sh====   #!/bin/bash  service tcpdump-monitor stop service tcpdump-monitor start exit   ====tcpdump-cleanup.sh====   #!/bin/bash  date=`date +%Y%m%d-%H:%M:%S`  #First we need to stop tcpdump from executing service tcpdump-monitor stop sleep 10m #tcpdump might have alot to do. so lets wait a bit before continueing  #Then go find all currently uncompressed files #Compress them #Store them somewhere else zip /var/log/tcpdump/tcpdump_archive/tcpdump-archive-$date.zip /var/log/tcpdump/*.tcpdmp /var/log/tcpdump/*.tcpdmp. -m  #Then go start TCPDump back up again #/home/tcpdump-startup.sh service tcpdump-monitor start exit  ===Create Init.d Service Script=== The following is a Debian init.d service script. This is used to manage the number of instances that are running so that multiple are not created.  Execute the following commands   sudo touch /etc/init.d/tcpdump-monitor sudo chmod 755 /etc/init.d/tcpdump-monitor   Open the just created tcpdump-monitor file in a text editor and fill in the following information   #!/bin/bash  ### BEGIN INIT INFO # Provides:          tcpdump-monitor # Required-Start:    $all # Required-Stop:  # Default-Start:     2 3 4 5 # Default-Stop:       # Short-Description: initiates tcpdump to be monitoring all traffic # Description:       This is a test daemon #                    This provides example about how to #                    write a Init script. ### END INIT INFO  PATH=/sbin:/usr/sbin:/bin:/usr/bin    NAME=tcpdump-monitor USER=root  date=`date +%Y%m%d-%H:%M:%S`  # 14400 seconds = 4 hours # 3600 seconds = 1 hour  # Z - set tcpdump execution permissiong to root # C - set max file size, when tcpdump wants to print to file, if the current file is bigger, it will append numbers to the filename # w - packets are written out raw to the specified file instead of formatted. these files can be parsed again with tcpdump -r  # i - set interface to listen on # G - set max seconds before trigger log rotation # n - don't resolve DNS names, this will reduce the amount of DNS traffic needed # S - print absolute rather then relative sequence numbers (better for debugging and comparing with wireshark # s - set snaplen. setting to 0 sets to default packet length of 65535. Useful for compatability   case \"$1\" in   start|\"\")         /usr/local/sbin/tcpdump -Z root -C 100 -w \"/var/log/tcpdump/%Y%m%d-%H:%M:%S.tcpdmp.\" -i eth0  -G 3600 -n -S -s 0 2&gt;/dev/null &amp;         exit 0         ;;   restart|reload|force-reload)         echo \"Error: argument '$1' not supported\" &gt;&amp;2         exit 4         ;;   stop|status)         tprocess=`ps aux | grep \"/usr/local/sbin/tcpdump -Z root -C 100 -w /var/log/tcpdump/%Y%m%d-%H:%M:%S.tcpdmp. -i eth0 -G 3600 -n -S -s 0\" | awk 'NR==1{print $2}'`         kill $tprocess         exit 0         ;;   *)         echo \"Usage: tcpdump.sh [start|stop]\" &gt;&amp;2         exit 3         ;; esac  The above script will run tcpdump and use a number of built in options to the tool itself. Every hour tcpdump will change the file it writes to, and if a large enough amount of data occurs, will split out the data it finds into incremented files. All of these files will be outputed with a the same timestamp and an appended number to signify order of each.  ===Register Init.d Script=== Registering is the easy part. This will make the tcpdump-monitor script available through the service command. Execute the following command   sudo update-rc.d tcpdump-monitor defaults  After executing, the script will be registered as a service. You can test this by running sudo service tcpdump-monitor start/stop/status. If there are issues where the command is not found, execute ldconfig or restart the system. On proper registering tcpdump-monitor should be available from the service command.  ====Other Init.d Tools==== The Following Is Only For Context Information. It IS NOT part of the setup Init.d also comes with functionality to boot it's services at start, and in most cases, it will show tcpdump-monitor at this point as started. By executing the following command you will also register the script to be booted at startup   sudo update-rc.d tcpdump-monitor enable As of this writing, this does not work with the current script configuration. Upon booting the service will say tcpdump-monitor has started but using ps it does not show it operating. You can verify if the startup worked by checking processes with the following command   ps aux | grep \"tcpdump\"  For completeness, To disable init.d from attempting to startup the tcpdump-monitor service, execute the following command   sudo update-rc.d tcpdump-monitor disable You can also unregister the service with the following command   sudo update-rc.d -f tcpdump-monitor remove  ===Configure CRON=== At this point our service can be started and stopped and kept track of by init.d. We now need to configure persistence functionality so that the process is started on bootup, checked that it is still running, and also triggers archiving of rotated logs  Open /etc/crontab in a test editor and enter the following to the bottom of the file   0 */4 * * * root /home/tcpdump-cleanup.sh  @reboot root /home/tcpdump-startup.sh  The above will trigger tcpdump-cleanup.sh every 4 hours and thus will process all created logs so far and archive them into the /var/log/tcpdump/tcpdump_archive folder. Additionally at startup (@reboot) the tcpdump-startup.sh script will be called which will startup our service upon system bootup  ===Enable Promisc Mode For TCPDump=== TCPDump reads its data straight from the eth0 network card. It will only read in data destined for the host itself. In order to read all traffic that may come its way, promiscuous mode needs to be enabled. You can enable promiscuous mode by executing the following command   ifconfig eth0 promisc  To disable it later execute   ifconfig eth0 -promisc  Note that this change is not persistant. In order to keep the change on reboot, create another shell script stored in /home with chmod 777 permissions, and add it to cron for @reboot.  ==Notes==  ==Sources=="}]}